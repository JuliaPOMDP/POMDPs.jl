var documenterSearchIndex = {"docs":
[{"location":"policy_interaction/#Interacting-with-Policies","page":"Interacting with Policies","title":"Interacting with Policies","text":"","category":"section"},{"location":"policy_interaction/","page":"Interacting with Policies","title":"Interacting with Policies","text":"A solution to a POMDP is a policy that maps beliefs or action-observation histories to actions. In POMDPs.jl, these are represented by Policy objects. See Solvers and Policies for more information about what a policy can represent in general.","category":"page"},{"location":"policy_interaction/","page":"Interacting with Policies","title":"Interacting with Policies","text":"One common task in evaluating POMDP solutions is examining the policies themselves. Since the internal representation of a policy is an esoteric implementation detail, it is best to interact with policies through the action and value interface functions. There are three relevant methods","category":"page"},{"location":"policy_interaction/","page":"Interacting with Policies","title":"Interacting with Policies","text":"action(policy, s) returns the best action (or one of the best) for the given state or belief.\nvalue(policy, s) returns the expected sum of future rewards if the policy is executed.\nvalue(policy, s, a) returns the \"Q-value\", that is, the expected sum of rewards if action a is taken on the next step and then the policy is executed.","category":"page"},{"location":"policy_interaction/","page":"Interacting with Policies","title":"Interacting with Policies","text":"Note that the quantities returned by these functions are what the policy/solver expects to be the case after its (usually approximate) computations; they may be far from the true value if the solution is not exactly optimal.","category":"page"},{"location":"install/#Installation","page":"Installation","title":"Installation","text":"","category":"section"},{"location":"install/","page":"Installation","title":"Installation","text":"If you have a running Julia distribution (Julia 0.4 or greater), you have everything you need to install POMDPs.jl. To install the package, simply run the following from the Julia REPL:","category":"page"},{"location":"install/","page":"Installation","title":"Installation","text":"import Pkg\nPkg.add(\"POMDPs\") # installs the POMDPs.jl package","category":"page"},{"location":"install/","page":"Installation","title":"Installation","text":"Some auxiliary packages and older versions of solvers may be found in the JuliaPOMDP registry. To install this registry, run:","category":"page"},{"location":"install/","page":"Installation","title":"Installation","text":"using Pkg; pkg\"registry add https://github.com/JuliaPOMDP/Registry\"","category":"page"},{"location":"install/","page":"Installation","title":"Installation","text":"Note: to use this registry, JuliaPro users must also run edit(normpath(Sys.BINDIR,\"..\",\"etc\",\"julia\",\"startup.jl\")), comment out the line ENV[\"DISABLE_FALLBACK\"] = \"true\", save the file, and restart JuliaPro as described in this issue.","category":"page"},{"location":"def_pomdp/#defining_pomdps","page":"Defining POMDPs and MDPs","title":"Defining POMDPs and MDPs","text":"","category":"section"},{"location":"def_pomdp/#Consider-starting-with-one-of-these-packages","page":"Defining POMDPs and MDPs","title":"Consider starting with one of these packages","text":"","category":"section"},{"location":"def_pomdp/","page":"Defining POMDPs and MDPs","title":"Defining POMDPs and MDPs","text":"Since POMDPs.jl was designed with performance and flexibility as first priorities, the interface is larger than needed to express most simple problems. For this reason, several packages and tools have been created to help users implement problems quickly. It is often easiest for new users to start with one of these.","category":"page"},{"location":"def_pomdp/","page":"Defining POMDPs and MDPs","title":"Defining POMDPs and MDPs","text":"QuickPOMDPs.jl provides structures for concisely defining simple POMDPs without object-oriented programming.\nPOMDPExamples.jl provides tutorials for defining problems. \nThe Tabular(PO)MDP model from POMDPModels.jl allows users to define POMDPs with matrices for the transitions, observations and rewards.\nThe gen function is the easiest way to wrap a pre-existing simulator from another project or written in another programming language so that it can be used with POMDPs.jl solvers and simulators.","category":"page"},{"location":"def_pomdp/#Overview","page":"Defining POMDPs and MDPs","title":"Overview","text":"","category":"section"},{"location":"def_pomdp/","page":"Defining POMDPs and MDPs","title":"Defining POMDPs and MDPs","text":"The expressive nature of POMDPs.jl gives problem writers the flexibility to write their problem in many forms. Custom POMDP problems are defined by implementing the functions specified by the POMDPs API.","category":"page"},{"location":"def_pomdp/","page":"Defining POMDPs and MDPs","title":"Defining POMDPs and MDPs","text":"In this guide, the interface is divided into two sections: functions that define static properties of the problem, and functions that describe the dynamics - how the states, observations and rewards change over time. There are two ways of specifying the dynamic behavior of a POMDP. The problem definition may include a mixture of explicit definitions of probability distributions, or generative definitions that simulate states and observations without explicitly defining the distributions. In scientific papers explicit definitions are often written as T(s  s a) for transitions and O(o  s a s) for observations, while a generative definition might be expressed as s o r = G(s a) (or s r = G(sa) for an MDP).","category":"page"},{"location":"def_pomdp/#What-do-I-need-to-implement?","page":"Defining POMDPs and MDPs","title":"What do I need to implement?","text":"","category":"section"},{"location":"def_pomdp/","page":"Defining POMDPs and MDPs","title":"Defining POMDPs and MDPs","text":"Because of the wide variety or problems and solvers that POMDPs.jl interfaces with, the question of which functions from the interface need to be implemented does not have a short answer for all cases. In general, a problem will be defined by implementing a combination of functions.","category":"page"},{"location":"def_pomdp/","page":"Defining POMDPs and MDPs","title":"Defining POMDPs and MDPs","text":"Specifically, a problem writer will need to define","category":"page"},{"location":"def_pomdp/","page":"Defining POMDPs and MDPs","title":"Defining POMDPs and MDPs","text":"Explicit or generative definitions for \nthe state transition model,\nthe reward function, and\nthe observation model.\nFunctions to define some other properties of the problem such as the state, action, and observation spaces, which states are terminal, etc.","category":"page"},{"location":"def_pomdp/","page":"Defining POMDPs and MDPs","title":"Defining POMDPs and MDPs","text":"The precise answer for which functions need to be implemented depends on two factors: problem complexity and which solver will be used. In particular, 2 questions should be asked:","category":"page"},{"location":"def_pomdp/","page":"Defining POMDPs and MDPs","title":"Defining POMDPs and MDPs","text":"Is it difficult or impossible to specify a probability distribution explicitly?\nWhat solvers will be used to solve this, and what are their requirements?","category":"page"},{"location":"def_pomdp/","page":"Defining POMDPs and MDPs","title":"Defining POMDPs and MDPs","text":"If the answer to (1) is yes, then a generative definition should be used. Question (2) should be answered by reading about the solvers and trying to run them. Some solvers have specified their requirements using the POMDPLinter package, however, these requirements are written separately from the solver code, and often the best way is to write a simple prototype problem and running the solver until all MethodErrors have been fixed.","category":"page"},{"location":"def_pomdp/","page":"Defining POMDPs and MDPs","title":"Defining POMDPs and MDPs","text":"note: Note\nIf a particular function is required by a solver but seems very difficult to implement for a particular problem, one should consider carefully whether the algorithm is capable of solving that problem. For example, if a problem has a complex hybrid state space, it will be more difficult to define states, but it is also true that solvers that require states such as SARSOP or IncrementalPruning, will usually not be able to solve such a problem, and solvers that can handle it, like ARDESPOT or MCVI, usually will not call states.","category":"page"},{"location":"def_pomdp/#Outline","page":"Defining POMDPs and MDPs","title":"Outline","text":"","category":"section"},{"location":"def_pomdp/","page":"Defining POMDPs and MDPs","title":"Defining POMDPs and MDPs","text":"The following pages provide more details on specific parts of the interface:","category":"page"},{"location":"def_pomdp/","page":"Defining POMDPs and MDPs","title":"Defining POMDPs and MDPs","text":"Static Properties\nSpaces and Distributions\nDynamics","category":"page"},{"location":"concepts/#Concepts-and-Architecture","page":"Concepts and Architecture","title":"Concepts and Architecture","text":"","category":"section"},{"location":"concepts/","page":"Concepts and Architecture","title":"Concepts and Architecture","text":"POMDPs.jl aims to coordinate the development of three software components: 1) a problem, 2) a solver, 3) an experiment. Each of these components has a set of abstract types associated with it and a set of functions that allow a user to define each component's behavior in a standardized way. An outline of the architecture is shown below.","category":"page"},{"location":"concepts/","page":"Concepts and Architecture","title":"Concepts and Architecture","text":"(Image: concepts)","category":"page"},{"location":"concepts/","page":"Concepts and Architecture","title":"Concepts and Architecture","text":"The MDP and POMDP types are associated with the problem definition. The Solver and Policy types are associated with the solver or decision-making agent. Typically, the Updater type is also associated with the solver, but a solver may sometimes be used with an updater that was implemented separately. The Simulator type is associated with the experiment.","category":"page"},{"location":"concepts/","page":"Concepts and Architecture","title":"Concepts and Architecture","text":"The code components of the POMDPs.jl ecosystem relevant to problems and solvers are shown below. The arrows represent the flow of information from the problems to the solvers. The figure shows the two interfaces that form POMDPs.jl - Explicit and Generative. Details about these interfaces can be found in the section on Defining POMDPs.","category":"page"},{"location":"concepts/","page":"Concepts and Architecture","title":"Concepts and Architecture","text":"(Image: interface_relationships)","category":"page"},{"location":"concepts/#POMDPs-and-MDPs","page":"Concepts and Architecture","title":"POMDPs and MDPs","text":"","category":"section"},{"location":"concepts/","page":"Concepts and Architecture","title":"Concepts and Architecture","text":"An MDP is a mathematical framework for sequential decision making under uncertainty, and where all of the uncertainty arises from outcomes that are partially random and partially under the control of a decision maker. Mathematically, an MDP is a tuple (S,A,T,R), where S is the state space, A is the action space, T is a transition function defining the probability of transitioning to each state given the state and action at the previous time, and R is a reward function mapping every possible transition (s,a,s') to a real reward value. For more information see a textbook such as [1]. In POMDPs.jl an MDP is represented by a concrete subtype of the MDP abstract type and a set of methods that define each of its components. S and A are defined by implementing states and actions for your specific MDP subtype. R is by implementing reward, and T is defined by implementing transition if the explicit interface is used or gen if the generative interface is used.","category":"page"},{"location":"concepts/","page":"Concepts and Architecture","title":"Concepts and Architecture","text":"A POMDP is a more general sequential decision making problem in which the agent is not sure what state they are in. The state is only partially observable by the decision making agent. Mathematically, a POMDP is a tuple (S,A,T,R,O,Z) where S, A, T, and R are the same as with MDPs, Z is the agent's observation space, and O defines the probability of receiving each observation at a transition. In POMDPs.jl, a POMDP is represented by a concrete subtype of the POMDP abstract type, Z may be defined by the observations function (though an explicit definition is often not required), and O is defined by implementing observation if the explicit interface is used or gen if the generative interface is used.","category":"page"},{"location":"concepts/","page":"Concepts and Architecture","title":"Concepts and Architecture","text":"POMDPs.jl contains additional functions for defining optional problem behavior such as a discount factor or a set of terminal states.","category":"page"},{"location":"concepts/","page":"Concepts and Architecture","title":"Concepts and Architecture","text":"More information can be found in the Defining POMDPs section.","category":"page"},{"location":"concepts/#Beliefs-and-Updaters","page":"Concepts and Architecture","title":"Beliefs and Updaters","text":"","category":"section"},{"location":"concepts/","page":"Concepts and Architecture","title":"Concepts and Architecture","text":"In a POMDP domain, the decision-making agent does not have complete information about the state of the problem, so the agent can only make choices based on its \"belief\" about the state. In the POMDP literature, the term \"belief\" is typically defined to mean a probability distribution over all possible states of the system. However, in practice, the agent often makes decisions based on an incomplete or lossy record of past observations that has a structure much different from a probability distribution. For example, if the agent is represented by a finite-state controller, as is the case for Monte-Carlo Value Iteration [2], the belief is the controller state, which is a node in a graph. Another example is an agent represented by a recurrent neural network. In this case, the agent's belief is the state of the network. In order to accommodate a wide variety of decision-making approaches in POMDPs.jl, we use the term \"belief\" to denote the set of information that the agent makes a decision on, which could be an exact state distribution, an action-observation history, a set of weighted particles, or the examples mentioned before. In code, the belief can be represented by any built-in or user-defined type.","category":"page"},{"location":"concepts/","page":"Concepts and Architecture","title":"Concepts and Architecture","text":"When an action is taken and a new observation is received, the belief is updated by the belief updater. In code, a belief updater is represented by a concrete subtype of the Updater abstract type, and the update(updater, belief, action, observation) function defines how the belief is updated when a new observation is received.","category":"page"},{"location":"concepts/","page":"Concepts and Architecture","title":"Concepts and Architecture","text":"Although the agent may use a specialized belief structure to make decisions, the information initially given to the agent about the state of the problem is usually most conveniently represented as a state distribution, thus the initialize_belief function is provided to convert a state distribution to a specialized belief structure that an updater can work with.","category":"page"},{"location":"concepts/","page":"Concepts and Architecture","title":"Concepts and Architecture","text":"In many cases, the belief structure is closely related to the solution technique, so it will be implemented by the programmer who writes the solver. In other cases, the agent can use a variety of belief structures to make decisions, so a domain-specific updater implemented by the programmer that wrote the problem description may be appropriate. Finally, some advanced generic belief updaters such as particle filters may be implemented by a third party. The convenience function updater(policy) can be used to get a suitable default updater for a policy, however many policies can work with other updaters.","category":"page"},{"location":"concepts/","page":"Concepts and Architecture","title":"Concepts and Architecture","text":"For more information on implementing a belief updater, see Defining a Belief Updater","category":"page"},{"location":"concepts/#Solvers-and-Policies","page":"Concepts and Architecture","title":"Solvers and Policies","text":"","category":"section"},{"location":"concepts/","page":"Concepts and Architecture","title":"Concepts and Architecture","text":"Sequential decision making under uncertainty involves both online and offline calculations. In the broad sense, the term \"solver\" as used in the node in the figure at the top of the page refers to the software package that performs the calculations at both of these times. However, the code is broken up into two pieces, the solver that performs calculations offline and the policy that performs calculations online.","category":"page"},{"location":"concepts/","page":"Concepts and Architecture","title":"Concepts and Architecture","text":"In the abstract, a policy is a mapping from every belief that an agent might take to an action. A policy is represented in code by a concrete subtype of the Policy abstract type. The programmer implements action to describe what computations need to be done online. For an online solver such as POMCP, all of the decision computation occurs within action while for an offline solver like SARSOP, there is very little computation within action. See Interacting with Policies for more information.","category":"page"},{"location":"concepts/","page":"Concepts and Architecture","title":"Concepts and Architecture","text":"The offline portion of the computation is carried out by the solver, which is represented by a concrete subtype of the Solver abstract type. Computations occur within the solve function. For an offline solver like SARSOP, nearly all of the decision computation occurs within this function, but for some online solvers such as POMCP, solve merely embeds the problem in the policy.","category":"page"},{"location":"concepts/#Simulators","page":"Concepts and Architecture","title":"Simulators","text":"","category":"section"},{"location":"concepts/","page":"Concepts and Architecture","title":"Concepts and Architecture","text":"A simulator defines a way to run one or more simulations. It is represented by a concrete subtype of the Simulator abstract type and the simulation is an implemention of simulate. Depending on the simulator, simulate may return a variety of data about the simulation, such as the discounted reward or the state history. All simulators should perform simulations consistent with the Simulation Standard.","category":"page"},{"location":"concepts/","page":"Concepts and Architecture","title":"Concepts and Architecture","text":"[1] Decision Making Under Uncertainty: Theory and Application by Mykel J. Kochenderfer, MIT Press, 2015","category":"page"},{"location":"concepts/","page":"Concepts and Architecture","title":"Concepts and Architecture","text":"[2] Bai, H., Hsu, D., & Lee, W. S. (2014). Integrated perception and planning in the continuous space: A POMDP approach. The International Journal of Robotics Research, 33(9), 1288-1302","category":"page"},{"location":"interfaces/#Spaces-and-Distributions","page":"Spaces and Distributions","title":"Spaces and Distributions","text":"","category":"section"},{"location":"interfaces/","page":"Spaces and Distributions","title":"Spaces and Distributions","text":"Two important components of the definitions of MDPs and POMDPs are spaces, which specify the possible states, actions, and observations in a problem and distributions, which define probability distributions. In order to provide for maximum flexibility spaces and distributions may be of any type (i.e. there are no abstract base types). Solvers and simulators will interact with space and distribution types using the functions defined below.","category":"page"},{"location":"interfaces/#space-interface","page":"Spaces and Distributions","title":"Spaces","text":"","category":"section"},{"location":"interfaces/","page":"Spaces and Distributions","title":"Spaces and Distributions","text":"A space object should contain the information needed to define the set of all possible states, actions or observations. The implementation will depend on the attributes of the elements. For example, if the space is continuous, the space object may only contain the limits of the continuous range. In the case of a discrete problem, a vector containing all states is appropriate for representing a space.","category":"page"},{"location":"interfaces/","page":"Spaces and Distributions","title":"Spaces and Distributions","text":"The following functions may be called on a space object (Click on a function to read its documentation):","category":"page"},{"location":"interfaces/","page":"Spaces and Distributions","title":"Spaces and Distributions","text":"rand\niterate and the rest of the iteration interface for discrete spaces.","category":"page"},{"location":"interfaces/#Distributions","page":"Spaces and Distributions","title":"Distributions","text":"","category":"section"},{"location":"interfaces/","page":"Spaces and Distributions","title":"Spaces and Distributions","text":"A distribution object represents a probability distribution.","category":"page"},{"location":"interfaces/","page":"Spaces and Distributions","title":"Spaces and Distributions","text":"The following functions may be called on a distribution object (Click on a function to read its documentation):","category":"page"},{"location":"interfaces/","page":"Spaces and Distributions","title":"Spaces and Distributions","text":"rand([rng,] d) [1]\nsupport\npdf\nmode\nmean","category":"page"},{"location":"interfaces/","page":"Spaces and Distributions","title":"Spaces and Distributions","text":"You can find some useful pre-made distribution objects in Distributions.jl or POMDPModelTools.jl.","category":"page"},{"location":"interfaces/","page":"Spaces and Distributions","title":"Spaces and Distributions","text":"[1]: Distributions should support both rand(rng::AbstractRNG, d) and rand(d). The recommended way to do this is by implmenting Base.rand(rng::AbstractRNG, s::Random.SamplerTrivial{<:YourDistribution}) from the julia rand interface.","category":"page"},{"location":"def_updater/#Defining-a-Belief-Updater","page":"Defining a Belief Updater","title":"Defining a Belief Updater","text":"","category":"section"},{"location":"def_updater/","page":"Defining a Belief Updater","title":"Defining a Belief Updater","text":"In this section we list the requirements for defining a belief updater. For a description of what a belief updater is, see Concepts and Architecture - Beliefs and Updaters. Typically a belief updater will have an associated belief type, and may be closely tied to a particular policy/planner.","category":"page"},{"location":"def_updater/#Defining-a-Belief-Type","page":"Defining a Belief Updater","title":"Defining a Belief Type","text":"","category":"section"},{"location":"def_updater/","page":"Defining a Belief Updater","title":"Defining a Belief Updater","text":"A belief object should contain all of the information needed for the next belief update and for the policy to make a decision. The belief type could be a pre-defined type such as a distribution from Distributions.jl or DiscreteBelief or SparseCat from POMDPModelTools.jl, or it could be a custom type.","category":"page"},{"location":"def_updater/","page":"Defining a Belief Updater","title":"Defining a Belief Updater","text":"Often, but not always, the belief will represent a probability distribution. In this case, the functions in the distribution interface should be implemented if possible. Implementing these functions will make the belief usable with many of the policies and planners in the POMDPs.jl ecosystem, and will make it easy for others to convert between beliefs and to interpret what a belief means.","category":"page"},{"location":"def_updater/#Histories-associated-with-a-belief","page":"Defining a Belief Updater","title":"Histories associated with a belief","text":"","category":"section"},{"location":"def_updater/","page":"Defining a Belief Updater","title":"Defining a Belief Updater","text":"If a complete or partial record of the action-observation history leading up to a belief is available, it is often helpful to give access to this by implementing the history or currentobs functions (see the docstrings for more details). This is especially useful if a problem-writer wants to implement a belief- or observation-dependent action space. Belief type implementers need only implement history, and currentobs will automatically be provided, though sometimes it is more convenient to implement currentobs directly.","category":"page"},{"location":"def_updater/#Defining-an-Updater","page":"Defining a Belief Updater","title":"Defining an Updater","text":"","category":"section"},{"location":"def_updater/","page":"Defining a Belief Updater","title":"Defining a Belief Updater","text":"To create an updater, one should define a subtype of the Updater abstract type and implement two methods, one to create the initial belief from the problem's initial state distribution and one to perform a belief update:","category":"page"},{"location":"def_updater/","page":"Defining a Belief Updater","title":"Defining a Belief Updater","text":"initialize_belief(updater, d) creates a belief from state distribution d appropriate to use with the updater. To extract information from d, use the functions from the distribution interface.\nupdate(updater, b, a, o) returns an updated belief given belief b, action a, and observation o. One can usually expect b to be the same type returned by initialize_belief because a careful user will always call initialize_belief before update, but it would also be reasonable to implement update for b of a different type if it is desirable to handle multiple belief types.","category":"page"},{"location":"def_updater/#Example:-History-Updater","page":"Defining a Belief Updater","title":"Example: History Updater","text":"","category":"section"},{"location":"def_updater/","page":"Defining a Belief Updater","title":"Defining a Belief Updater","text":"One trivial type of belief would be the action-observation history, a list containing the initial state distribution and every action taken and observation received. The history contains all of the information received up to the current time, but it is not usually very useful because most policies make decisions based on a state probability distribution. Here the belief type is simply the built in Vector{Any}, so we need only create the updater and write update and initialize_belief. Normally, update would contain belief update probability calculations, but in this example, we simply append the action and observation to the history.","category":"page"},{"location":"def_updater/","page":"Defining a Belief Updater","title":"Defining a Belief Updater","text":"(Note that this example is designed for readability rather than efficiency.)","category":"page"},{"location":"def_updater/","page":"Defining a Belief Updater","title":"Defining a Belief Updater","text":"import POMDPs\n\nstruct HistoryUpdater <: POMDPs.Updater end\n\ninitialize_belief(up::HistoryUpdater, d) = Any[d]\n\nfunction POMDPs.update(up::HistoryUpdater, b, a, o)\n    bp = copy(b)\n    push!(bp, a)\n    push!(bp, o)\n    return bp\nend","category":"page"},{"location":"def_updater/","page":"Defining a Belief Updater","title":"Defining a Belief Updater","text":"At each step, the history starts with the original distribution, then contains all the actions and observations received up to that point. The example below shows this for the crying baby problem (observations are true/false for crying and actions are true/false for feeding).","category":"page"},{"location":"def_updater/","page":"Defining a Belief Updater","title":"Defining a Belief Updater","text":"using POMDPPolicies\nusing POMDPSimulators\nusing POMDPModels\nusing Random\n\npomdp = BabyPOMDP()\npolicy = RandomPolicy(pomdp, rng=MersenneTwister(1))\nup = HistoryUpdater()\n\n# within stepthrough initialize_belief is called on the initial state distribution of the pomdp, then update is called at each step.\nfor b in stepthrough(pomdp, policy, up, \"b\", rng=MersenneTwister(2), max_steps=5)\n    @show b\nend\n\n# output\n\nb = Any[POMDPModels.BoolDistribution(0.0)]\nb = Any[POMDPModels.BoolDistribution(0.0), false, false]\nb = Any[POMDPModels.BoolDistribution(0.0), false, false, false, false]\nb = Any[POMDPModels.BoolDistribution(0.0), false, false, false, false, true, false]\nb = Any[POMDPModels.BoolDistribution(0.0), false, false, false, false, true, false, true, false]","category":"page"},{"location":"faq/#Frequently-Asked-Questions-(FAQ)","page":"Frequently Asked Questions (FAQ)","title":"Frequently Asked Questions (FAQ)","text":"","category":"section"},{"location":"faq/#How-do-I-save-my-policies?","page":"Frequently Asked Questions (FAQ)","title":"How do I save my policies?","text":"","category":"section"},{"location":"faq/","page":"Frequently Asked Questions (FAQ)","title":"Frequently Asked Questions (FAQ)","text":"We recommend using JLD2 to save the whole policy object:","category":"page"},{"location":"faq/","page":"Frequently Asked Questions (FAQ)","title":"Frequently Asked Questions (FAQ)","text":"using JLD2\nsave(\"my_policy.jld2\", \"policy\", policy)","category":"page"},{"location":"faq/#Why-isn't-the-solver-working?","page":"Frequently Asked Questions (FAQ)","title":"Why isn't the solver working?","text":"","category":"section"},{"location":"faq/","page":"Frequently Asked Questions (FAQ)","title":"Frequently Asked Questions (FAQ)","text":"There could be a number of things that are going wrong. Remeber, POMDPs can be fairly hard to work with, but don't panic.  If you have a discrete POMDP or MDP and you're using a solver that requires the explicit transition probabilities (you've implemented a pdf function), the first thing to try is make sure that your probability masses sum up to unity.  We've provide some tools in POMDPToolbox that can check this for you. If you have a POMDP called pomdp, you can run the checks by doing the following:","category":"page"},{"location":"faq/","page":"Frequently Asked Questions (FAQ)","title":"Frequently Asked Questions (FAQ)","text":"using POMDPTesting\nprobability_check(pomdp) # checks that both observation and transition functions give probs that sum to unity\nobs_prob_consistency_check(pomdp) # checks the observation probabilities\ntrans_prob_consistency_check(pomdp) # check the transition probabilities","category":"page"},{"location":"faq/","page":"Frequently Asked Questions (FAQ)","title":"Frequently Asked Questions (FAQ)","text":"If these throw an error, you may need to fix your transition or observation functions. ","category":"page"},{"location":"faq/#Why-do-I-need-to-put-type-assertions-pomdp::POMDP-into-the-function-signature?","page":"Frequently Asked Questions (FAQ)","title":"Why do I need to put type assertions pomdp::POMDP into the function signature?","text":"","category":"section"},{"location":"faq/","page":"Frequently Asked Questions (FAQ)","title":"Frequently Asked Questions (FAQ)","text":"Specifying the type in your function signature allows Julia to call the appropriate function when your custom type is passed into it. For example if a POMDPs.jl solver calls states on the POMDP that you passed into it, the correct states function will only get dispatched if you specified that the states function you wrote works with your POMDP type. Because Julia supports multiple-dispatch, these type assertion are a way for doing object-oriented programming in Julia.","category":"page"},{"location":"faq/#Why-are-all-the-solvers-in-separate-modules?","page":"Frequently Asked Questions (FAQ)","title":"Why are all the solvers in separate modules?","text":"","category":"section"},{"location":"faq/","page":"Frequently Asked Questions (FAQ)","title":"Frequently Asked Questions (FAQ)","text":"We did not put all the solvers and support tools into POMDPs.jl, because we wanted POMDPs.jl to be a lightweight interface package. This has a number of advantages. The first is that if a user only wants to use a few solvers from the JuliaPOMDP organization, they do not have to install all the other solvers and their dependencies. The second advantage is that people who are not directly part of the JuliaPOMDP organization can write their own solvers without going into the source code of other solvers. This makes the framework easier to adopt and to extend.","category":"page"},{"location":"faq/#How-can-I-implement-terminal-actions?","page":"Frequently Asked Questions (FAQ)","title":"How can I implement terminal actions?","text":"","category":"section"},{"location":"faq/","page":"Frequently Asked Questions (FAQ)","title":"Frequently Asked Questions (FAQ)","text":"Terminal actions are actions that cause the MDP to terminate without generating a new state. POMDPs.jl handles terminal conditions via the isterminal function on states, and does not directly support terminal actions. If your MDP has a terminal action, you need to implement the model functions accordingly to generate a terminal state. In both generative and explicit cases, you will need some dummy state, say spt, that can be recognized as terminal by the isterminal function. One way to do this is to give spt a state value that is out of bounds (e.g. a vector of NaNs or -1s) and then check for that in isterminal, so that this does not clash with any conventional termination conditions on the state.","category":"page"},{"location":"faq/","page":"Frequently Asked Questions (FAQ)","title":"Frequently Asked Questions (FAQ)","text":"If a terminal action is taken, regardless of current state, the transition function should return a distribution with only one next state, spt, with probability 1.0. In the generative case, the new state generated should be spt. The reward function or the r in generate_sr can be set according to the cost of the terminal action.","category":"page"},{"location":"faq/#Why-are-there-two-versions-of-reward?","page":"Frequently Asked Questions (FAQ)","title":"Why are there two versions of reward?","text":"","category":"section"},{"location":"faq/","page":"Frequently Asked Questions (FAQ)","title":"Frequently Asked Questions (FAQ)","text":"Both reward(m, s, a) and reward(m, s, a, sp) are included because of these two facts:","category":"page"},{"location":"faq/","page":"Frequently Asked Questions (FAQ)","title":"Frequently Asked Questions (FAQ)","text":"Some non-native solvers use reward(m, s, a)\nSometimes the reward depends on s and sp.","category":"page"},{"location":"faq/","page":"Frequently Asked Questions (FAQ)","title":"Frequently Asked Questions (FAQ)","text":"It is reasonable to implement both as long as the (s, a) version is the expectation of the (s, a, s') version (see below).","category":"page"},{"location":"faq/#How-do-I-implement-reward(m,-s,-a)-if-the-reward-depends-on-the-next-state?","page":"Frequently Asked Questions (FAQ)","title":"How do I implement reward(m, s, a) if the reward depends on the next state?","text":"","category":"section"},{"location":"faq/","page":"Frequently Asked Questions (FAQ)","title":"Frequently Asked Questions (FAQ)","text":"The solvers that require reward(m, s, a) only work on problems with finite state and action spaces. In this case, you can define reward(m, s, a) in terms of reward(m, s, a, sp) with the following code:","category":"page"},{"location":"faq/","page":"Frequently Asked Questions (FAQ)","title":"Frequently Asked Questions (FAQ)","text":"const rdict = Dict{Tuple{S,A}, Float64}()\n\nfor s in states(m)\n  for a in actions(m)\n    r = 0.0\n    td = transition(m, s, a) # transition distribution for s, a\n    for sp in support(td)\n      r += pdf(td, sp)*reward(m, s, a, sp)\n    end\n    rdict[(s, a)] = r\n  end\nend\n\nPOMDPs.reward(m, s, a) = rdict[(s, a)]","category":"page"},{"location":"api/#API-Documentation","page":"API Documentation","title":"API Documentation","text":"","category":"section"},{"location":"api/","page":"API Documentation","title":"API Documentation","text":"Docstrings for POMDPs.jl interface members can be accessed through Julia's built-in documentation system or in the list below.","category":"page"},{"location":"api/","page":"API Documentation","title":"API Documentation","text":"CurrentModule = POMDPs","category":"page"},{"location":"api/#Contents","page":"API Documentation","title":"Contents","text":"","category":"section"},{"location":"api/","page":"API Documentation","title":"API Documentation","text":"Pages = [\"api.md\"]","category":"page"},{"location":"api/#Index","page":"API Documentation","title":"Index","text":"","category":"section"},{"location":"api/","page":"API Documentation","title":"API Documentation","text":"Pages = [\"api.md\"]","category":"page"},{"location":"api/#Types","page":"API Documentation","title":"Types","text":"","category":"section"},{"location":"api/","page":"API Documentation","title":"API Documentation","text":"POMDP\nMDP\nSolver\nPolicy\nUpdater","category":"page"},{"location":"api/#POMDPs.POMDP","page":"API Documentation","title":"POMDPs.POMDP","text":"POMDP{S,A,O}\n\nAbstract base type for a partially observable Markov decision process.\n\nS: state type\nA: action type\nO: observation type\n\n\n\n\n\n","category":"type"},{"location":"api/#POMDPs.MDP","page":"API Documentation","title":"POMDPs.MDP","text":"MDP{S,A}\n\nAbstract base type for a fully observable Markov decision process.\n\nS: state type\nA: action type\n\n\n\n\n\n","category":"type"},{"location":"api/#POMDPs.Solver","page":"API Documentation","title":"POMDPs.Solver","text":"Base type for an MDP/POMDP solver\n\n\n\n\n\n","category":"type"},{"location":"api/#POMDPs.Policy","page":"API Documentation","title":"POMDPs.Policy","text":"Base type for a policy (a map from every possible belief, or more abstract policy state, to an optimal or suboptimal action)\n\n\n\n\n\n","category":"type"},{"location":"api/#POMDPs.Updater","page":"API Documentation","title":"POMDPs.Updater","text":"Abstract type for an object that defines how the belief should be updated\n\nA belief is a general construct that represents the knowledge an agent has about the state of the system. This can be a probability distribution, an action observation history or a more general representation.\n\n\n\n\n\n","category":"type"},{"location":"api/#Model-Functions","page":"API Documentation","title":"Model Functions","text":"","category":"section"},{"location":"api/#Dynamics","page":"API Documentation","title":"Dynamics","text":"","category":"section"},{"location":"api/","page":"API Documentation","title":"API Documentation","text":"transition\nobservation\nreward\ngen\n@gen","category":"page"},{"location":"api/#POMDPs.transition","page":"API Documentation","title":"POMDPs.transition","text":"transition(m::POMDP, state, action)\ntransition(m::MDP, state, action)\n\nReturn the transition distribution from the current state-action pair.\n\nIf it is difficult to define the probability density or mass function explicitly, consider using POMDPModelTools.ImplicitDistribution to define a generative model.\n\n\n\n\n\n","category":"function"},{"location":"api/#POMDPs.observation","page":"API Documentation","title":"POMDPs.observation","text":"observation(m::POMDP, statep)\nobservation(m::POMDP, action, statep)\nobservation(m::POMDP, state, action, statep)\n\nReturn the observation distribution. You need only define the method with the fewest arguments needed to determine the observation distribution.\n\nIf it is difficult to define the probability density or mass function explicitly, consider using POMDPModelTools.ImplicitDistribution to define a generative model.\n\nExample\n\nusing POMDPModelTools # for SparseCat\n\nstruct MyPOMDP <: POMDP{Int, Int, Int} end\n\nobservation(p::MyPOMDP, sp::Int) = SparseCat([sp-1, sp, sp+1], [0.1, 0.8, 0.1])\n\n\n\n\n\n","category":"function"},{"location":"api/#POMDPs.reward","page":"API Documentation","title":"POMDPs.reward","text":"reward(m::POMDP, s, a)\nreward(m::MDP, s, a)\n\nReturn the immediate reward for the s-a pair.\n\nreward(m::POMDP, s, a, sp)\nreward(m::MDP, s, a, sp)\n\nReturn the immediate reward for the s-a-s' triple\n\nreward(m::POMDP, s, a, sp, o)\n\nReturn the immediate reward for the s-a-s'-o quad\n\nFor some problems, it is easier to express reward(m, s, a, sp) or reward(m, s, a, sp, o), than reward(m, s, a), but some solvers, e.g. SARSOP, can only use reward(m, s, a). Both can be implemented for a problem, but when reward(m, s, a) is implemented, it should be consistent with reward(m, s, a, sp[, o]), that is, it should be the expected value over all destination states and observations.\n\n\n\n\n\n","category":"function"},{"location":"api/#POMDPs.gen","page":"API Documentation","title":"POMDPs.gen","text":"gen(m::Union{MDP,POMDP}, s, a, rng::AbstractRNG)\n\nFunction for implementing the entire MDP/POMDP generative model by returning a NamedTuple.\n\nSolver and simulator writers should use the @gen macro to call a generative model.\n\nArguments\n\nm: an MDP or POMDP model\ns: the current state\na: the action\nrng: a random number generator (Typically a MersenneTwister)\n\nReturn\n\nThe function should return a NamedTuple. With a subset of following entries:\n\nMDP\n\nsp: the next state\nr: the reward for the step\ninfo: extra debugging information, typically in an associative container like a NamedTuple\n\nPOMDP\n\nsp: the next state\no: the observation\nr: the reward for the step\ninfo: extra debugging information, typically in an associative container like a NamedTuple\n\nSome elements can be left out. For instance if o is left out of the return, the problem-writer can also implement observation and POMDPs.jl will automatically use it when needed.\n\nExample\n\nstruct LQRMDP <: MDP{Float64, Float64} end\n\nPOMDPs.gen(m::LQRMDP, s, a, rng) = (sp = s + a + randn(rng), r = -s^2 - a^2)\n\n\n\n\n\n","category":"function"},{"location":"api/#POMDPs.@gen","page":"API Documentation","title":"POMDPs.@gen","text":"@gen(X)(m, s, a)\n@gen(X)(m, s, a, rng::AbstractRNG)\n\nCall the generative model for a (PO)MDP m; Sample values from several nodes in the dynamic decision network. X is one or more symbols indicating which nodes to output.\n\nSolvers and simulators should usually call this rather than the gen function. Problem writers should implement methods of the gen function.\n\nArguments\n\nm: an MDP or POMDP model\ns: the current state\na: the action\nrng: a random number generator (Typically a MersenneTwister)\n\nReturn\n\nIf X, is a symbol, return a value sample from the corresponding node. If X is several symbols, return a Tuple of values sampled from the specified nodes.\n\nExamples\n\nLet m be an MDP or POMDP, s be a state of m, a be an action of m, and rng be an AbstractRNG.\n\n@gen(:sp, :r)(m, s, a, rng) returns a Tuple containing the next state and reward.\n@gen(:sp, :o, :r)(m, s, a, rng) returns a Tuple containing the next state, observation, and reward.\n@gen(:sp)(m, s, a, rng) returns the next state.\n\n\n\n\n\n","category":"macro"},{"location":"api/#Static-Properties","page":"API Documentation","title":"Static Properties","text":"","category":"section"},{"location":"api/","page":"API Documentation","title":"API Documentation","text":"states\nactions\nobservations\nisterminal\ndiscount\ninitialstate\ninitialobs\nstateindex\nactionindex\nobsindex\nconvert_s\nconvert_a\nconvert_o","category":"page"},{"location":"api/#POMDPs.states","page":"API Documentation","title":"POMDPs.states","text":"states(problem::POMDP)\nstates(problem::MDP)\n\nReturns the complete state space of a POMDP. \n\n\n\n\n\n","category":"function"},{"location":"api/#POMDPs.actions","page":"API Documentation","title":"POMDPs.actions","text":"actions(m::Union{MDP,POMDP})\n\nReturns the entire action space of a (PO)MDP.\n\n\n\nactions(m::Union{MDP,POMDP}, s)\n\nReturn the actions that can be taken from state s.\n\n\n\nactions(m::POMDP, b)\n\nReturn the actions that can be taken from belief b.\n\nTo implement an observation-dependent action space, use currentobs(b) to get the observation associated with belief b within the implementation of actions(m, b).\n\n\n\n\n\n","category":"function"},{"location":"api/#POMDPs.observations","page":"API Documentation","title":"POMDPs.observations","text":"observations(problem::POMDP)\n\nReturn the entire observation space.\n\n\n\n\n\n","category":"function"},{"location":"api/#POMDPs.isterminal","page":"API Documentation","title":"POMDPs.isterminal","text":"isterminal(m::Union{MDP,POMDP}, s)\n\nCheck if state s is terminal.\n\nIf a state is terminal, no actions will be taken in it and no additional rewards will be accumulated. Thus, the value function at such a state is, by definition, zero.\n\n\n\n\n\n","category":"function"},{"location":"api/#POMDPs.discount","page":"API Documentation","title":"POMDPs.discount","text":"discount(m::POMDP)\ndiscount(m::MDP)\n\nReturn the discount factor for the problem.\n\n\n\n\n\n","category":"function"},{"location":"api/#POMDPs.initialstate","page":"API Documentation","title":"POMDPs.initialstate","text":"initialstate(m::Union{POMDP,MDP})\n\nReturn a distribution of initial states for (PO)MDP m.\n\nIf it is difficult to define the probability density or mass function explicitly, consider using POMDPModelTools.ImplicitDistribution to define a model for sampling.\n\n\n\n\n\n","category":"function"},{"location":"api/#POMDPs.initialobs","page":"API Documentation","title":"POMDPs.initialobs","text":"initialobs(m::POMDP, s)\n\nReturn a distribution of initial observations for POMDP m and state s.\n\nIf it is difficult to define the probability density or mass function explicitly, consider using POMDPModelTools.ImplicitDistribution to define a model for sampling.\n\nThis function is only used in cases where the policy expects an initial observation rather than an initial belief, e.g. in a reinforcement learning setting. It is not used in a standard POMDP simulation.\n\n\n\n\n\n","category":"function"},{"location":"api/#POMDPs.stateindex","page":"API Documentation","title":"POMDPs.stateindex","text":"stateindex(problem::POMDP, s)\nstateindex(problem::MDP, s)\n\nReturn the integer index of state s. Used for discrete models only.\n\n\n\n\n\n","category":"function"},{"location":"api/#POMDPs.actionindex","page":"API Documentation","title":"POMDPs.actionindex","text":"actionindex(problem::POMDP, a)\nactionindex(problem::MDP, a)\n\nReturn the integer index of action a. Used for discrete models only.\n\n\n\n\n\n","category":"function"},{"location":"api/#POMDPs.obsindex","page":"API Documentation","title":"POMDPs.obsindex","text":"obsindex(problem::POMDP, o)\n\nReturn the integer index of observation o. Used for discrete models only.\n\n\n\n\n\n","category":"function"},{"location":"api/#POMDPs.convert_s","page":"API Documentation","title":"POMDPs.convert_s","text":"convert_s(::Type{V}, s, problem::Union{MDP,POMDP}) where V<:AbstractArray\nconvert_s(::Type{S}, vec::V, problem::Union{MDP,POMDP}) where {S,V<:AbstractArray}\n\nConvert a state to vectorized form or vice versa.\n\n\n\n\n\n","category":"function"},{"location":"api/#POMDPs.convert_a","page":"API Documentation","title":"POMDPs.convert_a","text":"convert_a(::Type{V}, a, problem::Union{MDP,POMDP}) where V<:AbstractArray\nconvert_a(::Type{A}, vec::V, problem::Union{MDP,POMDP}) where {A,V<:AbstractArray}\n\nConvert an action to vectorized form or vice versa.\n\n\n\n\n\n","category":"function"},{"location":"api/#POMDPs.convert_o","page":"API Documentation","title":"POMDPs.convert_o","text":"convert_o(::Type{V}, o, problem::Union{MDP,POMDP}) where V<:AbstractArray\nconvert_o(::Type{O}, vec::V, problem::Union{MDP,POMDP}) where {O,V<:AbstractArray}\n\nConvert an observation to vectorized form or vice versa.\n\n\n\n\n\n","category":"function"},{"location":"api/#Distributions-and-Spaces","page":"API Documentation","title":"Distributions and Spaces","text":"","category":"section"},{"location":"api/","page":"API Documentation","title":"API Documentation","text":"rand\npdf\nmode\nmean\nsupport","category":"page"},{"location":"api/#Base.rand","page":"API Documentation","title":"Base.rand","text":"rand(rng::AbstractRNG, d::Any)\n\nReturn a random element from distribution or space d.\n\nIf d is a state or transition distribution, the sample will be a state; if d is an action distribution, the sample will be an action or if d is an observation distribution, the sample will be an observation.\n\n\n\n\n\n","category":"function"},{"location":"api/#Distributions.pdf","page":"API Documentation","title":"Distributions.pdf","text":"pdf(d::Any, x::Any)\n\nEvaluate the probability density of distribution d at sample x.\n\n\n\n\n\n","category":"function"},{"location":"api/#StatsBase.mode","page":"API Documentation","title":"StatsBase.mode","text":"mode(d::Any)\n\nReturn the most likely value in a distribution d.\n\n\n\n\n\n","category":"function"},{"location":"api/#Statistics.mean","page":"API Documentation","title":"Statistics.mean","text":"mean(d::Any)\n\nReturn the mean of a distribution d.\n\n\n\n\n\n","category":"function"},{"location":"api/#Distributions.support","page":"API Documentation","title":"Distributions.support","text":"support(d::Any)\n\nReturn an iterable object containing the possible values that can be sampled from distribution d. Values with zero probability may be skipped.\n\n\n\n\n\n","category":"function"},{"location":"api/#Belief-Functions","page":"API Documentation","title":"Belief Functions","text":"","category":"section"},{"location":"api/","page":"API Documentation","title":"API Documentation","text":"update\ninitialize_belief\nhistory\ncurrentobs","category":"page"},{"location":"api/#POMDPs.update","page":"API Documentation","title":"POMDPs.update","text":"update(updater::Updater, belief_old, action, observation)\n\nReturn a new instance of an updated belief given belief_old and the latest action and observation.\n\n\n\n\n\n","category":"function"},{"location":"api/#POMDPs.initialize_belief","page":"API Documentation","title":"POMDPs.initialize_belief","text":"initialize_belief(updater::Updater,\n                     state_distribution::Any)\ninitialize_belief(updater::Updater, belief::Any)\n\nReturns a belief that can be updated using updater that has similar distribution to state_distribution or belief.\n\nThe conversion may be lossy. This function is also idempotent, i.e. there is a default implementation that passes the belief through when it is already the correct type: initialize_belief(updater::Updater, belief) = belief\n\n\n\n\n\n","category":"function"},{"location":"api/#POMDPs.history","page":"API Documentation","title":"POMDPs.history","text":"history(b)\n\nReturn the action-observation history associated with belief b.\n\nThe history should be an AbstractVector, Tuple, (or similar object that supports indexing with end) full of NamedTuples with keys :a and :o, i.e. history(b)[end][:a] should be the last action taken leading up to b, and history(b)[end][:o] should be the last observation received.\n\nIt is acceptable to return only part of the history if that is all that is available, but it should always end with the current observation. For example, it would be acceptable to return a structure containing only the last three observations in a length 3 Vector{NamedTuple{(:o,),Tuple{O}}.\n\n\n\n\n\n","category":"function"},{"location":"api/#POMDPs.currentobs","page":"API Documentation","title":"POMDPs.currentobs","text":"currentobs(b)\n\nReturn the latest observation associated with belief b.\n\nIf a solver or updater implements history(b) for a belief type, currentobs has a default implementation.\n\n\n\n\n\n","category":"function"},{"location":"api/#Policy-and-Solver-Functions","page":"API Documentation","title":"Policy and Solver Functions","text":"","category":"section"},{"location":"api/","page":"API Documentation","title":"API Documentation","text":"solve\nupdater\naction\nvalue","category":"page"},{"location":"api/#POMDPs.solve","page":"API Documentation","title":"POMDPs.solve","text":"solve(solver::Solver, problem::POMDP)\n\nSolves the POMDP using method associated with solver, and returns a policy.\n\n\n\n\n\n","category":"function"},{"location":"api/#POMDPs.updater","page":"API Documentation","title":"POMDPs.updater","text":"updater(policy::Policy)\n\nReturns a default Updater appropriate for a belief type that policy p can use\n\n\n\n\n\n","category":"function"},{"location":"api/#POMDPs.action","page":"API Documentation","title":"POMDPs.action","text":"action(policy::Policy, x)\n\nReturns the action that the policy deems best for the current state or belief, x.\n\nx is a generalized information state - can be a state in an MDP, a distribution in POMDP, or another specialized policy-dependent representation of the information needed to choose an action.\n\n\n\n\n\n","category":"function"},{"location":"api/#POMDPs.value","page":"API Documentation","title":"POMDPs.value","text":"value(p::Policy, s)\nvalue(p::Policy, s, a)\n\nReturns the utility value from policy p given the state (or belief), or state-action (or belief-action) pair.\n\nThe state-action version is commonly referred to as the Q-value.\n\n\n\n\n\n","category":"function"},{"location":"api/#Simulator","page":"API Documentation","title":"Simulator","text":"","category":"section"},{"location":"api/","page":"API Documentation","title":"API Documentation","text":"Simulator\nsimulate","category":"page"},{"location":"api/#POMDPs.Simulator","page":"API Documentation","title":"POMDPs.Simulator","text":"Base type for an object defining how simulations should be carried out.\n\n\n\n\n\n","category":"type"},{"location":"api/#POMDPs.simulate","page":"API Documentation","title":"POMDPs.simulate","text":"simulate(sim::Simulator, m::POMDP, p::Policy, u::Updater=updater(p), b0=initialstate(m), s0=rand(b0))\nsimulate(sim::Simulator, m::MDP, p::Policy, s0=rand(initialstate(m)))\n\nRun a simulation using the specified policy.\n\nThe return type is flexible and depends on the simulator. Simulations should adhere to the Simulation Standard.\n\n\n\n\n\n","category":"function"},{"location":"api/#Other","page":"API Documentation","title":"Other","text":"","category":"section"},{"location":"api/","page":"API Documentation","title":"API Documentation","text":"The following functions are not part of the API for specifying and solving POMDPs, but are included in the package.","category":"page"},{"location":"api/#Type-Inference","page":"API Documentation","title":"Type Inference","text":"","category":"section"},{"location":"api/","page":"API Documentation","title":"API Documentation","text":"statetype\nactiontype\nobstype","category":"page"},{"location":"api/#POMDPs.statetype","page":"API Documentation","title":"POMDPs.statetype","text":"statetype(t::Type)\nstatetype(p::Union{POMDP,MDP})\n\nReturn the state type for a problem type (the S in POMDP{S,A,O}).\n\ntype A <: POMDP{Int, Bool, Bool} end\n\nstatetype(A) # returns Int\n\n\n\n\n\n","category":"function"},{"location":"api/#POMDPs.actiontype","page":"API Documentation","title":"POMDPs.actiontype","text":"actiontype(t::Type)\nactiontype(p::Union{POMDP,MDP})\n\nReturn the state type for a problem type (the S in POMDP{S,A,O}).\n\ntype A <: POMDP{Bool, Int, Bool} end\n\nactiontype(A) # returns Int\n\n\n\n\n\n","category":"function"},{"location":"api/#POMDPs.obstype","page":"API Documentation","title":"POMDPs.obstype","text":"obstype(t::Type)\n\nReturn the state type for a problem type (the S in POMDP{S,A,O}).\n\ntype A <: POMDP{Bool, Bool, Int} end\n\nobstype(A) # returns Int\n\n\n\n\n\n","category":"function"},{"location":"api/#Utility-Tools","page":"API Documentation","title":"Utility Tools","text":"","category":"section"},{"location":"api/","page":"API Documentation","title":"API Documentation","text":"add_registry","category":"page"},{"location":"run_simulation/#Running-Simulations","page":"Running Simulations","title":"Running Simulations","text":"","category":"section"},{"location":"run_simulation/","page":"Running Simulations","title":"Running Simulations","text":"Running a simulation consists of two steps, creating a simulator and calling the simulate function. For example, given a POMDP or MDP model m, and a policy p, one can use the Rollout Simulator from the POMDPSimulators package to find the accumulated discounted reward from a single simulated trajectory as follows:","category":"page"},{"location":"run_simulation/","page":"Running Simulations","title":"Running Simulations","text":"sim = RolloutSimulator()\nr = simulate(sim, m, p)","category":"page"},{"location":"run_simulation/","page":"Running Simulations","title":"Running Simulations","text":"More inputs, such as a belief updater, initial state, initial belief, etc. may be specified as arguments to simulate. See the docstring for simulate and the appropriate \"Input\" sections in the Simulation Standard page for more information.","category":"page"},{"location":"run_simulation/","page":"Running Simulations","title":"Running Simulations","text":"More examples can be found in the POMDPExamples package. A variety of simulators that return more information and interact in different ways can be found in the POMDPSimulators package.","category":"page"},{"location":"simulation/#Simulation-Standard","page":"Simulation Standard","title":"Simulation Standard","text":"","category":"section"},{"location":"simulation/","page":"Simulation Standard","title":"Simulation Standard","text":"Important note: In most cases, users need not implement their own simulators. Several simulators that are compatible with the standard in this document are implemented in the POMDPSimulators package and allow interaction from a variety of perspectives. Moreover RLInterface.jl provides an OpenAI Gym style environment interface to interact with environments that is more flexible in some cases.","category":"page"},{"location":"simulation/","page":"Simulation Standard","title":"Simulation Standard","text":"In order to maintain consistency across the POMDPs.jl ecosystem, this page defines a standard for how simulations should be conducted. All simulators should be consistent with this page, and, if solvers are attempting to find an optimal POMDP policy, they should optimize the expected value of r_total below. In particular, this page should be consulted when questions about how less-obvious concepts like terminal states are handled.","category":"page"},{"location":"simulation/#POMDP-Simulation","page":"Simulation Standard","title":"POMDP Simulation","text":"","category":"section"},{"location":"simulation/#Inputs","page":"Simulation Standard","title":"Inputs","text":"","category":"section"},{"location":"simulation/","page":"Simulation Standard","title":"Simulation Standard","text":"In general, POMDP simulations take up to 5 inputs (see also the simulate docstring):","category":"page"},{"location":"simulation/","page":"Simulation Standard","title":"Simulation Standard","text":"pomdp::POMDP: pomdp model object (see POMDPs and MDPs)\npolicy::Policy: policy (see Solvers and Policies)\nup::Updater: belief updater (see Beliefs and Updaters)\nisd: initial state distribution\ns: initial state","category":"page"},{"location":"simulation/","page":"Simulation Standard","title":"Simulation Standard","text":"The last three of these inputs are optional. If they are not explicitly provided, they should be inferred using the following POMDPs.jl functions:","category":"page"},{"location":"simulation/","page":"Simulation Standard","title":"Simulation Standard","text":"up =updater(policy)\nisd =initialstate(pomdp)\ns =rand(initialstate(pomdp))","category":"page"},{"location":"simulation/#Simulation-Loop","page":"Simulation Standard","title":"Simulation Loop","text":"","category":"section"},{"location":"simulation/","page":"Simulation Standard","title":"Simulation Standard","text":"The main simulation loop is shown below. Note that the isterminal check prevents any actions from being taken and reward from being collected from a terminal state.","category":"page"},{"location":"simulation/","page":"Simulation Standard","title":"Simulation Standard","text":"Before the loop begins, initialize_belief is called to create the belief based on the initial state distribution - this is especially important when the belief is solver specific, such as the finite-state-machine used by MCVI. ","category":"page"},{"location":"simulation/","page":"Simulation Standard","title":"Simulation Standard","text":"b = initialize_belief(up, isd)\n\nr_total = 0.0\nd = 1.0\nwhile !isterminal(pomdp, s)\n    a = action(policy, b)\n    s, o, r = @gen(:sp,:o,:r)(pomdp, s, a)\n    r_total += d*r\n    d *= discount(pomdp)\n    b = update(up, b, a, o)\nend","category":"page"},{"location":"simulation/","page":"Simulation Standard","title":"Simulation Standard","text":"In terms of the explicit interface, the @gen macro above expands to the equivalent of:","category":"page"},{"location":"simulation/","page":"Simulation Standard","title":"Simulation Standard","text":"    sp = rand(transition(pomdp, s, a))\n    o = rand(observation(pomdp, s, a, sp))\n    r = reward(pomdp, s, a, sp, o)\n    s = sp","category":"page"},{"location":"simulation/#MDP-Simulation","page":"Simulation Standard","title":"MDP Simulation","text":"","category":"section"},{"location":"simulation/#Inputs-2","page":"Simulation Standard","title":"Inputs","text":"","category":"section"},{"location":"simulation/","page":"Simulation Standard","title":"Simulation Standard","text":"In general, MDP simulations take up to 3 inputs (see also the simulate docstring):","category":"page"},{"location":"simulation/","page":"Simulation Standard","title":"Simulation Standard","text":"mdp::MDP: mdp model object (see POMDPs and MDPs)\npolicy::Policy: policy (see Solvers and Policies)\ns: initial state","category":"page"},{"location":"simulation/","page":"Simulation Standard","title":"Simulation Standard","text":"The last of these inputs is optional. If the initial state is not explicitly provided, it should be generated using","category":"page"},{"location":"simulation/","page":"Simulation Standard","title":"Simulation Standard","text":"s =initialstate(mdp)","category":"page"},{"location":"simulation/#Simulation-Loop-2","page":"Simulation Standard","title":"Simulation Loop","text":"","category":"section"},{"location":"simulation/","page":"Simulation Standard","title":"Simulation Standard","text":"The main simulation loop is shown below. Note again that the isterminal check prevents any actions from being taken and reward from being collected from a terminal state.","category":"page"},{"location":"simulation/","page":"Simulation Standard","title":"Simulation Standard","text":"r_total = 0.0\ndisc = 1.0\nwhile !isterminal(mdp, s)\n    a = action(policy, s)\n    s, r = @gen(:sp,:r)(mdp, s, a)\n    r_total += d*r\n    d *= discount(mdp)\nend","category":"page"},{"location":"simulation/","page":"Simulation Standard","title":"Simulation Standard","text":"In terms of the explicit interface, the @gen macro above expands to the equivalent of:","category":"page"},{"location":"simulation/","page":"Simulation Standard","title":"Simulation Standard","text":"    sp = rand(transition(pomdp, s, a))\n    r = reward(pomdp, s, a, sp)\n    s = sp","category":"page"},{"location":"static/#static","page":"Defining Static (PO)MDP Properties","title":"Defining Static (PO)MDP Properties","text":"","category":"section"},{"location":"static/","page":"Defining Static (PO)MDP Properties","title":"Defining Static (PO)MDP Properties","text":"The definition of a (PO)MDP includes several static properties, which are defined with the functions listed in this section. This section is an overview, with links to the docstrings for detailed usage information.","category":"page"},{"location":"static/","page":"Defining Static (PO)MDP Properties","title":"Defining Static (PO)MDP Properties","text":"To use most solvers, it is only necessary to implement a few of these functions.","category":"page"},{"location":"static/#Spaces","page":"Defining Static (PO)MDP Properties","title":"Spaces","text":"","category":"section"},{"location":"static/","page":"Defining Static (PO)MDP Properties","title":"Defining Static (PO)MDP Properties","text":"The state, action and observation spaces are defined by the following functions:","category":"page"},{"location":"static/","page":"Defining Static (PO)MDP Properties","title":"Defining Static (PO)MDP Properties","text":"states(pomdp)\nactions(pomdp[, s])\nobservations(pomdp)","category":"page"},{"location":"static/","page":"Defining Static (PO)MDP Properties","title":"Defining Static (PO)MDP Properties","text":"The object returned by these functions should implement part or all of the interface for spaces. For discrete problems, a vector is appropriate.","category":"page"},{"location":"static/","page":"Defining Static (PO)MDP Properties","title":"Defining Static (PO)MDP Properties","text":"It is often important to limit the action space based on the current state, belief, or observation.  This can be accomplished with the actions(m, s) or actions(m, b) function. See Histories associated with a belief and the history and currentobs docstrings for more information.","category":"page"},{"location":"static/#Initial-Distributions","page":"Defining Static (PO)MDP Properties","title":"Initial Distributions","text":"","category":"section"},{"location":"static/","page":"Defining Static (PO)MDP Properties","title":"Defining Static (PO)MDP Properties","text":"initialstate(pomdp) should return the distribution of the initial state, either as an explicit distribution (e.g. a POMDPModelTools.SparseCat) that conforms to the distribution interface or with a POMDPModelTools.ImplicitDistribution to easily specify a function to sample from the space.","category":"page"},{"location":"static/","page":"Defining Static (PO)MDP Properties","title":"Defining Static (PO)MDP Properties","text":"initialobs(pomdp, state) is used to return the distribution of the initial observation in occasional cases where the policy expects an initial observation rather than an initial belief, e.g. in a reinforcement learning setting. It is not used in a standard POMDP simulation.","category":"page"},{"location":"static/#Discount-Factor","page":"Defining Static (PO)MDP Properties","title":"Discount Factor","text":"","category":"section"},{"location":"static/","page":"Defining Static (PO)MDP Properties","title":"Defining Static (PO)MDP Properties","text":"discount(pomdp) should return a number between 0 and 1 to define the discount factor.","category":"page"},{"location":"static/#Terminal-States","page":"Defining Static (PO)MDP Properties","title":"Terminal States","text":"","category":"section"},{"location":"static/","page":"Defining Static (PO)MDP Properties","title":"Defining Static (PO)MDP Properties","text":"If a problem has terminal states, they can be specified using the isterminal function. If a state s is terminal isterminal(pomdp, s) should return true, otherwise it should return false.","category":"page"},{"location":"static/","page":"Defining Static (PO)MDP Properties","title":"Defining Static (PO)MDP Properties","text":"In POMDPs.jl, no actions can be taken from terminal states, and no additional rewards can be collected, thus, the value function for a terminal state is zero. POMDPs.jl does not have a mechanism for defining terminal rewards apart from the reward function, so the problem should be defined so that any terminal rewards are collected as the system transitions into a terminal state.","category":"page"},{"location":"static/#Indexing","page":"Defining Static (PO)MDP Properties","title":"Indexing","text":"","category":"section"},{"location":"static/","page":"Defining Static (PO)MDP Properties","title":"Defining Static (PO)MDP Properties","text":"For discrete problems, some solvers rely on a fast method for finding the index of the states, actions, or observations in an ordered list. These indexing functions can be implemented as","category":"page"},{"location":"static/","page":"Defining Static (PO)MDP Properties","title":"Defining Static (PO)MDP Properties","text":"stateindex(pomdp, s)\nactionindex(pomdp, a)\nobsindex(pomdp, o)","category":"page"},{"location":"static/","page":"Defining Static (PO)MDP Properties","title":"Defining Static (PO)MDP Properties","text":"note: Note\nThe converse mapping (from indices to states) is not part of the POMDPs interface. A solver will typically create a vector containing all the states to define it.","category":"page"},{"location":"static/","page":"Defining Static (PO)MDP Properties","title":"Defining Static (PO)MDP Properties","text":"note: Note\nThere is no requirement that the object returned by the space functions above respect the same ordering as the index functions. The index functions are the sole definition of ordering of the states. The POMDPModelTools package contains convenience functions for constructing a list of states that respects the ordering specified by the index functions. For example, POMDPModelTools.ordered_states returns an AbstractVector of the states in the order specified by stateindex.","category":"page"},{"location":"static/#Conversion-to-vector-types","page":"Defining Static (PO)MDP Properties","title":"Conversion to vector types","text":"","category":"section"},{"location":"static/","page":"Defining Static (PO)MDP Properties","title":"Defining Static (PO)MDP Properties","text":"Some solvers (notably those that involve deep learning) rely on the ability to represent states, actions, and observations as vectors. To define a mapping between vectors and custom problem-specific representations, implement the following functions (see docstring for signature):","category":"page"},{"location":"static/","page":"Defining Static (PO)MDP Properties","title":"Defining Static (PO)MDP Properties","text":"convert_s\nconvert_a\nconvert_o","category":"page"},{"location":"offline_solver/#Example:-Defining-an-offline-solver","page":"Example: Defining an offline solver","title":"Example: Defining an offline solver","text":"","category":"section"},{"location":"offline_solver/","page":"Example: Defining an offline solver","title":"Example: Defining an offline solver","text":"In this example, we will define a simple offline solver that works for both POMDPs and MDPs. In order to focus on the code structure, we will not create an algorithm that finds an optimal policy, but rather a greedy policy, that is, one that optimizes the expected immediate reward. For information on using this solver in a simulation, see Running Simulations.","category":"page"},{"location":"offline_solver/","page":"Example: Defining an offline solver","title":"Example: Defining an offline solver","text":"We begin by creating a solver type. Since there are no adjustable parameters for the solver, it is an empty type, but for a more complex solver, parameters would usually be included as type fields.","category":"page"},{"location":"offline_solver/","page":"Example: Defining an offline solver","title":"Example: Defining an offline solver","text":"using POMDPs\n\nstruct GreedyOfflineSolver <: Solver end","category":"page"},{"location":"offline_solver/","page":"Example: Defining an offline solver","title":"Example: Defining an offline solver","text":"Next, we define the functions that will make the solver work for both MDPs and POMDPs.","category":"page"},{"location":"offline_solver/#MDP-Case","page":"Example: Defining an offline solver","title":"MDP Case","text":"","category":"section"},{"location":"offline_solver/","page":"Example: Defining an offline solver","title":"Example: Defining an offline solver","text":"Finding a greedy policy for an MDP consists of determining the action that has the best reward for each state. First, we create a simple policy object that holds a greedy action for each state.","category":"page"},{"location":"offline_solver/","page":"Example: Defining an offline solver","title":"Example: Defining an offline solver","text":"struct DictPolicy{S,A} <: Policy\n    actions::Dict{S,A}\nend\n\nPOMDPs.action(p::DictPolicy, s) = p.actions[s]","category":"page"},{"location":"offline_solver/","page":"Example: Defining an offline solver","title":"Example: Defining an offline solver","text":"note: Note\nA POMDPPolicies.VectorPolicy could be used here. We include this example to show how to define a custom policy.","category":"page"},{"location":"offline_solver/","page":"Example: Defining an offline solver","title":"Example: Defining an offline solver","text":"The solve function calculates the best greedy action for each state and saves it in a policy. To have the widest possible compatibility with POMDP models, we want to use reward(m, s, a, sp) instead of reward(m, s, a), which means we need to calculate the expectation of the reward over transitions to every possible next state.","category":"page"},{"location":"offline_solver/","page":"Example: Defining an offline solver","title":"Example: Defining an offline solver","text":"function POMDPs.solve(::GreedyOfflineSolver, m::MDP)\n\n    best_actions = Dict{statetype(m), actiontype(m)}()\n\n    for s in states(m)\n        if !isterminal(m, s)\n            best = -Inf\n            for a in actions(m)\n                td = transition(m, s, a)\n                r = 0.0\n                for sp in support(td)\n                    r += pdf(td, sp) * reward(m, s, a, sp)\n                end\n                if r >= best\n                    best_actions[s] = a\n                end\n            end\n        end\n    end\n    \n    return DictPolicy(best_actions)\nend","category":"page"},{"location":"offline_solver/","page":"Example: Defining an offline solver","title":"Example: Defining an offline solver","text":"note: Note\nWe limited this implementation to using basic POMDPs.jl implementation functions, but tools such as POMDPModelTools.StateActionReward, POMDPModelTools.ordered_states, and POMDPModelTools.weighted_iterator could have been used for a more concise and efficient implementation.","category":"page"},{"location":"offline_solver/","page":"Example: Defining an offline solver","title":"Example: Defining an offline solver","text":"We can now verify whether the policy produces the greedy action on an example from POMDPModels:","category":"page"},{"location":"offline_solver/","page":"Example: Defining an offline solver","title":"Example: Defining an offline solver","text":"using POMDPModels\n\ngw = SimpleGridWorld(size=(2,1), rewards=Dict(GWPos(2,1)=>1.0))\npolicy = solve(GreedyOfflineSolver(), gw)\n\naction(policy, GWPos(1,1))\n\n# output\n\n:right","category":"page"},{"location":"offline_solver/#POMDP-Case","page":"Example: Defining an offline solver","title":"POMDP Case","text":"","category":"section"},{"location":"offline_solver/","page":"Example: Defining an offline solver","title":"Example: Defining an offline solver","text":"For a POMDP, the greedy solution is the action that maximizes the expected immediate reward according to the belief. Since there are an infinite number of possible beliefs, the greedy solution for every belief cannot be calculated online. However, the greedy policy can take the form of an alpha vector policy where each action has an associated alpha vector with each entry corresponding to the immediate reward from taking the action in that state.","category":"page"},{"location":"offline_solver/","page":"Example: Defining an offline solver","title":"Example: Defining an offline solver","text":"Again, because a POMDP, may have reward(m, s, a, sp, o) instead of reward(m, s, a), we use the former and calculate the expectation over all next states and observations.","category":"page"},{"location":"offline_solver/","page":"Example: Defining an offline solver","title":"Example: Defining an offline solver","text":"import POMDPPolicies\n\nfunction POMDPs.solve(::GreedyOfflineSolver, m::POMDP)\n\n    alphas = Vector{Float64}[]\n\n    for a in actions(m)\n        alpha = zeros(length(states(m)))\n        for s in states(m)\n            if !isterminal(m, s)\n                r = 0.0\n                td = transition(m, s, a)\n                for sp in support(td)\n                    tp = pdf(td, sp)\n                    od = observation(m, s, a, sp)\n                    for o in support(od)\n                        r += tp * pdf(od, o) * reward(m, s, a, sp, o)\n                    end\n                end\n                alpha[stateindex(m, s)] = r\n            end\n        end\n        push!(alphas, alpha)\n    end\n    \n    return POMDPPolicies.AlphaVectorPolicy(m, alphas, collect(actions(m)))\nend","category":"page"},{"location":"offline_solver/","page":"Example: Defining an offline solver","title":"Example: Defining an offline solver","text":"We can now verify that a policy created by the solver determines the correct greedy actions:","category":"page"},{"location":"offline_solver/","page":"Example: Defining an offline solver","title":"Example: Defining an offline solver","text":"using POMDPModels\nusing POMDPModelTools # for Deterministic, Uniform\n\ntiger = TigerPOMDP()\npolicy = solve(GreedyOfflineSolver(), tiger)\n\n@assert action(policy, Deterministic(TIGER_LEFT)) == TIGER_OPEN_RIGHT\n@assert action(policy, Deterministic(TIGER_RIGHT)) == TIGER_OPEN_LEFT\n@assert action(policy, Uniform(states(tiger))) == TIGER_LISTEN","category":"page"},{"location":"def_solver/#Solvers","page":"Solvers","title":"Solvers","text":"","category":"section"},{"location":"def_solver/","page":"Solvers","title":"Solvers","text":"Defining a solver involves creating or using four pieces of code:","category":"page"},{"location":"def_solver/","page":"Solvers","title":"Solvers","text":"A subtype of Solver that holds the parameters and configuration options for the solver.\nA subtype of Policy that holds all of the data needed to choose actions online.\nA method of solve that takes the solver and a (PO)MDP as arguments, performs all of the offline computations for solving the problem, and returns the policy.\nA method of action that takes in the policy and a state or belief and returns an action.","category":"page"},{"location":"def_solver/","page":"Solvers","title":"Solvers","text":"In many cases, items 2 and 4 can be satisfied with an off-the-shelf solver from POMDPPolicies.jl. POMDPModelTools.jl also contains many tools that are useful for defining solvers in a robust, concise, and readable manner.","category":"page"},{"location":"def_solver/#Online-and-Offline-Solvers","page":"Solvers","title":"Online and Offline Solvers","text":"","category":"section"},{"location":"def_solver/","page":"Solvers","title":"Solvers","text":"Generally, solvers can be grouped into two categories: Offline solvers that do most of their computational work before interacting with the environment, and online solvers that do their work online. Although offline and online solvers both use the exact same Solver, solve, Policy, action structure, the work of defining online and offline solvers is focused on different portions.","category":"page"},{"location":"def_solver/","page":"Solvers","title":"Solvers","text":"For an offline solver, most of the implementation effort will be spent on the [solve] function, and an off-the-shelf policy from POMDPPolicies.jl will typically be used.","category":"page"},{"location":"def_solver/","page":"Solvers","title":"Solvers","text":"For an online solver, the solve function typically does little or no work, but merely creates a policy object that will carry out computation online. It is typical in POMDPs.jl to use the term \"Planner\" to name a Policy object for an online solver that carries out a large amount of computation at interaction time. In this case most of the effort will be focused on implementing the action method for the \"Planner\" Policy type.","category":"page"},{"location":"def_solver/#Examples","page":"Solvers","title":"Examples","text":"","category":"section"},{"location":"def_solver/","page":"Solvers","title":"Solvers","text":"Solver implementation is most clearly explained through examples. The following sections contain examples of both online and offline solver definitions:","category":"page"},{"location":"def_solver/","page":"Solvers","title":"Solvers","text":"Pages = [\"offline_solver.md\", \"online_solver.md\"]","category":"page"},{"location":"online_solver/#Example:-Defining-an-online-solver","page":"Example: Defining an online solver","title":"Example: Defining an online solver","text":"","category":"section"},{"location":"online_solver/","page":"Example: Defining an online solver","title":"Example: Defining an online solver","text":"In this example, we will define a simple online solver that works for both POMDPs and MDPs. In order to focus on the code structure, we will not create an algorithm that finds an optimal policy, but rather a greedy policy, that is, one that optimizes the expected immediate reward. For information on using this solver in a simulation, see Running Simulations.","category":"page"},{"location":"online_solver/","page":"Example: Defining an online solver","title":"Example: Defining an online solver","text":"In order to handle the widest range of problems, we will use @gen to generate Mone Carlo samples to estimate the reward even if only a simulator is available. We begin by creating the necessary types and the solve function. The only solver parameter is the number of samples used to estimate the reward at each step.","category":"page"},{"location":"online_solver/","page":"Example: Defining an online solver","title":"Example: Defining an online solver","text":"using POMDPs\n\nstruct MonteCarloGreedySolver <: Solver\n    num_samples::Int\nend\n\nstruct MonteCarloGreedyPlanner{M} <: Policy\n    m::M\n    num_samples::Int\nend\n\nPOMDPs.solve(sol::MonteCarloGreedySolver, m) = MonteCarloGreedyPlanner(m, sol.num_samples)","category":"page"},{"location":"online_solver/","page":"Example: Defining an online solver","title":"Example: Defining an online solver","text":"Next, we define the action function where the online work takes place.","category":"page"},{"location":"online_solver/#MDP-Case","page":"Example: Defining an online solver","title":"MDP Case","text":"","category":"section"},{"location":"online_solver/","page":"Example: Defining an online solver","title":"Example: Defining an online solver","text":"function POMDPs.action(p::MonteCarloGreedyPlanner{<:MDP}, s)\n    best_reward = -Inf\n    local best_action\n    for a in actions(p.m)\n        reward_sum = sum(@gen(:r)(p.m, s, a) for _ in 1:p.num_samples)\n        if reward_sum >= best_reward\n            best_reward = reward_sum\n            best_action = a\n        end\n    end\n    return best_action\nend","category":"page"},{"location":"online_solver/#POMDP-Case","page":"Example: Defining an online solver","title":"POMDP Case","text":"","category":"section"},{"location":"online_solver/","page":"Example: Defining an online solver","title":"Example: Defining an online solver","text":"function POMDPs.action(p::MonteCarloGreedyPlanner{<:POMDP}, b)\n    best_reward = -Inf\n    local best_action\n    for a in actions(p.m)\n        s = rand(b)\n        reward_sum = sum(@gen(:r)(p.m, s, a) for _ in 1:p.num_samples)\n        if reward_sum >= best_reward\n            best_reward = reward_sum\n            best_action = a\n        end\n    end\n    return best_action\nend\n\n# output\n","category":"page"},{"location":"online_solver/#Verification","page":"Example: Defining an online solver","title":"Verification","text":"","category":"section"},{"location":"online_solver/","page":"Example: Defining an online solver","title":"Example: Defining an online solver","text":"We can now verify that the online planner works in some simple cases:","category":"page"},{"location":"online_solver/","page":"Example: Defining an online solver","title":"Example: Defining an online solver","text":"using POMDPModels\n\ngw = SimpleGridWorld(size=(2,1), rewards=Dict(GWPos(2,1)=>1.0))\nsolver = MonteCarloGreedySolver(1000)\nplanner = solve(solver, gw)\n\naction(planner, GWPos(1,1))\n\n# output\n\n:right","category":"page"},{"location":"online_solver/","page":"Example: Defining an online solver","title":"Example: Defining an online solver","text":"using POMDPModels\nusing POMDPModelTools # for Deterministic, Uniform\n\ntiger = TigerPOMDP()\nsolver = MonteCarloGreedySolver(1000)\n\nplanner = solve(solver, tiger)\n\n@assert action(planner, Deterministic(TIGER_LEFT)) == TIGER_OPEN_RIGHT\n@assert action(planner, Deterministic(TIGER_RIGHT)) == TIGER_OPEN_LEFT\n# note action(planner, Uniform(states(tiger))) is not very reliable with this number of samples","category":"page"},{"location":"get_started/#Getting-Started","page":"Getting Started","title":"Getting Started","text":"","category":"section"},{"location":"get_started/","page":"Getting Started","title":"Getting Started","text":"Before writing our own POMDP problems or solvers, let's try out some of the available solvers and problem models available in JuliaPOMDP.","category":"page"},{"location":"get_started/","page":"Getting Started","title":"Getting Started","text":"Here is a short piece of code that solves the Tiger POMDP using QMDP, and evaluates the results. Note that you must have the QMDP, POMDPModels, and POMDPToolbox modules installed.","category":"page"},{"location":"get_started/","page":"Getting Started","title":"Getting Started","text":"using POMDPs, QMDP, POMDPModels, POMDPSimulators\n\n# initialize problem and solver\npomdp = TigerPOMDP() # from POMDPModels\nsolver = QMDPSolver() # from QMDP\n\n# compute a policy\npolicy = solve(solver, pomdp)\n\n#evaluate the policy\nbelief_updater = updater(policy) # the default QMDP belief updater (discrete Bayesian filter)\ninit_dist = initialstate_distribution(pomdp) # from POMDPModels\nhr = HistoryRecorder(max_steps=100) # from POMDPSimulators\nhist = simulate(hr, pomdp, policy, belief_updater, init_dist) # run 100 step simulation\nprintln(\"reward: $(discounted_reward(hist))\")","category":"page"},{"location":"get_started/","page":"Getting Started","title":"Getting Started","text":"The first part of the code loads the desired packages and initializes the problem and the solver. Next, we compute a POMDP policy. Lastly, we evaluate the results.","category":"page"},{"location":"get_started/","page":"Getting Started","title":"Getting Started","text":"There are a few things to mention here. First, the TigerPOMDP type implements all the functions required by QMDPSolver to compute a policy. Second, each policy has a default updater (essentially a filter used to update the belief of the POMDP). To learn more about Updaters check out the Concepts section.","category":"page"},{"location":"dynamics/#dynamics","page":"Defining (PO)MDP Dynamics","title":"Defining (PO)MDP Dynamics","text":"","category":"section"},{"location":"dynamics/","page":"Defining (PO)MDP Dynamics","title":"Defining (PO)MDP Dynamics","text":"The dynamics of a (PO)MDP define how states, observations, and rewards are generated at each time step. One way to visualize the structure of (PO)MDP is with a dynamic decision network (DDN) (see for example Decision Making under Uncertainty by Kochenderfer et al. or this webpage for more discussion of dynamic decision networks).","category":"page"},{"location":"dynamics/","page":"Defining (PO)MDP Dynamics","title":"Defining (PO)MDP Dynamics","text":"The POMDPs.jl DDN models are shown below:","category":"page"},{"location":"dynamics/","page":"Defining (PO)MDP Dynamics","title":"Defining (PO)MDP Dynamics","text":"Standard MDP DDN Standard POMDP DDN\n(Image: MDP DDN) (Image: POMDP DDN)","category":"page"},{"location":"dynamics/","page":"Defining (PO)MDP Dynamics","title":"Defining (PO)MDP Dynamics","text":"note: Note\nIn order to provide additional flexibility, these DDNs have :s:o, :sp:r and :o:r edges that are typically absent from the DDNs traditionally used in the (PO)MDP literature. Traditional (PO)MDP algorithms are compatible with these DDNs because only R(sa), the expectation of R(s a s o) over all s and o is needed to make optimal decisions.","category":"page"},{"location":"dynamics/","page":"Defining (PO)MDP Dynamics","title":"Defining (PO)MDP Dynamics","text":"The task of defining the dynamics of a (PO)MDP consists of defining a model for each of the nodes in the DDN. Models for each node can either be implemented separately through the transition, observation, and reward functions, or together with the gen function. ","category":"page"},{"location":"dynamics/#Separate-explicit-or-generative-definition","page":"Defining (PO)MDP Dynamics","title":"Separate explicit or generative definition","text":"","category":"section"},{"location":"dynamics/","page":"Defining (PO)MDP Dynamics","title":"Defining (PO)MDP Dynamics","text":"transition(pomdp, s, a) defines the state transition probability distribution for state s and action a. This defines an explicit model for the :sp DDN node.\nobservation(pomdp, [s,] a, sp) defines the observation distribution given that action a was taken and the state is now sp (The observation can optionally depend on s - see docstring). This defines an explicit model for the :o DDN node.\nreward(pomdp, s, a[, sp[, o]]) defines the reward, which is a deterministic function of the state and action (and optionally sp and o - see docstring). This defines an explicit model for the :r DDN node.","category":"page"},{"location":"dynamics/","page":"Defining (PO)MDP Dynamics","title":"Defining (PO)MDP Dynamics","text":"transition and observation should return distribution objects that implement part or all of the distribution interface. Some predefined distributions can be found in Distributions.jl or POMDPModelTools.jl, or custom types that represent distributions appropriate for the problem may be created.","category":"page"},{"location":"dynamics/","page":"Defining (PO)MDP Dynamics","title":"Defining (PO)MDP Dynamics","text":"note: Note\nThere is no requirement that a problem defined using the explicit interface be discrete; it is straightforward to define continuous POMDPs with the explicit interface, provided that the distributions have some finite parameterization.","category":"page"},{"location":"dynamics/#Combined-generative-definition","page":"Defining (PO)MDP Dynamics","title":"Combined generative definition","text":"","category":"section"},{"location":"dynamics/","page":"Defining (PO)MDP Dynamics","title":"Defining (PO)MDP Dynamics","text":"If the state, observation, and reward are generated simultaneously, a new method of the gen function should be implemented to return the state, observation and reward in a single NamedTuple.","category":"page"},{"location":"dynamics/#Examples","page":"Defining (PO)MDP Dynamics","title":"Examples","text":"","category":"section"},{"location":"dynamics/","page":"Defining (PO)MDP Dynamics","title":"Defining (PO)MDP Dynamics","text":"An example of defining a problem using separate functions can be found at:  https://github.com/JuliaPOMDP/POMDPExamples.jl/blob/master/notebooks/Defining-a-POMDP-with-the-Explicit-Interface.ipynb","category":"page"},{"location":"dynamics/","page":"Defining (PO)MDP Dynamics","title":"Defining (PO)MDP Dynamics","text":"An example of defining a problem with a combined gen function can be found at: https://github.com/JuliaPOMDP/POMDPExamples.jl/blob/master/notebooks/Defining-a-POMDP-with-the-Generative-Interface.ipynb","category":"page"},{"location":"#[POMDPs.jl](https://github.com/JuliaPOMDP/POMDPs.jl)","page":"POMDPs.jl","title":"POMDPs.jl","text":"","category":"section"},{"location":"","page":"POMDPs.jl","title":"POMDPs.jl","text":"A Julia interface for defining, solving and simulating partially observable Markov decision processes and their fully observable counterparts.","category":"page"},{"location":"#Package-Features","page":"POMDPs.jl","title":"Package Features","text":"","category":"section"},{"location":"","page":"POMDPs.jl","title":"POMDPs.jl","text":"General interface that can handle problems with discrete and continuous state/action/observation spaces\nA number of popular state-of-the-art solvers available to use out of the box\nTools that make it easy to define problems and simulate solutions\nSimple integration of custom solvers into the existing interface","category":"page"},{"location":"#Available-Packages","page":"POMDPs.jl","title":"Available Packages","text":"","category":"section"},{"location":"","page":"POMDPs.jl","title":"POMDPs.jl","text":"The POMDPs.jl package contains the interface used for expressing and solving Markov decision processes (MDPs) and partially observable Markov decision processes (POMDPs) in the Julia programming language. The JuliaPOMDP community maintains these packages. The list of solver and support packages is maintained at the POMDPs.jl Readme.","category":"page"},{"location":"#Documentation-Outline","page":"POMDPs.jl","title":"Documentation Outline","text":"","category":"section"},{"location":"","page":"POMDPs.jl","title":"POMDPs.jl","text":"Documentation comes in four forms:","category":"page"},{"location":"","page":"POMDPs.jl","title":"POMDPs.jl","text":"How-to examples are available in the POMDPExamples package and in pages in this document with \"Example\" in the title.\nAn explanatory guide is available in the sections outlined below.\nReference docstrings for the entire interface are available in the API Documentation section.","category":"page"},{"location":"","page":"POMDPs.jl","title":"POMDPs.jl","text":"When updating these documents, make sure this is synced with docs/make.jl!!","category":"page"},{"location":"#Basics","page":"POMDPs.jl","title":"Basics","text":"","category":"section"},{"location":"","page":"POMDPs.jl","title":"POMDPs.jl","text":"Pages = [\"install.md\", \"get_started.md\", \"concepts.md\"]","category":"page"},{"location":"#Defining-POMDP-Models","page":"POMDPs.jl","title":"Defining POMDP Models","text":"","category":"section"},{"location":"","page":"POMDPs.jl","title":"POMDPs.jl","text":"Pages = [ \"def_pomdp.md\", \"static.md\", \"interfaces.md\", \"dynamics.md\"]","category":"page"},{"location":"#Writing-Solvers-and-Updaters","page":"POMDPs.jl","title":"Writing Solvers and Updaters","text":"","category":"section"},{"location":"","page":"POMDPs.jl","title":"POMDPs.jl","text":"Pages = [ \"def_solver.md\", \"offline_solver.md\", \"online_solver.md\", \"def_updater.md\" ]","category":"page"},{"location":"#Analyzing-Results","page":"POMDPs.jl","title":"Analyzing Results","text":"","category":"section"},{"location":"","page":"POMDPs.jl","title":"POMDPs.jl","text":"Pages = [ \"simulation.md\", \"run_simulation.md\", \"policy_interaction.md\" ]","category":"page"},{"location":"#Reference","page":"POMDPs.jl","title":"Reference","text":"","category":"section"},{"location":"","page":"POMDPs.jl","title":"POMDPs.jl","text":"Pages = [\"faq.md\", \"api.md\"]","category":"page"}]
}
