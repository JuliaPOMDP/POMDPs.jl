var documenterSearchIndex = {"docs":
[{"location":"ddns/#Dynamic-Decision-Networks-1","page":"Dynamic Decision Networks","title":"Dynamic Decision Networks","text":"","category":"section"},{"location":"ddns/#","page":"Dynamic Decision Networks","title":"Dynamic Decision Networks","text":"Part of the conceptual definition of a POMDP or MDP is a dynamic decision network (DDN) that defines which random variables are dependent on each other. Usually, problem writers will not have to interact directly with the DDN, but it is a helpful concept for understanding, and it can be customized for special problem types.","category":"page"},{"location":"ddns/#","page":"Dynamic Decision Networks","title":"Dynamic Decision Networks","text":"The standard POMDPs.jl DDN models are shown below:","category":"page"},{"location":"ddns/#","page":"Dynamic Decision Networks","title":"Dynamic Decision Networks","text":"Standard MDP DDN Standard POMDP DDN\n(Image: MDP DDN) (Image: POMDP DDN)","category":"page"},{"location":"ddns/#","page":"Dynamic Decision Networks","title":"Dynamic Decision Networks","text":"note: Note\nIn order to provide additional flexibility, these DDNs have :s→:o, :sp→:r and :o→:r edges that are typically absent from the DDNs traditionally used in the (PO)MDP literature. Traditional (PO)MDP algorithms are compatible with these DDNs because only R(sa), the expectation of R(s a s o) over all s and o is needed to make optimal decisions.","category":"page"},{"location":"ddns/#DDN-structure-representation-1","page":"Dynamic Decision Networks","title":"DDN structure representation","text":"","category":"section"},{"location":"ddns/#","page":"Dynamic Decision Networks","title":"Dynamic Decision Networks","text":"In POMDPs.jl, each DDN node corresponds to a Symbol. Often a p character (mnemonic: \"prime\") is appended to denote a new value for the next timestep, e.g. :sp represents s, the state at the next step.","category":"page"},{"location":"ddns/#","page":"Dynamic Decision Networks","title":"Dynamic Decision Networks","text":"A DDNStructure object contains the names of all the nodes, the edges between the nodes, and an object for each node that defines its implementation.","category":"page"},{"location":"ddns/#","page":"Dynamic Decision Networks","title":"Dynamic Decision Networks","text":"Currently, there are four types of nodes:","category":"page"},{"location":"ddns/#","page":"Dynamic Decision Networks","title":"Dynamic Decision Networks","text":"DistributionDDNNode to define nodes with stochastic output.\nFunctionDDNNode to define a node that is a deterministic function of other nodes.\nConstantDDNNode for a constant.\nGenericDDNNode for a node that has no implementation other than gen (see Defining behavior for nodes below).","category":"page"},{"location":"ddns/#","page":"Dynamic Decision Networks","title":"Dynamic Decision Networks","text":"This set is not expected to handle all possible behavior, so new types are likely to be added in the future (and they should be requested when concrete needs are encountered).","category":"page"},{"location":"ddns/#Defining-behavior-for-nodes-1","page":"Dynamic Decision Networks","title":"Defining behavior for nodes","text":"","category":"section"},{"location":"ddns/#","page":"Dynamic Decision Networks","title":"Dynamic Decision Networks","text":"For any node in the DDN, the function gen(::DDNNode{:nodename}, m, parent_values..., rng) will be called to sample a value (see the docstring for more information). This method can always be implemented to provide a generative definition for a node.","category":"page"},{"location":"ddns/#","page":"Dynamic Decision Networks","title":"Dynamic Decision Networks","text":"Some nodes can alternatively have an explicit implementation. For example, a DistributionDDNNode contains a function that is called with the (PO)MDP models and values sampled from the parent nodes to return a distribution. The state transition node, :sp, is a particular case of this. If gen(::GenVar{:sp}, m, s, a, rng) is not defined by the problem writer, rand(rng, transition(m, s, a)) will be called to generate values for :sp.","category":"page"},{"location":"ddns/#Mixing-generative-and-explicit-node-definitions-for-a-POMDP-1","page":"Dynamic Decision Networks","title":"Mixing generative and explicit node definitions for a POMDP","text":"","category":"section"},{"location":"ddns/#","page":"Dynamic Decision Networks","title":"Dynamic Decision Networks","text":"POMDP models will often contain a mixture of Generative and explicit definitions, and this is an encouraged paradigm. For example","category":"page"},{"location":"ddns/#","page":"Dynamic Decision Networks","title":"Dynamic Decision Networks","text":"using Distributions\nstruct MyPOMDP <: POMDP{Float64, Float64, Float64} end\nPOMDPs.gen(::GenVar{:sp}, m::MyPOMDP, s, a, rng) = s+a\nPOMDPs.observation(::GenVar{:o}, m, s, a, sp, rng) = Normal(sp)","category":"page"},{"location":"ddns/#","page":"Dynamic Decision Networks","title":"Dynamic Decision Networks","text":"would be a suitable distribution for a POMDP that will be solved with particle filtering methods where an explicit observation definition is needed, but a generative state transition definition is sufficient.","category":"page"},{"location":"ddns/#","page":"Dynamic Decision Networks","title":"Dynamic Decision Networks","text":"note: Note\nIt is usually best to avoid providing both a generative and explicit definition of the same node because it is easy to introduce inconsistency.","category":"page"},{"location":"ddns/#Customizing-the-DDN-1","page":"Dynamic Decision Networks","title":"Customizing the DDN","text":"","category":"section"},{"location":"ddns/#","page":"Dynamic Decision Networks","title":"Dynamic Decision Networks","text":"The DDN structure for a particular (PO)MDP problem type is defined with the DDNStructure trait, which should return a DDNStructure object (or something else that implements the appropriate methods). See the docstring for an example.","category":"page"},{"location":"ddns/#","page":"Dynamic Decision Networks","title":"Dynamic Decision Networks","text":"If a specialized DDN structure is to be compatible with standard POMDP solvers, it should have the standard :sp, :r, and :o nodes.","category":"page"},{"location":"ddns/#","page":"Dynamic Decision Networks","title":"Dynamic Decision Networks","text":"Currently (as of September, 2019), no solver has special behavior based on the DDN structure, but it is expected that packages will define new DDN structures for specialized cases like constrained POMDPs, mixed observability MDPs, or factored POMDPs. If you are considering creating a solver that relies on a specific DDN structure, please contact the developers so we can coordinate.","category":"page"},{"location":"policy_interaction/#Interacting-with-Policies-1","page":"Interacting with Policies","title":"Interacting with Policies","text":"","category":"section"},{"location":"policy_interaction/#","page":"Interacting with Policies","title":"Interacting with Policies","text":"A solution to a POMDP is a policy that maps beliefs or action-observation histories to actions. In POMDPs.jl, these are represented by Policy objects. See Solvers and Policies for more information about what a policy can represent in general.","category":"page"},{"location":"policy_interaction/#","page":"Interacting with Policies","title":"Interacting with Policies","text":"One common task in evaluating POMDP solutions is examining the policies themselves. Since the internal representation of a policy is an esoteric implementation detail, it is best to interact with policies through the action and value interface functions. There are three relevant methods","category":"page"},{"location":"policy_interaction/#","page":"Interacting with Policies","title":"Interacting with Policies","text":"action(policy, s) returns the best action (or one of the best) for the given state or belief.\nvalue(policy, s) returns the expected sum of future rewards if the policy is executed.\nvalue(policy, s, a) returns the \"Q-value\", that is, the expected sum of rewards if action a is taken on the next step and then the policy is executed.","category":"page"},{"location":"policy_interaction/#","page":"Interacting with Policies","title":"Interacting with Policies","text":"Note that the quantities returned by these functions are what the policy/solver expects to be the case after its (usually approximate) computations; they may be far from the true value if the solution is not exactly optimal.","category":"page"},{"location":"install/#Installation-1","page":"Installation","title":"Installation","text":"","category":"section"},{"location":"install/#","page":"Installation","title":"Installation","text":"If you have a running Julia distribution (Julia 0.4 or greater), you have everything you need to install POMDPs.jl. To install the package, simply run the following from the Julia REPL:","category":"page"},{"location":"install/#","page":"Installation","title":"Installation","text":"import Pkg\nPkg.add(\"POMDPs\") # installs the POMDPs.jl package","category":"page"},{"location":"install/#","page":"Installation","title":"Installation","text":"Once you have POMDPs.jl installed, you can install any package that is part of the JuliaPOMDP community by running:","category":"page"},{"location":"install/#","page":"Installation","title":"Installation","text":"using POMDPs, Pkg\nPOMDPs.add_registry()\nPkg.add(\"SARSOP\") # installs the SARSOP solver","category":"page"},{"location":"install/#","page":"Installation","title":"Installation","text":"The code above will download and install all dependencies automatically. All JuliaPOMDP packages have been tested on Linux and OS X, and most have been tested on Windows.","category":"page"},{"location":"install/#","page":"Installation","title":"Installation","text":"To get a list of all the available packages run:","category":"page"},{"location":"install/#","page":"Installation","title":"Installation","text":"POMDPs.available() # prints a list of all the available packages that can be installed with POMDPs.add","category":"page"},{"location":"def_pomdp/#defining_pomdps-1","page":"Defining POMDPs and MDPs","title":"Defining POMDPs and MDPs","text":"","category":"section"},{"location":"def_pomdp/#Consider-starting-with-one-of-these-packages-1","page":"Defining POMDPs and MDPs","title":"Consider starting with one of these packages","text":"","category":"section"},{"location":"def_pomdp/#","page":"Defining POMDPs and MDPs","title":"Defining POMDPs and MDPs","text":"Since POMDPs.jl was designed with performance and flexibility as first priorities, the interface is larger than needed to express most simple problems. For this reason, several packages and tools have been created to help users implement problems quickly. It is often easiest for new users to start with one of these.","category":"page"},{"location":"def_pomdp/#","page":"Defining POMDPs and MDPs","title":"Defining POMDPs and MDPs","text":"QuickPOMDPs.jl provides structures for concisely defining simple POMDPs without object-oriented programming.\nPOMDPExamples.jl provides tutorials for defining problems. \nThe Tabular(PO)MDP model from POMDPModels.jl allows users to define POMDPs with matrices for the transitions, observations and rewards.\nThe gen function is the easiest way to wrap a pre-existing simulator from another project or written in another programming language so that it can be used with POMDPs.jl solvers and simulators. See also RLInterface.jl for an even higher level interface for simulators where the state is not accessible.","category":"page"},{"location":"def_pomdp/#Overview-1","page":"Defining POMDPs and MDPs","title":"Overview","text":"","category":"section"},{"location":"def_pomdp/#","page":"Defining POMDPs and MDPs","title":"Defining POMDPs and MDPs","text":"The expressive nature of POMDPs.jl gives problem writers the flexibility to write their problem in many forms. Custom POMDP problems are defined by implementing the functions specified by the POMDPs API.","category":"page"},{"location":"def_pomdp/#","page":"Defining POMDPs and MDPs","title":"Defining POMDPs and MDPs","text":"note: Note\nThe main generative and explicit interfaces use an object-oriented programming paradigm and require familiarity with Julia. For users new to Julia, QuickPOMDPs usually requires less knowledge of the language and no object-oriented programming.","category":"page"},{"location":"def_pomdp/#","page":"Defining POMDPs and MDPs","title":"Defining POMDPs and MDPs","text":"There are two ways of specifying the state dynamics and observation behavior of a POMDP. The problem definition may include a mixture of explicit definitions of probability distributions, or generative definitions that simulate states and observations without explicitly defining the distributions. In scientific papers explicit definitions are often written as T(s  s a) for transitions and O(o  s a s) for observations, while a generative definition might be expressed as s o r = G(s a) (or s r = G(sa) for an MDP).","category":"page"},{"location":"def_pomdp/#","page":"Defining POMDPs and MDPs","title":"Defining POMDPs and MDPs","text":"Accordingly, the POMDPs.jl model API is grouped into three sections:","category":"page"},{"location":"def_pomdp/#","page":"Defining POMDPs and MDPs","title":"Defining POMDPs and MDPs","text":"The explicit interface containing functions that explicitly define distributions for DDN nodes.\nThe generative interface containing functions that return sampled states and observations for DDN nodes.\nCommon functions used in both.","category":"page"},{"location":"def_pomdp/#What-do-I-need-to-implement?-1","page":"Defining POMDPs and MDPs","title":"What do I need to implement?","text":"","category":"section"},{"location":"def_pomdp/#","page":"Defining POMDPs and MDPs","title":"Defining POMDPs and MDPs","text":"Because of the wide variety or problems and solvers that POMDPs.jl interfaces with, the question of which functions from the interface need to be implemented does not have a short answer for all cases. In general, a problem will be defined by implementing a combination of functions from the generative, explicit, and common parts of the interface.","category":"page"},{"location":"def_pomdp/#","page":"Defining POMDPs and MDPs","title":"Defining POMDPs and MDPs","text":"Specifically, a problem writer will need to define","category":"page"},{"location":"def_pomdp/#","page":"Defining POMDPs and MDPs","title":"Defining POMDPs and MDPs","text":"Explicit or generative definitions for \nthe state transition model (DDN node :sp),\nthe reward function (DDN node :r), and\nthe observation model (DDN node :o, for POMDPs only).\nFunctions to define some other properties of the problem such as the state, action, and observation spaces, which states are terminal, etc.","category":"page"},{"location":"def_pomdp/#","page":"Defining POMDPs and MDPs","title":"Defining POMDPs and MDPs","text":"note: Note\nSince an explicit definition for a DDN node contains all of the information required for a generative definition, POMDPs.jl will automatically synthesize the generative functions for that node at runtime if an explicit model is available. Thus, there is never a need for both generative and explicit definitions of a node, and it is usually best to avoid redundant definitions because it is easy for them to become inconsistent.","category":"page"},{"location":"def_pomdp/#","page":"Defining POMDPs and MDPs","title":"Defining POMDPs and MDPs","text":"The precise answer for which functions need to be implemented depends on two factors: problem complexity and which solver will be used. In particular, 2 questions should be asked:","category":"page"},{"location":"def_pomdp/#","page":"Defining POMDPs and MDPs","title":"Defining POMDPs and MDPs","text":"Is it difficult or impossible to specify a probability distribution explicitly?\nWhat solvers will be used to solve this, and what are their requirements?","category":"page"},{"location":"def_pomdp/#","page":"Defining POMDPs and MDPs","title":"Defining POMDPs and MDPs","text":"If the answer to (1) is yes, then a generative definition should be used. Question (2) should be answered by reading about the solvers and trying to run them, or through the requirements interface if it has been defined for the solver.","category":"page"},{"location":"def_pomdp/#","page":"Defining POMDPs and MDPs","title":"Defining POMDPs and MDPs","text":"note: Note\nIf a particular function is required by a solver but seems very difficult to implement for a particular problem, one should consider carefully whether the algorithm is capable of solving that problem. For example, if a problem has a complex hybrid state space, it will be more difficult to define states, but it is also true that solvers that require states such as SARSOP or IncrementalPruning, will usually not be able to solve such a problem, and solvers that can handle it, like ARDESPOT or MCVI, usually will not call states.","category":"page"},{"location":"def_pomdp/#Outline-1","page":"Defining POMDPs and MDPs","title":"Outline","text":"","category":"section"},{"location":"def_pomdp/#","page":"Defining POMDPs and MDPs","title":"Defining POMDPs and MDPs","text":"The following pages provide more details on specific parts of the interface:","category":"page"},{"location":"def_pomdp/#","page":"Defining POMDPs and MDPs","title":"Defining POMDPs and MDPs","text":"Dynamic Decision Networks\nExplicit DDN node definitions\nGenerative DDN node definitions\nBasic Properties (common part of the api)\nSpaces and Distributions\nRequirements","category":"page"},{"location":"specifying_requirements/#specifying_requirements-1","page":"Specifying Requirements","title":"Specifying Requirements","text":"","category":"section"},{"location":"specifying_requirements/#Purpose-1","page":"Specifying Requirements","title":"Purpose","text":"","category":"section"},{"location":"specifying_requirements/#","page":"Specifying Requirements","title":"Specifying Requirements","text":"When a researcher or student wants to use a solver in the POMDPs ecosystem, the first question they will ask is \"What do I have to implement to use this solver?\". The requirements interface provides a standard way for solver writers to answer this question.","category":"page"},{"location":"specifying_requirements/#Internal-interface-1","page":"Specifying Requirements","title":"Internal interface","text":"","category":"section"},{"location":"specifying_requirements/#","page":"Specifying Requirements","title":"Specifying Requirements","text":"The most important functions in the requirements interface are get_requirements, check_requirements, and show_requirements.","category":"page"},{"location":"specifying_requirements/#","page":"Specifying Requirements","title":"Specifying Requirements","text":"get_requirements(f::Function, args::Tuple{...}) should be implemented by a solver or simulator writer for all important functions that use the POMDPs.jl interface. In practice, this function will rarely by implemented directly because the @POMDP_require macro automatically creates it. The function should return a RequirementSet object containing all of the methods POMDPs.jl functions that need to be implemented for the function to work with the specified arguments.","category":"page"},{"location":"specifying_requirements/#","page":"Specifying Requirements","title":"Specifying Requirements","text":"check_requirements returns true if all of the requirements in a RequirementSet are met, and show_requirements prints out a list of the requirements in a RequirementSet and indicates which ones have been met.","category":"page"},{"location":"specifying_requirements/#pomdp_require_section-1","page":"Specifying Requirements","title":"@POMDP_require","text":"","category":"section"},{"location":"specifying_requirements/#","page":"Specifying Requirements","title":"Specifying Requirements","text":"The @POMDP_require macro is the main point of interaction with the requirements system for solver writers. It uses a special syntax to automatically implement get_requirements. This is best shown by example. Consider this @POMDP_require block from the DiscreteValueIteration package:","category":"page"},{"location":"specifying_requirements/#","page":"Specifying Requirements","title":"Specifying Requirements","text":"@POMDP_require solve(solver::ValueIterationSolver, mdp::Union{MDP,POMDP}) begin\n    P = typeof(mdp)\n    S = statetype(P)\n    A = actiontype(P)\n    @req discount(::P)\n    @req n_states(::P)\n    @req n_actions(::P)\n    @subreq ordered_states(mdp)\n    @subreq ordered_actions(mdp)\n    @req transition(::P,::S,::A)\n    @req reward(::P,::S,::A,::S)\n    @req stateindex(::P,::S)\n    as = actions(mdp)\n    ss = states(mdp)\n    @req iterator(::typeof(as))\n    @req iterator(::typeof(ss))\n    s = first(iterator(ss))\n    a = first(iterator(as))\n    dist = transition(mdp, s, a)\n    D = typeof(dist)\n    @req iterator(::D)\n    @req pdf(::D,::S)\nend","category":"page"},{"location":"specifying_requirements/#","page":"Specifying Requirements","title":"Specifying Requirements","text":"The first expression argument to the macro is a function signature specifying what the requirements apply to. The above example implements get_requirements{P<:Union{POMDP,MDP}}(solve::typeof(solve), args::Tuple{ValueIterationSolver,P}) which will construct a RequirementSet containing the requirements for executing the solve function with ValueIterationSolver and MDP or POMDP arguments at runtime.","category":"page"},{"location":"specifying_requirements/#","page":"Specifying Requirements","title":"Specifying Requirements","text":"The second expression is a begin-end block that specifies the requirements. The arguments in the function signature (solver and mdp in this example) may be used within the block.","category":"page"},{"location":"specifying_requirements/#","page":"Specifying Requirements","title":"Specifying Requirements","text":"The @req macro is used to specify a required function. Each @req should be followed by a function with the argument types specified. The @subreq macro is used to denote that the requirements of another function are also required. Each @subreq should be followed by a function call.","category":"page"},{"location":"specifying_requirements/#requirements_info-1","page":"Specifying Requirements","title":"requirements_info","text":"","category":"section"},{"location":"specifying_requirements/#","page":"Specifying Requirements","title":"Specifying Requirements","text":"While the @POMDP_require macro is used to specify requirements for a specific method, the requirements_info function is a more flexible communication tool for a solver writer. requirements_info should print out a message describing the requirements for a solver. The exact form of the message is up to the solver writer, but it should be carefully thought-out because problem-writers will be directed to call the function (via the @requirements_info macro) as the first step in using a new solver (see tutorial).","category":"page"},{"location":"specifying_requirements/#","page":"Specifying Requirements","title":"Specifying Requirements","text":"By default, requirements_info calls show_requirements on the solve function. This is adequate in many cases, but in some cases, notably for online solvers such as MCTS, the requirements for solve do not give a good indication of the requirements for using the solver. Instead, the requirements for action should be displayed. The following example shows a more informative version of requirements_info from the MCTS package. Since action requires a state argument, requirements_info prompts the user to provide one.","category":"page"},{"location":"specifying_requirements/#","page":"Specifying Requirements","title":"Specifying Requirements","text":"function POMDPs.requirements_info(solver::AbstractMCTSSolver, problem::Union{POMDP,MDP})\n    if statetype(typeof(problem)) <: Number\n        s = one(statetype(typeof(problem)))\n        requirements_info(solver, problem, s)\n    else\n        println(\"\"\"\n            Since MCTS is an online solver, most of the computation occurs in `action(policy, state)`. In order to view the requirements for this function, please, supply a state as the third argument to `requirements_info`, e.g.\n\n                @requirements_info $(typeof(solver))() $(typeof(problem))() $(statetype(typeof(problem)))()\n\n                \"\"\")\n    end\nend\n\nfunction POMDPs.requirements_info(solver::AbstractMCTSSolver, problem::Union{POMDP,MDP}, s)\n    policy = solve(solver, problem)\n    requirements_info(policy, s)\nend\n\nfunction POMDPs.requirements_info(policy::AbstractMCTSPolicy, s)\n    @show_requirements action(policy, s)\nend","category":"page"},{"location":"specifying_requirements/#@warn_requirements-1","page":"Specifying Requirements","title":"@warn_requirements","text":"","category":"section"},{"location":"specifying_requirements/#","page":"Specifying Requirements","title":"Specifying Requirements","text":"The @warn_requirements macro is a useful tool to improve usability of a solver. It will show a requirements list only if some requirements are not met. It might be used, for example, in the solve function to give a problem writer a useful error if some required methods are missing (assuming the solver writer has already used @POMDP_require to specify the requirements for solve):","category":"page"},{"location":"specifying_requirements/#","page":"Specifying Requirements","title":"Specifying Requirements","text":"function solve(solver::ValueIterationSolver, mdp::Union{POMDP, MDP})\n    @warn_requirements solve(solver, mdp)\n\n    # do the work of solving\nend","category":"page"},{"location":"specifying_requirements/#","page":"Specifying Requirements","title":"Specifying Requirements","text":"@warn_requirements does perform a runtime check of requirements every time it is called, so it should not be used in code that may be used in fast, high-performance loops.","category":"page"},{"location":"specifying_requirements/#implemented_section-1","page":"Specifying Requirements","title":"Determining whether a function is implemented","text":"","category":"section"},{"location":"specifying_requirements/#","page":"Specifying Requirements","title":"Specifying Requirements","text":"When checking requirements in check_requirements, or printing in show_requirements, the implemented function is used to determine whether an implementation for a function is available. For example implemented(discount, Tuple{NewPOMDP}) should return true if the writer of the NewPOMDP problem has implemented discount for their problem. In most cases, the default implementation,","category":"page"},{"location":"specifying_requirements/#","page":"Specifying Requirements","title":"Specifying Requirements","text":"implemented(f::Function, TT::TupleType) = method_exists(f, TT)","category":"page"},{"location":"specifying_requirements/#","page":"Specifying Requirements","title":"Specifying Requirements","text":"will automatically handle this, but there may be cases in which you want to override the behavior of implemented, for example, if the function can be synthesized from other functions. Examples of this can be found in the default implementations of the generative interface funcitons.","category":"page"},{"location":"concepts/#Concepts-and-Architecture-1","page":"Concepts and Architecture","title":"Concepts and Architecture","text":"","category":"section"},{"location":"concepts/#","page":"Concepts and Architecture","title":"Concepts and Architecture","text":"POMDPs.jl aims to coordinate the development of three software components: 1) a problem, 2) a solver, 3) an experiment. Each of these components has a set of abstract types associated with it and a set of functions that allow a user to define each component's behavior in a standardized way. An outline of the architecture is shown below.","category":"page"},{"location":"concepts/#","page":"Concepts and Architecture","title":"Concepts and Architecture","text":"(Image: concepts)","category":"page"},{"location":"concepts/#","page":"Concepts and Architecture","title":"Concepts and Architecture","text":"The MDP and POMDP types are associated with the problem definition. The Solver and Policy types are associated with the solver or decision-making agent. Typically, the Updater type is also associated with the solver, but a solver may sometimes be used with an updater that was implemented separately. The Simulator type is associated with the experiment.","category":"page"},{"location":"concepts/#","page":"Concepts and Architecture","title":"Concepts and Architecture","text":"The code components of the POMDPs.jl ecosystem relevant to problems and solvers are shown below. The arrows represent the flow of information from the problems to the solvers. The figure shows the two interfaces that form POMDPs.jl - Explicit and Generative. Details about these interfaces can be found in the section on Defining POMDPs.","category":"page"},{"location":"concepts/#","page":"Concepts and Architecture","title":"Concepts and Architecture","text":"(Image: interface_relationships)","category":"page"},{"location":"concepts/#POMDPs-and-MDPs-1","page":"Concepts and Architecture","title":"POMDPs and MDPs","text":"","category":"section"},{"location":"concepts/#","page":"Concepts and Architecture","title":"Concepts and Architecture","text":"An MDP is a mathematical framework for sequential decision making under uncertainty, and where all of the uncertainty arises from outcomes that are partially random and partially under the control of a decision maker. Mathematically, an MDP is a tuple (S,A,T,R), where S is the state space, A is the action space, T is a transition function defining the probability of transitioning to each state given the state and action at the previous time, and R is a reward function mapping every possible transition (s,a,s') to a real reward value. For more information see a textbook such as [1]. In POMDPs.jl an MDP is represented by a concrete subtype of the MDP abstract type and a set of methods that define each of its components. S and A are defined by implementing states and actions for your specific MDP subtype. R is by implementing reward, and T is defined by implementing transition if the explicit interface is used or gen if the generative interface is used.","category":"page"},{"location":"concepts/#","page":"Concepts and Architecture","title":"Concepts and Architecture","text":"A POMDP is a more general sequential decision making problem in which the agent is not sure what state they are in. The state is only partially observable by the decision making agent. Mathematically, a POMDP is a tuple (S,A,T,R,O,Z) where S, A, T, and R are the same as with MDPs, Z is the agent's observation space, and O defines the probability of receiving each observation at a transition. In POMDPs.jl, a POMDP is represented by a concrete subtype of the POMDP abstract type, Z may be defined by the observations function (though an explicit definition is often not required), and O is defined by implementing observation if the explicit interface is used or gen if the generative interface is used.","category":"page"},{"location":"concepts/#","page":"Concepts and Architecture","title":"Concepts and Architecture","text":"POMDPs.jl can also be extended to accommodate POMDP-like problem classes with expanded dynamic decision networks, such as constrained or factored POMDPs, and it contains functions for defining optional problem behavior such as a discount factor or a set of terminal states.","category":"page"},{"location":"concepts/#","page":"Concepts and Architecture","title":"Concepts and Architecture","text":"More information can be found in the Defining POMDPs section.","category":"page"},{"location":"concepts/#Beliefs-and-Updaters-1","page":"Concepts and Architecture","title":"Beliefs and Updaters","text":"","category":"section"},{"location":"concepts/#","page":"Concepts and Architecture","title":"Concepts and Architecture","text":"In a POMDP domain, the decision-making agent does not have complete information about the state of the problem, so the agent can only make choices based on its \"belief\" about the state. In the POMDP literature, the term \"belief\" is typically defined to mean a probability distribution over all possible states of the system. However, in practice, the agent often makes decisions based on an incomplete or lossy record of past observations that has a structure much different from a probability distribution. For example, if the agent is represented by a finite-state controller, as is the case for Monte-Carlo Value Iteration [2], the belief is the controller state, which is a node in a graph. Another example is an agent represented by a recurrent neural network. In this case, the agent's belief is the state of the network. In order to accommodate a wide variety of decision-making approaches in POMDPs.jl, we use the term \"belief\" to denote the set of information that the agent makes a decision on, which could be an exact state distribution, an action-observation history, a set of weighted particles, or the examples mentioned before. In code, the belief can be represented by any built-in or user-defined type.","category":"page"},{"location":"concepts/#","page":"Concepts and Architecture","title":"Concepts and Architecture","text":"When an action is taken and a new observation is received, the belief is updated by the belief updater. In code, a belief updater is represented by a concrete subtype of the Updater abstract type, and the update(updater, belief, action, observation) function defines how the belief is updated when a new observation is received.","category":"page"},{"location":"concepts/#","page":"Concepts and Architecture","title":"Concepts and Architecture","text":"Although the agent may use a specialized belief structure to make decisions, the information initially given to the agent about the state of the problem is usually most conveniently represented as a state distribution, thus the initialize_belief function is provided to convert a state distribution to a specialized belief structure that an updater can work with.","category":"page"},{"location":"concepts/#","page":"Concepts and Architecture","title":"Concepts and Architecture","text":"In many cases, the belief structure is closely related to the solution technique, so it will be implemented by the programmer who writes the solver. In other cases, the agent can use a variety of belief structures to make decisions, so a domain-specific updater implemented by the programmer that wrote the problem description may be appropriate. Finally, some advanced generic belief updaters such as particle filters may be implemented by a third party. The convenience function updater(policy) can be used to get a suitable default updater for a policy, however many policies can work with other updaters.","category":"page"},{"location":"concepts/#","page":"Concepts and Architecture","title":"Concepts and Architecture","text":"For more information on implementing a belief updater, see Defining a Belief Updater","category":"page"},{"location":"concepts/#Solvers-and-Policies-1","page":"Concepts and Architecture","title":"Solvers and Policies","text":"","category":"section"},{"location":"concepts/#","page":"Concepts and Architecture","title":"Concepts and Architecture","text":"Sequential decision making under uncertainty involves both online and offline calculations. In the broad sense, the term \"solver\" as used in the node in the figure at the top of the page refers to the software package that performs the calculations at both of these times. However, the code is broken up into two pieces, the solver that performs calculations offline and the policy that performs calculations online.","category":"page"},{"location":"concepts/#","page":"Concepts and Architecture","title":"Concepts and Architecture","text":"In the abstract, a policy is a mapping from every belief that an agent might take to an action. A policy is represented in code by a concrete subtype of the Policy abstract type. The programmer implements action to describe what computations need to be done online. For an online solver such as POMCP, all of the decision computation occurs within action while for an offline solver like SARSOP, there is very little computation within action. See Interacting with Policies for more information.","category":"page"},{"location":"concepts/#","page":"Concepts and Architecture","title":"Concepts and Architecture","text":"The offline portion of the computation is carried out by the solver, which is represented by a concrete subtype of the Solver abstract type. Computations occur within the solve function. For an offline solver like SARSOP, nearly all of the decision computation occurs within this function, but for some online solvers such as POMCP, solve merely embeds the problem in the policy.","category":"page"},{"location":"concepts/#Simulators-1","page":"Concepts and Architecture","title":"Simulators","text":"","category":"section"},{"location":"concepts/#","page":"Concepts and Architecture","title":"Concepts and Architecture","text":"A simulator defines a way to run one or more simulations. It is represented by a concrete subtype of the Simulator abstract type and the simulation is an implemention of simulate. Depending on the simulator, simulate may return a variety of data about the simulation, such as the discounted reward or the state history. All simulators should perform simulations consistent with the Simulation Standard.","category":"page"},{"location":"concepts/#","page":"Concepts and Architecture","title":"Concepts and Architecture","text":"[1] Decision Making Under Uncertainty: Theory and Application by Mykel J. Kochenderfer, MIT Press, 2015","category":"page"},{"location":"concepts/#","page":"Concepts and Architecture","title":"Concepts and Architecture","text":"[2] Bai, H., Hsu, D., & Lee, W. S. (2014). Integrated perception and planning in the continuous space: A POMDP approach. The International Journal of Robotics Research, 33(9), 1288-1302","category":"page"},{"location":"interfaces/#Spaces-and-Distributions-1","page":"Spaces and Distributions","title":"Spaces and Distributions","text":"","category":"section"},{"location":"interfaces/#","page":"Spaces and Distributions","title":"Spaces and Distributions","text":"Two important components of the definitions of MDPs and POMDPs are spaces, which specify the possible states, actions, and observations in a problem and distributions, which define probability distributions. In order to provide for maximum flexibility spaces and distributions may be of any type (i.e. there are no abstract base types). Solvers and simulators will interact with space and distribution types using the functions defined below.","category":"page"},{"location":"interfaces/#space-interface-1","page":"Spaces and Distributions","title":"Spaces","text":"","category":"section"},{"location":"interfaces/#","page":"Spaces and Distributions","title":"Spaces and Distributions","text":"A space object should contain the information needed to define the set of all possible states, actions or observations. The implementation will depend on the attributes of the elements. For example, if the space is continuous, the space object may only contain the limits of the continuous range. In the case of a discrete problem, a vector containing all states is appropriate for representing a space.","category":"page"},{"location":"interfaces/#","page":"Spaces and Distributions","title":"Spaces and Distributions","text":"The following functions may be called on a space object (Click on a function to read its documentation):","category":"page"},{"location":"interfaces/#","page":"Spaces and Distributions","title":"Spaces and Distributions","text":"rand\nlength\ndimensions","category":"page"},{"location":"interfaces/#Distributions-1","page":"Spaces and Distributions","title":"Distributions","text":"","category":"section"},{"location":"interfaces/#","page":"Spaces and Distributions","title":"Spaces and Distributions","text":"A distribution object represents a probability distribution.","category":"page"},{"location":"interfaces/#","page":"Spaces and Distributions","title":"Spaces and Distributions","text":"The following functions may be called on a distribution object (Click on a function to read its documentation):","category":"page"},{"location":"interfaces/#","page":"Spaces and Distributions","title":"Spaces and Distributions","text":"rand(rng, d) [1]\nsupport\npdf\nmode\nmean","category":"page"},{"location":"interfaces/#","page":"Spaces and Distributions","title":"Spaces and Distributions","text":"[1] rand(rng::AbstractRNG, d) where d is a distribution object is the only method of rand that is officially part of the POMDPs.jl interface, so it is the only required method for new distributions. However, users may wish to hook into the official julia rand interface to enable more flexible rand calls.","category":"page"},{"location":"basic_properties/#basic-1","page":"Defining Basic (PO)MDP Properties","title":"Defining Basic (PO)MDP Properties","text":"","category":"section"},{"location":"basic_properties/#","page":"Defining Basic (PO)MDP Properties","title":"Defining Basic (PO)MDP Properties","text":"In addition to the dynamic decision network (DDN) that defines the state and observation dynamics, a POMDPs.jl problem definition will include definitions of various other properties. Each of these properties is defined by implementing a new method of an interface function for the problem.","category":"page"},{"location":"basic_properties/#","page":"Defining Basic (PO)MDP Properties","title":"Defining Basic (PO)MDP Properties","text":"To use most solvers, it is only necessary to implement a few of these functions.","category":"page"},{"location":"basic_properties/#Spaces-1","page":"Defining Basic (PO)MDP Properties","title":"Spaces","text":"","category":"section"},{"location":"basic_properties/#","page":"Defining Basic (PO)MDP Properties","title":"Defining Basic (PO)MDP Properties","text":"The state, action and observation spaces are defined by the following functions:","category":"page"},{"location":"basic_properties/#","page":"Defining Basic (PO)MDP Properties","title":"Defining Basic (PO)MDP Properties","text":"states(pomdp)\nactions(pomdp[, s])\nobservations(pomdp)","category":"page"},{"location":"basic_properties/#","page":"Defining Basic (PO)MDP Properties","title":"Defining Basic (PO)MDP Properties","text":"The object returned by these functions should implement part or all of the interface for spaces. For discrete problems, a vector is appropriate.","category":"page"},{"location":"basic_properties/#","page":"Defining Basic (PO)MDP Properties","title":"Defining Basic (PO)MDP Properties","text":"It is often important to limit the action space based on the current state, belief, or observation.  This can be accomplished with the actions(m, s) or actions(m, b) function. See Histories associated with a belief and the history and currentobs docstrings for more information.","category":"page"},{"location":"basic_properties/#Discount-Factor-1","page":"Defining Basic (PO)MDP Properties","title":"Discount Factor","text":"","category":"section"},{"location":"basic_properties/#","page":"Defining Basic (PO)MDP Properties","title":"Defining Basic (PO)MDP Properties","text":"discount(pomdp) should return a number between 0 and 1 to define the discount factor.","category":"page"},{"location":"basic_properties/#Terminal-States-1","page":"Defining Basic (PO)MDP Properties","title":"Terminal States","text":"","category":"section"},{"location":"basic_properties/#","page":"Defining Basic (PO)MDP Properties","title":"Defining Basic (PO)MDP Properties","text":"If a problem has terminal states, they can be specified using the isterminal function. If a state s is terminal isterminal(pomdp, s) should return true, otherwise it should return false.","category":"page"},{"location":"basic_properties/#","page":"Defining Basic (PO)MDP Properties","title":"Defining Basic (PO)MDP Properties","text":"In POMDPs.jl, no actions can be taken from terminal states, and no additional rewards can be collected, thus, the value function for a terminal state is zero. POMDPs.jl does not have a mechanism for defining terminal rewards apart from the reward function, so the problem should be defined so that any terminal rewards are collected as the system transitions into a terminal state.","category":"page"},{"location":"basic_properties/#Indexing-1","page":"Defining Basic (PO)MDP Properties","title":"Indexing","text":"","category":"section"},{"location":"basic_properties/#","page":"Defining Basic (PO)MDP Properties","title":"Defining Basic (PO)MDP Properties","text":"For discrete problems, some solvers rely on a fast method for finding the index of the states, actions, or observations in an ordered list. These indexing functions can be implemented as","category":"page"},{"location":"basic_properties/#","page":"Defining Basic (PO)MDP Properties","title":"Defining Basic (PO)MDP Properties","text":"stateindex(pomdp, s)\nactionindex(pomdp, a)\nobsindex(pomdp, o)","category":"page"},{"location":"basic_properties/#","page":"Defining Basic (PO)MDP Properties","title":"Defining Basic (PO)MDP Properties","text":"Note that the converse mapping (from indices to states) is not part of the POMDPs interface. A solver will typically create a vector containing all the states to define it.","category":"page"},{"location":"basic_properties/#Conversion-to-vector-types-1","page":"Defining Basic (PO)MDP Properties","title":"Conversion to vector types","text":"","category":"section"},{"location":"basic_properties/#","page":"Defining Basic (PO)MDP Properties","title":"Defining Basic (PO)MDP Properties","text":"Some solvers (notably those that involve deep learning) rely on the ability to represent states, actions, and observations as vectors. To define a mapping between vectors and custom problem-specific representations, implement the following functions (see docstring for signature):","category":"page"},{"location":"basic_properties/#","page":"Defining Basic (PO)MDP Properties","title":"Defining Basic (PO)MDP Properties","text":"convert_s\nconvert_a\nconvert_o","category":"page"},{"location":"def_updater/#Defining-a-Belief-Updater-1","page":"Defining a Belief Updater","title":"Defining a Belief Updater","text":"","category":"section"},{"location":"def_updater/#","page":"Defining a Belief Updater","title":"Defining a Belief Updater","text":"In this section we list the requirements for defining a belief updater. For a description of what a belief updater is, see Concepts and Architecture - Beliefs and Updaters. Typically a belief updater will have an associated belief type, and may be closely tied to a particular policy/planner.","category":"page"},{"location":"def_updater/#Defining-a-Belief-Type-1","page":"Defining a Belief Updater","title":"Defining a Belief Type","text":"","category":"section"},{"location":"def_updater/#","page":"Defining a Belief Updater","title":"Defining a Belief Updater","text":"A belief object should contain all of the information needed for the next belief update and for the policy to make a decision. The belief type could be a pre-defined type such as a distribution from Distributions.jl or DiscreteBelief or SparseCat from POMDPModelTools.jl, or it could be a custom type.","category":"page"},{"location":"def_updater/#","page":"Defining a Belief Updater","title":"Defining a Belief Updater","text":"Often, but not always, the belief will represent a probability distribution. In this case, the functions in the distribution interface should be implemented if possible. Implementing these functions will make the belief usable with many of the policies and planners in the POMDPs.jl ecosystem, and will make it easy for others to convert between beliefs and to interpret what a belief means.","category":"page"},{"location":"def_updater/#Histories-associated-with-a-belief-1","page":"Defining a Belief Updater","title":"Histories associated with a belief","text":"","category":"section"},{"location":"def_updater/#","page":"Defining a Belief Updater","title":"Defining a Belief Updater","text":"If a complete or partial record of the action-observation history leading up to a belief is available, it is often helpful to give access to this by implementing the history or currentobs functions (see the docstrings for more details). This is especially useful if a problem-writer wants to implement a belief- or observation-dependent action space. Belief type implementers need only implement history, and currentobs will automatically be provided, though sometimes it is more convenient to implement currentobs directly.","category":"page"},{"location":"def_updater/#Defining-an-Updater-1","page":"Defining a Belief Updater","title":"Defining an Updater","text":"","category":"section"},{"location":"def_updater/#","page":"Defining a Belief Updater","title":"Defining a Belief Updater","text":"To create an updater, one should define a subtype of the Updater abstract type and implement two methods, one to create the initial belief from the problem's initial state distribution and one to perform a belief update:","category":"page"},{"location":"def_updater/#","page":"Defining a Belief Updater","title":"Defining a Belief Updater","text":"initialize_belief(updater, d) creates a belief from state distribution d appropriate to use with the updater. To extract information from d, use the functions from the distribution interface.\nupdate(updater, b, a, o) returns an updated belief given belief b, action a, and observation o. One can usually expect b to be the same type returned by initialize_belief because a careful user will always call initialize_belief before update, but it would also be reasonable to implement update for b of a different type if it is desirable to handle multiple belief types.","category":"page"},{"location":"def_updater/#Example:-History-Updater-1","page":"Defining a Belief Updater","title":"Example: History Updater","text":"","category":"section"},{"location":"def_updater/#","page":"Defining a Belief Updater","title":"Defining a Belief Updater","text":"One trivial type of belief would be the action-observation history, a list containing the initial state distribution and every action taken and observation received. The history contains all of the information received up to the current time, but it is not usually very useful because most policies make decisions based on a state probability distribution. Here the belief type is simply the built in Vector{Any}, so we need only create the updater and write update and initialize_belief. Normally, update would contain belief update probability calculations, but in this example, we simply append the action and observation to the history.","category":"page"},{"location":"def_updater/#","page":"Defining a Belief Updater","title":"Defining a Belief Updater","text":"(Note that this example is designed for readability rather than efficiency.)","category":"page"},{"location":"def_updater/#","page":"Defining a Belief Updater","title":"Defining a Belief Updater","text":"import POMDPs\n\nstruct HistoryUpdater <: POMDPs.Updater end\n\ninitialize_belief(up::HistoryUpdater, d) = Any[d]\n\nfunction POMDPs.update(up::HistoryUpdater, b, a, o)\n    bp = copy(b)\n    push!(bp, a)\n    push!(bp, o)\n    return bp\nend","category":"page"},{"location":"def_updater/#","page":"Defining a Belief Updater","title":"Defining a Belief Updater","text":"At each step, the history starts with the original distribution, then contains all the actions and observations received up to that point. The example below shows this for the crying baby problem (observations are true/false for crying and actions are true/false for feeding).","category":"page"},{"location":"def_updater/#","page":"Defining a Belief Updater","title":"Defining a Belief Updater","text":"using POMDPPolicies\nusing POMDPSimulators\nusing POMDPModels\nusing Random\n\npomdp = BabyPOMDP()\npolicy = RandomPolicy(pomdp, rng=MersenneTwister(1))\nup = HistoryUpdater()\n\n# within stepthrough initialize_belief is called on the initial state distribution of the pomdp, then update is called at each step.\nfor b in stepthrough(pomdp, policy, up, \"b\", rng=MersenneTwister(2), max_steps=5)\n    @show b\nend\n\n# output\n\nb = Any[POMDPModels.BoolDistribution(0.0)]\nb = Any[POMDPModels.BoolDistribution(0.0), false, false]\nb = Any[POMDPModels.BoolDistribution(0.0), false, false, false, false]\nb = Any[POMDPModels.BoolDistribution(0.0), false, false, false, false, true, false]\nb = Any[POMDPModels.BoolDistribution(0.0), false, false, false, false, true, false, true, false]","category":"page"},{"location":"faq/#Frequently-Asked-Questions-(FAQ)-1","page":"Frequently Asked Questions (FAQ)","title":"Frequently Asked Questions (FAQ)","text":"","category":"section"},{"location":"faq/#How-do-I-save-my-policies?-1","page":"Frequently Asked Questions (FAQ)","title":"How do I save my policies?","text":"","category":"section"},{"location":"faq/#","page":"Frequently Asked Questions (FAQ)","title":"Frequently Asked Questions (FAQ)","text":"We recommend using JLD2 to save the whole policy object:","category":"page"},{"location":"faq/#","page":"Frequently Asked Questions (FAQ)","title":"Frequently Asked Questions (FAQ)","text":"using JLD2\nsave(\"my_policy.jld2\", \"policy\", policy)","category":"page"},{"location":"faq/#Why-isn't-the-solver-working?-1","page":"Frequently Asked Questions (FAQ)","title":"Why isn't the solver working?","text":"","category":"section"},{"location":"faq/#","page":"Frequently Asked Questions (FAQ)","title":"Frequently Asked Questions (FAQ)","text":"There could be a number of things that are going wrong. Remeber, POMDPs can be fairly hard to work with, but don't panic.  If you have a discrete POMDP or MDP and you're using a solver that requires the explicit transition probabilities (you've implemented a pdf function), the first thing to try is make sure that your probability masses sum up to unity.  We've provide some tools in POMDPToolbox that can check this for you. If you have a POMDP called pomdp, you can run the checks by doing the following:","category":"page"},{"location":"faq/#","page":"Frequently Asked Questions (FAQ)","title":"Frequently Asked Questions (FAQ)","text":"using POMDPTesting\nprobability_check(pomdp) # checks that both observation and transition functions give probs that sum to unity\nobs_prob_consistency_check(pomdp) # checks the observation probabilities\ntrans_prob_consistency_check(pomdp) # check the transition probabilities","category":"page"},{"location":"faq/#","page":"Frequently Asked Questions (FAQ)","title":"Frequently Asked Questions (FAQ)","text":"If these throw an error, you may need to fix your transition or observation functions. ","category":"page"},{"location":"faq/#Why-do-I-need-to-put-type-assertions-pomdp::POMDP-into-the-function-signature?-1","page":"Frequently Asked Questions (FAQ)","title":"Why do I need to put type assertions pomdp::POMDP into the function signature?","text":"","category":"section"},{"location":"faq/#","page":"Frequently Asked Questions (FAQ)","title":"Frequently Asked Questions (FAQ)","text":"Specifying the type in your function signature allows Julia to call the appropriate function when your custom type is passed into it. For example if a POMDPs.jl solver calls states on the POMDP that you passed into it, the correct states function will only get dispatched if you specified that the states function you wrote works with your POMDP type. Because Julia supports multiple-dispatch, these type assertion are a way for doing object-oriented programming in Julia.","category":"page"},{"location":"faq/#Why-are-all-the-solvers-in-separate-modules?-1","page":"Frequently Asked Questions (FAQ)","title":"Why are all the solvers in separate modules?","text":"","category":"section"},{"location":"faq/#","page":"Frequently Asked Questions (FAQ)","title":"Frequently Asked Questions (FAQ)","text":"We did not put all the solvers and support tools into POMDPs.jl, because we wanted POMDPs.jl to be a lightweight interface package. This has a number of advantages. The first is that if a user only wants to use a few solvers from the JuliaPOMDP organization, they do not have to install all the other solvers and their dependencies. The second advantage is that people who are not directly part of the JuliaPOMDP organization can write their own solvers without going into the source code of other solvers. This makes the framework easier to adopt and to extend.","category":"page"},{"location":"faq/#How-can-I-implement-terminal-actions?-1","page":"Frequently Asked Questions (FAQ)","title":"How can I implement terminal actions?","text":"","category":"section"},{"location":"faq/#","page":"Frequently Asked Questions (FAQ)","title":"Frequently Asked Questions (FAQ)","text":"Terminal actions are actions that cause the MDP to terminate without generating a new state. POMDPs.jl handles terminal conditions via the isterminal function on states, and does not directly support terminal actions. If your MDP has a terminal action, you need to implement the model functions accordingly to generate a terminal state. In both generative and explicit cases, you will need some dummy state, say spt, that can be recognized as terminal by the isterminal function. One way to do this is to give spt a state value that is out of bounds (e.g. a vector of NaNs or -1s) and then check for that in isterminal, so that this does not clash with any conventional termination conditions on the state.","category":"page"},{"location":"faq/#","page":"Frequently Asked Questions (FAQ)","title":"Frequently Asked Questions (FAQ)","text":"If a terminal action is taken, regardless of current state, the transition function should return a distribution with only one next state, spt, with probability 1.0. In the generative case, the new state generated should be spt. The reward function or the r in generate_sr can be set according to the cost of the terminal action.","category":"page"},{"location":"faq/#Why-are-there-two-versions-of-reward?-1","page":"Frequently Asked Questions (FAQ)","title":"Why are there two versions of reward?","text":"","category":"section"},{"location":"faq/#","page":"Frequently Asked Questions (FAQ)","title":"Frequently Asked Questions (FAQ)","text":"Both reward(m, s, a) and reward(m, s, a, sp) are included because of these two facts:","category":"page"},{"location":"faq/#","page":"Frequently Asked Questions (FAQ)","title":"Frequently Asked Questions (FAQ)","text":"Some non-native solvers use reward(m, s, a)\nSometimes the reward depends on s and sp.","category":"page"},{"location":"faq/#","page":"Frequently Asked Questions (FAQ)","title":"Frequently Asked Questions (FAQ)","text":"It is reasonable to implement both as long as the (s, a) version is the expectation of the (s, a, s') version (see below).","category":"page"},{"location":"faq/#How-do-I-implement-reward(m,-s,-a)-if-the-reward-depends-on-the-next-state?-1","page":"Frequently Asked Questions (FAQ)","title":"How do I implement reward(m, s, a) if the reward depends on the next state?","text":"","category":"section"},{"location":"faq/#","page":"Frequently Asked Questions (FAQ)","title":"Frequently Asked Questions (FAQ)","text":"The solvers that require reward(m, s, a) only work on problems with finite state and action spaces. In this case, you can define reward(m, s, a) in terms of reward(m, s, a, sp) with the following code:","category":"page"},{"location":"faq/#","page":"Frequently Asked Questions (FAQ)","title":"Frequently Asked Questions (FAQ)","text":"const rdict = Dict{Tuple{S,A}, Float64}()\n\nfor s in states(m)\n  for a in actions(m)\n    r = 0.0\n    td = transition(m, s, a) # transition distribution for s, a\n    for sp in support(td)\n      r += pdf(td, sp)*reward(m, s, a, sp)\n    end\n    rdict[(s, a)] = r\n  end\nend\n\nPOMDPs.reward(m, s, a) = rdict[(s, a)]","category":"page"},{"location":"api/#API-Documentation-1","page":"API Documentation","title":"API Documentation","text":"","category":"section"},{"location":"api/#","page":"API Documentation","title":"API Documentation","text":"Documentation for the POMDPs.jl user interface. You can get help for any type or function in the module by typing ? in the Julia REPL followed by the name of type or function. For example:","category":"page"},{"location":"api/#","page":"API Documentation","title":"API Documentation","text":"julia> using POMDPs\njulia> ?\nhelp?> reward\nsearch: reward\n\n  reward{S,A,O}(pomdp::POMDP{S,A,O}, state::S, action::A, statep::S)\n\n  Returns the immediate reward for the s-a-s triple\n\n  reward{S,A,O}(pomdp::POMDP{S,A,O}, state::S, action::A)\n\n  Returns the immediate reward for the s-a pair\n","category":"page"},{"location":"api/#","page":"API Documentation","title":"API Documentation","text":"CurrentModule = POMDPs","category":"page"},{"location":"api/#Contents-1","page":"API Documentation","title":"Contents","text":"","category":"section"},{"location":"api/#","page":"API Documentation","title":"API Documentation","text":"Pages = [\"api.md\"]","category":"page"},{"location":"api/#Index-1","page":"API Documentation","title":"Index","text":"","category":"section"},{"location":"api/#","page":"API Documentation","title":"API Documentation","text":"Pages = [\"api.md\"]","category":"page"},{"location":"api/#Types-1","page":"API Documentation","title":"Types","text":"","category":"section"},{"location":"api/#","page":"API Documentation","title":"API Documentation","text":"POMDP\nMDP\nSolver\nPolicy\nUpdater","category":"page"},{"location":"api/#POMDPs.POMDP","page":"API Documentation","title":"POMDPs.POMDP","text":"POMDP{S,A,O}\n\nAbstract base type for a partially observable Markov decision process.\n\nS: state type\nA: action type\nO: observation type\n\n\n\n\n\n","category":"type"},{"location":"api/#POMDPs.MDP","page":"API Documentation","title":"POMDPs.MDP","text":"MDP{S,A}\n\nAbstract base type for a fully observable Markov decision process.\n\nS: state type\nA: action type\n\n\n\n\n\n","category":"type"},{"location":"api/#POMDPs.Solver","page":"API Documentation","title":"POMDPs.Solver","text":"Base type for an MDP/POMDP solver\n\n\n\n\n\n","category":"type"},{"location":"api/#POMDPs.Policy","page":"API Documentation","title":"POMDPs.Policy","text":"Base type for a policy (a map from every possible belief, or more abstract policy state, to an optimal or suboptimal action)\n\n\n\n\n\n","category":"type"},{"location":"api/#POMDPs.Updater","page":"API Documentation","title":"POMDPs.Updater","text":"Abstract type for an object that defines how the belief should be updated\n\nA belief is a general construct that represents the knowledge an agent has about the state of the system. This can be a probability distribution, an action observation history or a more general representation.\n\n\n\n\n\n","category":"type"},{"location":"api/#Model-Functions-1","page":"API Documentation","title":"Model Functions","text":"","category":"section"},{"location":"api/#explicit_api-1","page":"API Documentation","title":"Explicit","text":"","category":"section"},{"location":"api/#","page":"API Documentation","title":"API Documentation","text":"These functions return distributions.","category":"page"},{"location":"api/#","page":"API Documentation","title":"API Documentation","text":"transition\nobservation\ninitialstate_distribution\nreward","category":"page"},{"location":"api/#POMDPs.transition","page":"API Documentation","title":"POMDPs.transition","text":"transition(problem::POMDP, state, action)\ntransition(problem::MDP, state, action)\n\nReturn the transition distribution from the current state-action pair\n\n\n\n\n\n","category":"function"},{"location":"api/#POMDPs.observation","page":"API Documentation","title":"POMDPs.observation","text":"observation(problem::POMDP, statep)\nobservation(problem::POMDP, action, statep)\nobservation(problem::POMDP, state, action, statep)\n\nReturn the observation distribution. You need only define the method with the fewest arguments needed to determine the observation distribution.\n\nExample\n\nusing POMDPModelTools # for SparseCat\n\nstruct MyPOMDP <: POMDP{Int, Int, Int} end\n\nobservation(p::MyPOMDP, sp::Int) = SparseCat([sp-1, sp, sp+1], [0.1, 0.8, 0.1])\n\n\n\n\n\n","category":"function"},{"location":"api/#POMDPs.initialstate_distribution","page":"API Documentation","title":"POMDPs.initialstate_distribution","text":"initialstate_distribution(pomdp::POMDP)\ninitialstate_distribution(mdp::MDP)\n\nReturn a distribution of the initial state of the pomdp or mdp.\n\n\n\n\n\n","category":"function"},{"location":"api/#POMDPs.reward","page":"API Documentation","title":"POMDPs.reward","text":"reward(m::POMDP, s, a)\nreward(m::MDP, s, a)\n\nReturn the immediate reward for the s-a pair.\n\nreward(m::POMDP, s, a, sp)\nreward(m::MDP, s, a, sp)\n\nReturn the immediate reward for the s-a-s' triple\n\nreward(m::POMDP, s, a, sp, o)\n\nReturn the immediate reward for the s-a-s'-o quad\n\nFor some problems, it is easier to express reward(m, s, a, sp) or reward(m, s, a, sp, o), than reward(m, s, a), but some solvers, e.g. SARSOP, can only use reward(m, s, a). Both can be implemented for a problem, but when reward(m, s, a) is implemented, it should be consistent with reward(m, s, a, sp[, o]), that is, it should be the expected value over all destination states and observations.\n\n\n\n\n\n","category":"function"},{"location":"api/#generative_api-1","page":"API Documentation","title":"Generative","text":"","category":"section"},{"location":"api/#","page":"API Documentation","title":"API Documentation","text":"These functions should return states, observations, and/or rewards.","category":"page"},{"location":"api/#","page":"API Documentation","title":"API Documentation","text":"note: Note\ngen in POMDPs.jl v0.8 corresponds to the generate_ functions in previous versions","category":"page"},{"location":"api/#","page":"API Documentation","title":"API Documentation","text":"gen\ninitialstate\ninitialobs","category":"page"},{"location":"api/#POMDPs.gen","page":"API Documentation","title":"POMDPs.gen","text":"gen(...)\n\nSample from generative model of a POMDP or MDP.\n\nThere are 3 versions:\n\nFor problem-writers, the most convenient version to implement is gen(m::Union{MDP,POMDP}, s, a, rng::AbstractRNG), which returns a NamedTuple.\nSolvers and simulators should use the version with a DDNOut argument.\nDefining behavior for and sampling from individual nodes of the dynamic decision network can be accomplished using the version with a DDNNode argument.\n\nSee below for detailed documentation for each type.\n\n\n\ngen(t::DDNOut{X}, m::Union{MDP,POMDP}, s, a, rng::AbstractRNG) where X\n\nSample values from several nodes in the dynamic decision network. \n\nAn implementation of this method is automatically provided by POMDPs.jl. Solvers and simulators should use this version. Problem writers may implement it directly in special cases (see the POMDPs.jl documentation for more information).\n\nArguments\n\nt::DDNOut: which DDN nodes the function should sample from.\nm: an MDP or POMDP model\ns: the current state\na: the action\nrng: a random number generator (Typically a MersenneTwister)\n\nReturn\n\nIf the DDNOut parameter, X, is a symbol, return a value sample from the corresponding node. If X is a tuple of symbols, return a Tuple of values sampled from the specified nodes.\n\nExamples\n\nLet m be an MDP or POMDP, s be a state of m, a be an action of m, and rng be an AbstractRNG.\n\ngen(DDNOut(:sp, :r), m, s, a, rng) returns a Tuple containing the next state and reward.\ngen(DDNOut(:sp, :o, :r), m, s, a, rng) returns a Tuple containing the next state, observation, and reward.\ngen(DDNOut(:sp), m, s, a, rng) returns the next state.\n\n\n\ngen(m::Union{MDP,POMDP}, s, a, rng::AbstractRNG)\n\nConvenience function for implementing the entire MDP/POMDP generative model in one function by returning a NamedTuple.\n\nThe NamedTuple version of gen is the most convenient for problem writers to implement. However, it should never be used directly by solvers or simulators. Instead solvers and simulators should use the version with a DDNOut first argument. \n\nArguments\n\nm: an MDP or POMDP model\ns: the current state\na: the action\nrng: a random number generator (Typically a MersenneTwister)\n\nReturn\n\nThe function should return a NamedTuple. Typically, this NamedTuple will be (sp=<next state>, r=<reward>) for an MDP or (sp=<next state>, o=<observation>, r=<reward>) for aPOMDP`.\n\n\n\ngen(v::DDNNode{name}, m::Union{MDP,POMDP}, depargs..., rng::AbstractRNG)\n\nSample a value from a node in the dynamic decision network. \n\nThese functions will be used within gen(::DDNOut, ...) to sample values for all outputs and their dependencies. They may be implemented directly by a problem-writer if they wish to implement a generative model for a particular node in the dynamic decision network, and may be called in solvers to sample a value for a particular node.\n\nArguments\n\nv::DDNNode{name}: which DDN node the function should sample from.\ndepargs: values for all the dependent nodes. Dependencies are determined by deps(DDNStructure(m), name).\nrng: a random number generator (Typically a MersenneTwister)\n\nReturn\n\nA sampled value from the specified node.\n\nExamples\n\nLet m be a POMDP, s and sp be states of m, a be an action of m, and rng be an AbstractRNG.\n\ngen(DDNNode(:sp), m, s, a, rng) returns the next state.\ngen(DDNNode(:o), m, s, a, sp, rng) returns the observation given the previous state, action, and new state.\n\n\n\n\n\n","category":"function"},{"location":"api/#POMDPs.initialstate","page":"API Documentation","title":"POMDPs.initialstate","text":"initialstate(m::Union{POMDP,MDP}, rng::AbstractRNG)\n\nReturn a sampled initial state for the problem m.\n\nUsually the initial state is sampled from an initial state distribution. The random number generator rng should be used to draw this sample (e.g. use rand(rng) instead of rand()).\n\n\n\n\n\n","category":"function"},{"location":"api/#POMDPs.initialobs","page":"API Documentation","title":"POMDPs.initialobs","text":"initialobs(m::POMDP, s, rng::AbstractRNG)\n\nReturn a sampled initial observation for the problem m and state s.\n\nThis function is only used in cases where the policy expects an initial observation rather than an initial belief, e.g. in a reinforcement learning setting. It is not used in a standard POMDP simulation.\n\nBy default, it will fall back to observation(m, s). The random number generator rng should be used to draw this sample (e.g. use rand(rng) instead of rand()).\n\n\n\n\n\n","category":"function"},{"location":"api/#common_api-1","page":"API Documentation","title":"Common","text":"","category":"section"},{"location":"api/#","page":"API Documentation","title":"API Documentation","text":"states\nactions\nobservations\nisterminal\ndiscount\nstateindex\nactionindex\nobsindex\nconvert_s\nconvert_a\nconvert_o","category":"page"},{"location":"api/#POMDPs.states","page":"API Documentation","title":"POMDPs.states","text":"states(problem::POMDP)\nstates(problem::MDP)\n\nReturns the complete state space of a POMDP. \n\n\n\n\n\n","category":"function"},{"location":"api/#POMDPs.actions","page":"API Documentation","title":"POMDPs.actions","text":"actions(m::Union{MDP,POMDP})\n\nReturns the entire action space of a (PO)MDP.\n\n\n\nactions(m::Union{MDP,POMDP}, s)\n\nReturn the actions that can be taken from state s.\n\n\n\nactions(m::POMDP, b)\n\nReturn the actions that can be taken from belief b.\n\nTo implement an observation-dependent action space, use currentobs(b) to get the observation associated with belief b within the implementation of actions(m, b).\n\n\n\n\n\n","category":"function"},{"location":"api/#POMDPs.observations","page":"API Documentation","title":"POMDPs.observations","text":"observations(problem::POMDP)\n\nReturn the entire observation space.\n\n\n\n\n\n","category":"function"},{"location":"api/#POMDPs.isterminal","page":"API Documentation","title":"POMDPs.isterminal","text":"isterminal(m::Union{MDP,POMDP}, s)\n\nCheck if state s is terminal.\n\nIf a state is terminal, no actions will be taken in it and no additional rewards will be accumulated. Thus, the value at such a state is, by definition, zero.\n\n\n\n\n\n","category":"function"},{"location":"api/#POMDPs.discount","page":"API Documentation","title":"POMDPs.discount","text":"discount(problem::POMDP)\ndiscount(problem::MDP)\n\nReturn the discount factor for the problem.\n\n\n\n\n\n","category":"function"},{"location":"api/#POMDPs.stateindex","page":"API Documentation","title":"POMDPs.stateindex","text":"stateindex(problem::POMDP, s)\nstateindex(problem::MDP, s)\n\nReturn the integer index of state s. Used for discrete models only.\n\n\n\n\n\n","category":"function"},{"location":"api/#POMDPs.actionindex","page":"API Documentation","title":"POMDPs.actionindex","text":"actionindex(problem::POMDP, a)\nactionindex(problem::MDP, a)\n\nReturn the integer index of action a. Used for discrete models only.\n\n\n\n\n\n","category":"function"},{"location":"api/#POMDPs.obsindex","page":"API Documentation","title":"POMDPs.obsindex","text":"obsindex(problem::POMDP, o)\n\nReturn the integer index of observation o. Used for discrete models only.\n\n\n\n\n\n","category":"function"},{"location":"api/#POMDPs.convert_s","page":"API Documentation","title":"POMDPs.convert_s","text":"convert_s(::Type{V}, s, problem::Union{MDP,POMDP}) where V<:AbstractArray\nconvert_s(::Type{S}, vec::V, problem::Union{MDP,POMDP}) where {S,V<:AbstractArray}\n\nConvert a state to vectorized form or vice versa.\n\n\n\n\n\n","category":"function"},{"location":"api/#POMDPs.convert_a","page":"API Documentation","title":"POMDPs.convert_a","text":"convert_a(::Type{V}, a, problem::Union{MDP,POMDP}) where V<:AbstractArray\nconvert_a(::Type{A}, vec::V, problem::Union{MDP,POMDP}) where {A,V<:AbstractArray}\n\nConvert an action to vectorized form or vice versa.\n\n\n\n\n\n","category":"function"},{"location":"api/#POMDPs.convert_o","page":"API Documentation","title":"POMDPs.convert_o","text":"convert_o(::Type{V}, o, problem::Union{MDP,POMDP}) where V<:AbstractArray\nconvert_o(::Type{O}, vec::V, problem::Union{MDP,POMDP}) where {O,V<:AbstractArray}\n\nConvert an observation to vectorized form or vice versa.\n\n\n\n\n\n","category":"function"},{"location":"api/#Distribution/Space-Functions-1","page":"API Documentation","title":"Distribution/Space Functions","text":"","category":"section"},{"location":"api/#","page":"API Documentation","title":"API Documentation","text":"rand\npdf\nmode\nmean\ndimensions\nsupport","category":"page"},{"location":"api/#Base.rand","page":"API Documentation","title":"Base.rand","text":"rand(rng::AbstractRNG, d::Any)\n\nReturn a random element from distribution or space d.\n\nIf d is a state or transition distribution, the sample will be a state; if d is an action distribution, the sample will be an action or if d is an observation distribution, the sample will be an observation.\n\n\n\n\n\n","category":"function"},{"location":"api/#Distributions.pdf","page":"API Documentation","title":"Distributions.pdf","text":"pdf(d::Any, x::Any)\n\nEvaluate the probability density of distribution d at sample x.\n\n\n\n\n\n","category":"function"},{"location":"api/#StatsBase.mode","page":"API Documentation","title":"StatsBase.mode","text":"mode(d::Any)\n\nReturn the most likely value in a distribution d.\n\n\n\n\n\n","category":"function"},{"location":"api/#Statistics.mean","page":"API Documentation","title":"Statistics.mean","text":"mean(d::Any)\n\nReturn the mean of a distribution d.\n\n\n\n\n\n","category":"function"},{"location":"api/#POMDPs.dimensions","page":"API Documentation","title":"POMDPs.dimensions","text":"dimensions(s::Any)\n\nReturns the number of dimensions in space s.\n\n\n\n\n\n","category":"function"},{"location":"api/#Distributions.support","page":"API Documentation","title":"Distributions.support","text":"support(d::Any)\n\nReturn an iterable object containing the possible values that can be sampled from distribution d. Values with zero probability may be skipped.\n\n\n\n\n\n","category":"function"},{"location":"api/#Dynamic-decision-networks-1","page":"API Documentation","title":"Dynamic decision networks","text":"","category":"section"},{"location":"api/#","page":"API Documentation","title":"API Documentation","text":"DDNStructure\nDDNNode\nDDNOut\nDistributionDDNNode\nFunctionDDNNode\nConstantDDNNode\nGenericDDNNode","category":"page"},{"location":"api/#POMDPs.DDNStructure","page":"API Documentation","title":"POMDPs.DDNStructure","text":"DDNStructure(::Type{M}) where M <: Union{MDP, POMDP}\n\nTrait of an MDP/POMDP type for describing the structure of the dynamic Baysian network.\n\nExample\n\nstruct MyMDP <: MDP{Int, Int} end\nPOMDPs.gen(::MyMDP, s, a, rng) = (sp=s+a+rand(rng, [1,2,3]), r=s^2)\n\n# make a new node, delta_s, that is deterministically equal to sp - s\nfunction POMDPs.DDNStructure(::Type{MyMDP})\n    ddn = mdp_ddn()\n    return add_node(ddn, :delta_s, FunctionDDNNode((m,s,sp)->sp-s), (:s, :sp))\nend\n\ngen(DDNOut(:delta_s), MyMDP(), 1, 1, Random.GLOBAL_RNG)\n\n\n\n\n\n","category":"type"},{"location":"api/#POMDPs.DDNNode","page":"API Documentation","title":"POMDPs.DDNNode","text":"DDNNode(x::Symbol)\nDDNNode{x::Symbol}()\n\nReference to a named node in the POMDP or MDP dynamic decision network (DDN).\n\nNote that gen(::DDNNode, m, depargs..., rng) always takes an argument for each dependency whereas gen(::DDNOut, m, s, a, rng) only takes s and a arguments (the inputs to the entire DDN).\n\nDDNNode is a \"value type\". See the documentation of Val for more conceptual details about value types.\n\n\n\n\n\n","category":"type"},{"location":"api/#POMDPs.DDNOut","page":"API Documentation","title":"POMDPs.DDNOut","text":"DDNOut(x::Symbol)\nDDNOut{x::Symbol}()\nDDNOut(::Symbol, ::Symbol,...)\nDDNOut{x::NTuple{N, Symbol}}()\n\nReference to one or more named nodes in the POMDP or MDP dynamic decision network (DDN).\n\nNote that gen(::DDNOut, m, s, a, rng) always takes s and a arguments (the inputs to the entire DDN) while gen(::DDNNode, m, depargs..., rng) takes a variable number of arguments (one for each dependency).\n\nDDNOut is a \"value type\". See the documentation of Val for more conceptual details about value types.\n\n\n\n\n\n","category":"type"},{"location":"api/#POMDPs.DistributionDDNNode","page":"API Documentation","title":"POMDPs.DistributionDDNNode","text":"DDN node defined by a function that maps the model and values from the parent nodes to a distribution\n\nExample\n\nDistributionDDNNode((m, s, a)->POMDPModelTools.Deterministic(s+a))\n\n\n\n\n\n","category":"type"},{"location":"api/#POMDPs.FunctionDDNNode","page":"API Documentation","title":"POMDPs.FunctionDDNNode","text":"DDN node defined by a function that determinisitically maps the model and values from the parent nodes to a new value.\n\nExample\n\nFunctionDDNNode((m, s, a)->s+a)\n\n\n\n\n\n","category":"type"},{"location":"api/#POMDPs.ConstantDDNNode","page":"API Documentation","title":"POMDPs.ConstantDDNNode","text":"DDN node that always takes a deterministic constant value.\n\n\n\n\n\n","category":"type"},{"location":"api/#POMDPs.GenericDDNNode","page":"API Documentation","title":"POMDPs.GenericDDNNode","text":"DDN node that can only have a generative model; gen(::DDNNode{:x}, ...) must be implemented for a node of this type.\n\n\n\n\n\n","category":"type"},{"location":"api/#Belief-Functions-1","page":"API Documentation","title":"Belief Functions","text":"","category":"section"},{"location":"api/#","page":"API Documentation","title":"API Documentation","text":"update\ninitialize_belief\nhistory\ncurrentobs","category":"page"},{"location":"api/#POMDPs.update","page":"API Documentation","title":"POMDPs.update","text":"update(updater::Updater, belief_old, action, observation)\n\nReturn a new instance of an updated belief given belief_old and the latest action and observation.\n\n\n\n\n\n","category":"function"},{"location":"api/#POMDPs.initialize_belief","page":"API Documentation","title":"POMDPs.initialize_belief","text":"initialize_belief(updater::Updater,\n                     state_distribution::Any)\ninitialize_belief(updater::Updater, belief::Any)\n\nReturns a belief that can be updated using updater that has similar distribution to state_distribution or belief.\n\nThe conversion may be lossy. This function is also idempotent, i.e. there is a default implementation that passes the belief through when it is already the correct type: initialize_belief(updater::Updater, belief) = belief\n\n\n\n\n\n","category":"function"},{"location":"api/#POMDPs.history","page":"API Documentation","title":"POMDPs.history","text":"history(b)\n\nReturn the action-observation history associated with belief b.\n\nThe history should be an AbstractVector, Tuple, (or similar object that supports indexing with end) full of NamedTuples with keys :a and :o, i.e. history(b)[end][:a] should be the last action taken leading up to b, and history(b)[end][:o] should be the last observation received.\n\nIt is acceptable to return only part of the history if that is all that is available, but it should always end with the current observation. For example, it would be acceptable to return a structure containing only the last three observations in a length 3 Vector{NamedTuple{(:o,),Tuple{O}}.\n\n\n\n\n\n","category":"function"},{"location":"api/#POMDPs.currentobs","page":"API Documentation","title":"POMDPs.currentobs","text":"currentobs(b)\n\nReturn the latest observation associated with belief b.\n\nIf a solver or updater implements history(b) for a belief type, currentobs has a default implementation.\n\n\n\n\n\n","category":"function"},{"location":"api/#Policy-and-Solver-Functions-1","page":"API Documentation","title":"Policy and Solver Functions","text":"","category":"section"},{"location":"api/#","page":"API Documentation","title":"API Documentation","text":"solve\nupdater\naction\nvalue","category":"page"},{"location":"api/#POMDPs.solve","page":"API Documentation","title":"POMDPs.solve","text":"solve(solver::Solver, problem::POMDP)\n\nSolves the POMDP using method associated with solver, and returns a policy.\n\n\n\n\n\n","category":"function"},{"location":"api/#POMDPs.updater","page":"API Documentation","title":"POMDPs.updater","text":"updater(policy::Policy)\n\nReturns a default Updater appropriate for a belief type that policy p can use\n\n\n\n\n\n","category":"function"},{"location":"api/#POMDPs.action","page":"API Documentation","title":"POMDPs.action","text":"action(policy::Policy, x)\n\nReturns the action that the policy deems best for the current state or belief, x.\n\nx is a generalized information state - can be a state in an MDP, a distribution in POMDP, or another specialized policy-dependent representation of the information needed to choose an action.\n\n\n\n\n\n","category":"function"},{"location":"api/#POMDPs.value","page":"API Documentation","title":"POMDPs.value","text":"value(p::Policy, s)\nvalue(p::Policy, s, a)\n\nReturns the utility value from policy p given the state (or belief), or state-action (or belief-action) pair.\n\nThe state-action version is commonly referred to as the Q-value.\n\n\n\n\n\n","category":"function"},{"location":"api/#Simulator-1","page":"API Documentation","title":"Simulator","text":"","category":"section"},{"location":"api/#","page":"API Documentation","title":"API Documentation","text":"Simulator\nsimulate","category":"page"},{"location":"api/#POMDPs.Simulator","page":"API Documentation","title":"POMDPs.Simulator","text":"Base type for an object defining how simulations should be carried out.\n\n\n\n\n\n","category":"type"},{"location":"api/#POMDPs.simulate","page":"API Documentation","title":"POMDPs.simulate","text":"simulate(sim::Simulator, m::POMDP, p::Policy, u::Updater=updater(p), b0=initialstate_distribution(m), s0=initialstate(m, rng))\nsimulate(sim::Simulator, m::MDP, p::Policy, s0=initialstate(m, rng))\n\nRun a simulation using the specified policy.\n\nThe return type is flexible and depends on the simulator. Simulations should adhere to the Simulation Standard.\n\n\n\n\n\n","category":"function"},{"location":"api/#Other-1","page":"API Documentation","title":"Other","text":"","category":"section"},{"location":"api/#","page":"API Documentation","title":"API Documentation","text":"The following functions are not part of the API for specifying and solving POMDPs, but are included in the package.","category":"page"},{"location":"api/#Type-Inference-1","page":"API Documentation","title":"Type Inference","text":"","category":"section"},{"location":"api/#","page":"API Documentation","title":"API Documentation","text":"statetype\nactiontype\nobstype","category":"page"},{"location":"api/#POMDPs.statetype","page":"API Documentation","title":"POMDPs.statetype","text":"statetype(t::Type)\nstatetype(p::Union{POMDP,MDP})\n\nReturn the state type for a problem type (the S in POMDP{S,A,O}).\n\ntype A <: POMDP{Int, Bool, Bool} end\n\nstatetype(A) # returns Int\n\n\n\n\n\n","category":"function"},{"location":"api/#POMDPs.actiontype","page":"API Documentation","title":"POMDPs.actiontype","text":"actiontype(t::Type)\nactiontype(p::Union{POMDP,MDP})\n\nReturn the state type for a problem type (the S in POMDP{S,A,O}).\n\ntype A <: POMDP{Bool, Int, Bool} end\n\nactiontype(A) # returns Int\n\n\n\n\n\n","category":"function"},{"location":"api/#POMDPs.obstype","page":"API Documentation","title":"POMDPs.obstype","text":"obstype(t::Type)\n\nReturn the state type for a problem type (the S in POMDP{S,A,O}).\n\ntype A <: POMDP{Bool, Bool, Int} end\n\nobstype(A) # returns Int\n\n\n\n\n\n","category":"function"},{"location":"api/#Requirements-Specification-1","page":"API Documentation","title":"Requirements Specification","text":"","category":"section"},{"location":"api/#","page":"API Documentation","title":"API Documentation","text":"check_requirements\nshow_requirements\nget_requirements\nrequirements_info\n@POMDP_require\n@POMDP_requirements\n@requirements_info\n@get_requirements\n@show_requirements\n@warn_requirements\n@req\n@subreq\nimplemented","category":"page"},{"location":"api/#POMDPs.check_requirements","page":"API Documentation","title":"POMDPs.check_requirements","text":"check_requirements(r::AbstractRequirementSet)\n\nCheck whether the methods in r have implementations with implemented(). Return true if all methods have implementations.\n\n\n\n\n\n","category":"function"},{"location":"api/#POMDPs.show_requirements","page":"API Documentation","title":"POMDPs.show_requirements","text":"show_requirements(r::AbstractRequirementSet)\n\nCheck whether the methods in r have implementations with implemented() and print out a formatted list showing which are missing. Return true if all methods have implementations.\n\n\n\n\n\n","category":"function"},{"location":"api/#POMDPs.get_requirements","page":"API Documentation","title":"POMDPs.get_requirements","text":"get_requirements(f::Function, args::Tuple)\n\nReturn a RequirementSet for the function f and arguments args.\n\n\n\n\n\n","category":"function"},{"location":"api/#POMDPs.requirements_info","page":"API Documentation","title":"POMDPs.requirements_info","text":"requirements_info(s::Solver, p::Union{POMDP,MDP}, ...)\n\nPrint information about the requirement for solver s.\n\n\n\n\n\n","category":"function"},{"location":"api/#POMDPs.@POMDP_require","page":"API Documentation","title":"POMDPs.@POMDP_require","text":"@POMDP_require solve(s::CoolSolver, p::POMDP) begin\n    PType = typeof(p)\n    @req states(::PType)\n    @req actions(::PType)\n    @req transition(::PType, ::S, ::A)\n    s = first(states(p))\n    a = first(actions(p))\n    t_dist = transition(p, s, a)\n    @req rand(::AbstractRNG, ::typeof(t_dist))\nend\n\nCreate a get_requirements implementation for the function signature and the requirements block.\n\n\n\n\n\n","category":"macro"},{"location":"api/#POMDPs.@POMDP_requirements","page":"API Documentation","title":"POMDPs.@POMDP_requirements","text":"reqs = @POMDP_requirements CoolSolver begin\n    PType = typeof(p)\n    @req states(::PType)\n    @req actions(::PType)\n    @req transition(::PType, ::S, ::A)\n    s = first(states(p))\n    a = first(actions(p))\n    t_dist = transition(p, s, a)\n    @req rand(::AbstractRNG, ::typeof(t_dist))\nend\n\nCreate a RequirementSet object.\n\n\n\n\n\n","category":"macro"},{"location":"api/#POMDPs.@requirements_info","page":"API Documentation","title":"POMDPs.@requirements_info","text":"@requirements_info ASolver() [YourPOMDP()]\n\nPrint information about the requirements for a solver.\n\n\n\n\n\n","category":"macro"},{"location":"api/#POMDPs.@get_requirements","page":"API Documentation","title":"POMDPs.@get_requirements","text":"@get_requirements f(arg1, arg2)\n\nCall get_requirements(f, (arg1,arg2)).\n\n\n\n\n\n","category":"macro"},{"location":"api/#POMDPs.@show_requirements","page":"API Documentation","title":"POMDPs.@show_requirements","text":"@show_requirements solve(solver, problem)\n\nPrint a a list of requirements for a function call.\n\n\n\n\n\n","category":"macro"},{"location":"api/#POMDPs.@warn_requirements","page":"API Documentation","title":"POMDPs.@warn_requirements","text":"@warn_requirements solve(solver, problem)\n\nPrint a warning if there are unmet requirements.\n\n\n\n\n\n","category":"macro"},{"location":"api/#POMDPs.@req","page":"API Documentation","title":"POMDPs.@req","text":"@req f( ::T1, ::T2)\n\nConvert a f( ::T1, ::T2) expression to a (f, Tuple{T1,T2})::Req for pushing to a RequirementSet.\n\nIf in a @POMDP_requirements or @POMDP_require block, marks the requirement for including in the set of requirements.\n\n\n\n\n\n","category":"macro"},{"location":"api/#POMDPs.@subreq","page":"API Documentation","title":"POMDPs.@subreq","text":"@subreq f(arg1, arg2)\n\nIn a @POMDP_requirements or @POMDP_require block, include the requirements for f(arg1, arg2) as a child argument set.\n\n\n\n\n\n","category":"macro"},{"location":"api/#POMDPs.implemented","page":"API Documentation","title":"POMDPs.implemented","text":"implemented(function, Tuple{Arg1Type, Arg2Type})\n\nCheck whether there is an implementation available that will return a suitable value.\n\n\n\n\n\n","category":"function"},{"location":"api/#Utility-Tools-1","page":"API Documentation","title":"Utility Tools","text":"","category":"section"},{"location":"api/#","page":"API Documentation","title":"API Documentation","text":"add_registry\navailable","category":"page"},{"location":"api/#POMDPs.add_registry","page":"API Documentation","title":"POMDPs.add_registry","text":"add_registry()\n\nAdds the JuliaPOMDP registry\n\n\n\n\n\n","category":"function"},{"location":"api/#POMDPs.available","page":"API Documentation","title":"POMDPs.available","text":"available()\n\nPrints all the available packages in the JuliaPOMDP registry\n\n\n\n\n\n","category":"function"},{"location":"run_simulation/#Running-Simulations-1","page":"Running Simulations","title":"Running Simulations","text":"","category":"section"},{"location":"run_simulation/#","page":"Running Simulations","title":"Running Simulations","text":"Running a simulation consists of two steps, creating a simulator and calling the simulate function. For example, given a POMDP or MDP model m, and a policy p, one can use the Rollout Simulator from the POMDPSimulators package to find the accumulated discounted reward from a single simulated trajectory as follows:","category":"page"},{"location":"run_simulation/#","page":"Running Simulations","title":"Running Simulations","text":"sim = RolloutSimulator()\nr = simulate(sim, m, p)","category":"page"},{"location":"run_simulation/#","page":"Running Simulations","title":"Running Simulations","text":"More inputs, such as a belief updater, initial state, initial belief, etc. may be specified as arguments to simulate. See the docstring for simulate and the appropriate \"Input\" sections in the Simulation Standard page for more information.","category":"page"},{"location":"run_simulation/#","page":"Running Simulations","title":"Running Simulations","text":"More examples can be found in the POMDPExamples package. A variety of simulators that return more information and interact in different ways can be found in the POMDPSimulators package.","category":"page"},{"location":"simulation/#Simulation-Standard-1","page":"Simulation Standard","title":"Simulation Standard","text":"","category":"section"},{"location":"simulation/#","page":"Simulation Standard","title":"Simulation Standard","text":"Important note: In most cases, users need not implement their own simulators. Several simulators that are compatible with the standard in this document are implemented in the POMDPSimulators package and allow interaction from a variety of perspectives. Moreover RLInterface.jl provides an OpenAI Gym style environment interface to interact with environments that is more flexible in some cases.","category":"page"},{"location":"simulation/#","page":"Simulation Standard","title":"Simulation Standard","text":"In order to maintain consistency across the POMDPs.jl ecosystem, this page defines a standard for how simulations should be conducted. All simulators should be consistent with this page, and, if solvers are attempting to find an optimal POMDP policy, they should optimize the expected value of r_total below. In particular, this page should be consulted when questions about how less-obvious concepts like terminal states are handled.","category":"page"},{"location":"simulation/#POMDP-Simulation-1","page":"Simulation Standard","title":"POMDP Simulation","text":"","category":"section"},{"location":"simulation/#Inputs-1","page":"Simulation Standard","title":"Inputs","text":"","category":"section"},{"location":"simulation/#","page":"Simulation Standard","title":"Simulation Standard","text":"In general, POMDP simulations take up to 5 inputs (see also the simulate docstring):","category":"page"},{"location":"simulation/#","page":"Simulation Standard","title":"Simulation Standard","text":"pomdp::POMDP: pomdp model object (see POMDPs and MDPs)\npolicy::Policy: policy (see Solvers and Policies)\nup::Updater: belief updater (see Beliefs and Updaters)\nisd: initial state distribution\ns: initial state","category":"page"},{"location":"simulation/#","page":"Simulation Standard","title":"Simulation Standard","text":"The last three of these inputs are optional. If they are not explicitly provided, they should be inferred using the following POMDPs.jl functions:","category":"page"},{"location":"simulation/#","page":"Simulation Standard","title":"Simulation Standard","text":"up =updater(policy)\nisd =initialstate_distribution(pomdp)\ns =initialstate(pomdp, rng)","category":"page"},{"location":"simulation/#","page":"Simulation Standard","title":"Simulation Standard","text":"In addition, a random number generator rng is assumed to be available.","category":"page"},{"location":"simulation/#Simulation-Loop-1","page":"Simulation Standard","title":"Simulation Loop","text":"","category":"section"},{"location":"simulation/#","page":"Simulation Standard","title":"Simulation Standard","text":"The main simulation loop is shown below. Note that the isterminal check prevents any actions from being taken and reward from being collected from a terminal state.","category":"page"},{"location":"simulation/#","page":"Simulation Standard","title":"Simulation Standard","text":"Before the loop begins, initialize_belief is called to create the belief based on the initial state distribution - this is especially important when the belief is solver specific, such as the finite-state-machine used by MCVI. ","category":"page"},{"location":"simulation/#","page":"Simulation Standard","title":"Simulation Standard","text":"b = initialize_belief(up, isd)\n\nr_total = 0.0\nd = 1.0\nwhile !isterminal(pomdp, s)\n    a = action(policy, b)\n    s, o, r = gen(DDNOut(:sp,:o,:r), pomdp, s, a, rng)\n    r_total += d*r\n    d *= discount(pomdp)\n    b = update(up, b, a, o)\nend","category":"page"},{"location":"simulation/#","page":"Simulation Standard","title":"Simulation Standard","text":"In terms of the explicit interface, gen above can be expressed as:","category":"page"},{"location":"simulation/#","page":"Simulation Standard","title":"Simulation Standard","text":"function gen(::DDNOut{(:sp,:o,:r)}, pomdp, s, a, rng)\n    sp = rand(rng, transition(pomdp, s, a))\n    o = rand(rng, observation(pomdp, s, a, sp))\n    r = reward(pomdp, s, a, sp)\n    return sp, o, r\nend","category":"page"},{"location":"simulation/#MDP-Simulation-1","page":"Simulation Standard","title":"MDP Simulation","text":"","category":"section"},{"location":"simulation/#Inputs-2","page":"Simulation Standard","title":"Inputs","text":"","category":"section"},{"location":"simulation/#","page":"Simulation Standard","title":"Simulation Standard","text":"In general, MDP simulations take up to 3 inputs (see also the simulate docstring):","category":"page"},{"location":"simulation/#","page":"Simulation Standard","title":"Simulation Standard","text":"mdp::MDP: mdp model object (see POMDPs and MDPs)\npolicy::Policy: policy (see Solvers and Policies)\ns: initial state","category":"page"},{"location":"simulation/#","page":"Simulation Standard","title":"Simulation Standard","text":"The last of these inputs is optional. If the initial state is not explicitly provided, it should be generated using","category":"page"},{"location":"simulation/#","page":"Simulation Standard","title":"Simulation Standard","text":"s =initialstate(mdp, rng)","category":"page"},{"location":"simulation/#","page":"Simulation Standard","title":"Simulation Standard","text":"In addition, a random number generator rng is assumed to be available.","category":"page"},{"location":"simulation/#Simulation-Loop-2","page":"Simulation Standard","title":"Simulation Loop","text":"","category":"section"},{"location":"simulation/#","page":"Simulation Standard","title":"Simulation Standard","text":"The main simulation loop is shown below. Note again that the isterminal check prevents any actions from being taken and reward from being collected from a terminal state.","category":"page"},{"location":"simulation/#","page":"Simulation Standard","title":"Simulation Standard","text":"r_total = 0.0\ndisc = 1.0\nwhile !isterminal(mdp, s)\n    a = action(policy, s)\n    s, r = gen(DDNOut(:sp,:r), mdp, s, a, rng)\n    r_total += d*r\n    d *= discount(mdp)\nend","category":"page"},{"location":"simulation/#","page":"Simulation Standard","title":"Simulation Standard","text":"In terms of the explicit interface, gen above can be expressed as:","category":"page"},{"location":"simulation/#","page":"Simulation Standard","title":"Simulation Standard","text":"function gen(::DDNOut{(:sp,:r)}, mdp, s, a, rng)\n    sp = rand(rng, transition(pomdp, s, a))\n    r = reward(pomdp, s, a, sp)\n    return sp, r\nend","category":"page"},{"location":"requirements/#requirements-1","page":"Interface Requirements for Problems","title":"Interface Requirements for Problems","text":"","category":"section"},{"location":"requirements/#","page":"Interface Requirements for Problems","title":"Interface Requirements for Problems","text":"Due to the large variety of problems that can be expressed as MDPs and POMDPs and the wide variety of solution techniques available, there is considerable variation in which of the POMDPs.jl interface functions must be implemented to use each solver. No solver requires all of the functions in the interface, so it is wise to determine which functions are needed before jumping into implementation.","category":"page"},{"location":"requirements/#","page":"Interface Requirements for Problems","title":"Interface Requirements for Problems","text":"Solvers can communicate these requirements through the @requirements_info and @show_requirements macros. @requirements_info should give an overview of the requirements for a solver, which is supplied as the first argument, the macro can usually be more informative if a problem is specified as the second arg. For example, if you are implementing a new problem NewMDP and want to use the DiscreteValueIteration solver, you might run the following:","category":"page"},{"location":"requirements/#","page":"Interface Requirements for Problems","title":"Interface Requirements for Problems","text":"(Image: requirements_info for a new problem)","category":"page"},{"location":"requirements/#","page":"Interface Requirements for Problems","title":"Interface Requirements for Problems","text":"Note that a few of the requirements could not be shown because actions is not implemented for the new problem.","category":"page"},{"location":"requirements/#","page":"Interface Requirements for Problems","title":"Interface Requirements for Problems","text":"If you would like to see a list of all of the requirements for a solver, try running @requirements_info with a fully implemented model from POMDPModels, for example,","category":"page"},{"location":"requirements/#","page":"Interface Requirements for Problems","title":"Interface Requirements for Problems","text":"(Image: requirements_info for a fully-implemented problem)","category":"page"},{"location":"requirements/#","page":"Interface Requirements for Problems","title":"Interface Requirements for Problems","text":"@show_requirements is a lower-level tool that can be used to show the requirements for a specific function call, for example","category":"page"},{"location":"requirements/#","page":"Interface Requirements for Problems","title":"Interface Requirements for Problems","text":"@show_requirements solve(ValueIterationSolver(), NewMDP())","category":"page"},{"location":"requirements/#","page":"Interface Requirements for Problems","title":"Interface Requirements for Problems","text":"or","category":"page"},{"location":"requirements/#","page":"Interface Requirements for Problems","title":"Interface Requirements for Problems","text":"policy = solve(ValueIterationSolver(), GridWorld())\n@show_requirements action(policy, GridWorldState(1,1))","category":"page"},{"location":"requirements/#","page":"Interface Requirements for Problems","title":"Interface Requirements for Problems","text":"In some cases, a solver writer may not have specified the requirements, in which case the requirements query macros will output","category":"page"},{"location":"requirements/#","page":"Interface Requirements for Problems","title":"Interface Requirements for Problems","text":"[No requirements specified]","category":"page"},{"location":"requirements/#","page":"Interface Requirements for Problems","title":"Interface Requirements for Problems","text":"In this case, please file an issue on the solver's github page to encourage the solver writer to specify requirements.","category":"page"},{"location":"explicit/#explicit_doc-1","page":"Explicit (PO)MDP Interface","title":"Explicit (PO)MDP Interface","text":"","category":"section"},{"location":"explicit/#","page":"Explicit (PO)MDP Interface","title":"Explicit (PO)MDP Interface","text":"When using the explicit interface, the transition and observation probabilities must be explicitly defined.","category":"page"},{"location":"explicit/#","page":"Explicit (PO)MDP Interface","title":"Explicit (PO)MDP Interface","text":"note: Note\nThere is no requirement that a problem defined using the explicit interface be discrete; it is straightforward to define continuous POMDPs with the explicit interface, provided that the distributions have some finite parameterization.","category":"page"},{"location":"explicit/#Explicit-(PO)MDP-interface-1","page":"Explicit (PO)MDP Interface","title":"Explicit (PO)MDP interface","text":"","category":"section"},{"location":"explicit/#","page":"Explicit (PO)MDP Interface","title":"Explicit (PO)MDP Interface","text":"The explicit interface consists of the following functions:","category":"page"},{"location":"explicit/#","page":"Explicit (PO)MDP Interface","title":"Explicit (PO)MDP Interface","text":"initialstate_distribution(pomdp) specifies the initial distribution of states for a problem (this is also translated to the initial belief for pomdps).\ntransition(pomdp, s, a) defines the state transition probability distribution for state s and action a. This defines an explicit model for the :sp DDN node.\nobservation(pomdp, [s,] a, sp) defines the observation distribution given that action a was taken and the state is now sp (The observation can optionally depend on s - see docstring). This defines an explicit model for the :o DDN node.\nreward(pomdp, s, a[, sp[, o]]) defines the reward, which is a deterministic function of the state and action (and optionally sp and o - see docstring). This defines an explicit model for the :r DDN node.","category":"page"},{"location":"explicit/#","page":"Explicit (PO)MDP Interface","title":"Explicit (PO)MDP Interface","text":"transition, observation, and initialstate_distribution should return distribution objects that implement part or all of the distribution interface. Some predefined distributions can be found in Distributions.jl or POMDPModelTools.jl, or custom types that represent distributions appropriate for the problem may be created.","category":"page"},{"location":"explicit/#Example-1","page":"Explicit (PO)MDP Interface","title":"Example","text":"","category":"section"},{"location":"explicit/#","page":"Explicit (PO)MDP Interface","title":"Explicit (PO)MDP Interface","text":"An example of defining a problem using the explicit interface can be found at:  https://github.com/JuliaPOMDP/POMDPExamples.jl/blob/master/notebooks/Defining-a-POMDP-with-the-Explicit-Interface.ipynb","category":"page"},{"location":"generative/#generative_doc-1","page":"Generative (PO)MDP Interface","title":"Generative (PO)MDP Interface","text":"","category":"section"},{"location":"generative/#Quick-Start-1","page":"Generative (PO)MDP Interface","title":"Quick Start","text":"","category":"section"},{"location":"generative/#","page":"Generative (PO)MDP Interface","title":"Generative (PO)MDP Interface","text":"A generative model for most (PO)MDPs can be completely defined with one function:","category":"page"},{"location":"generative/#","page":"Generative (PO)MDP Interface","title":"Generative (PO)MDP Interface","text":"function POMDPs.gen(m::YourPOMDPType, s, a, rng)\n    # do dynamics/transition calculations here\n    return (sp= #=new state=#, r= #=reward=#, o= #=observation=#)\nend","category":"page"},{"location":"generative/#","page":"Generative (PO)MDP Interface","title":"Generative (PO)MDP Interface","text":"(o is not needed for MDPs.)","category":"page"},{"location":"generative/#Interface-Description-1","page":"Generative (PO)MDP Interface","title":"Interface Description","text":"","category":"section"},{"location":"generative/#","page":"Generative (PO)MDP Interface","title":"Generative (PO)MDP Interface","text":"The generative interface consists of two functions:","category":"page"},{"location":"generative/#","page":"Generative (PO)MDP Interface","title":"Generative (PO)MDP Interface","text":"gen returns samples (e.g. states, observations and rewards) from a generative POMDP model.\ninitialstate returns a sampled initial state.","category":"page"},{"location":"generative/#","page":"Generative (PO)MDP Interface","title":"Generative (PO)MDP Interface","text":"The generative interface is typically used when it is easier to return sampled states and observations rather than explicit distributions as in the Explicit interface. This type of model is often referred to as a \"black-box\" model.","category":"page"},{"location":"generative/#","page":"Generative (PO)MDP Interface","title":"Generative (PO)MDP Interface","text":"In some special cases (e.g. reinforcement learning with RLInterface.jl), an initial observation is needed before any actions are taken. In this case, the initialobs function will be used.","category":"page"},{"location":"generative/#The-[gen](@ref)-function-1","page":"Generative (PO)MDP Interface","title":"The gen function","text":"","category":"section"},{"location":"generative/#","page":"Generative (PO)MDP Interface","title":"Generative (PO)MDP Interface","text":"The gen function has three versions differentiated by the type of the first argument.","category":"page"},{"location":"generative/#","page":"Generative (PO)MDP Interface","title":"Generative (PO)MDP Interface","text":"gen(m::Union{POMDP, MDP}, s, a, rng) provides a way to implement a generative model for an entire (PO)MDP in a single function. It should return values for a subset of the DDN Nodes as a NamedTuple.\nThis is typically the quickest and easiest way to implement a new POMDP model or wrap an existing simulator.\nExample (defined by a problem writer): gen(m::MyPOMDP, s, a, rng) = (sp=s+a, r=s^2, o=s+a+randn(rng))\nThis version should never be called by a solver or simulator, since there is no guarantee of which values will be present in the returned object.\nValues for DDN nodes not present in the returned NamedTuple will be generated in the normal way with gen(::DDNNode, ...) or an explicit representation.\ngen(::DDNNode{nodename}, m, parent_values..., rng) defines the generative model for a single DDN node. Together, a group of these functions can define a problem.\nExample (defined by a problem writer): gen(::DDNNode{:o}, m::MyPOMDP, s, a, sp, rng) = sp + randn(rng)\nSolver writers should only directly call this version in very rare cases when it needs to access to values for a particular node of the DDN generated by specific values of its parent nodes.\ngen(::DDNOut{nodenames}, m, s, a, rng) returns a value or tuple of values for a subset of nodes in the DDN. The arguments are values for the input nodes (currently :s and :a), treating the entire DDN as a single black box.\nThis is the version that solvers and simulators should call.\nExample (called in a solver): sp, o, r = gen(DDNOut(:sp,:o,:r), m, s, a, rng)\nThis function is automatically synthesized by POMDPs.jl by combining gen(m, s, a, rng) and gen(::DDNNode, ...) or explicit model definitions for all DDN nodes.\nThis version should only be implemented directly by problem writers in very rare cases when they need precise control for efficiency.","category":"page"},{"location":"generative/#","page":"Generative (PO)MDP Interface","title":"Generative (PO)MDP Interface","text":"In all versions, m is a (PO)MDP model, and rng is a random number generator.","category":"page"},{"location":"generative/#Examples-1","page":"Generative (PO)MDP Interface","title":"Examples","text":"","category":"section"},{"location":"generative/#","page":"Generative (PO)MDP Interface","title":"Generative (PO)MDP Interface","text":"An example of defining a problem with the generative interface can be found in the POMDPExamples package.","category":"page"},{"location":"generative/#Random-number-generators-1","page":"Generative (PO)MDP Interface","title":"Random number generators","text":"","category":"section"},{"location":"generative/#","page":"Generative (PO)MDP Interface","title":"Generative (PO)MDP Interface","text":"The rng argument to functions in the generative interface is a random number generator such as Random.GLOBAL_RNG or another MersenneTwister. It should be used to generate all random numbers within the function (e.g. use rand(rng) instead of rand()). This will ensure that all simulations are exactly repeatable. See the Julia documentation on random numbers for more information about these objects.","category":"page"},{"location":"generative/#Performance-considerations-1","page":"Generative (PO)MDP Interface","title":"Performance considerations","text":"","category":"section"},{"location":"generative/#","page":"Generative (PO)MDP Interface","title":"Generative (PO)MDP Interface","text":"In general, calling gen(::DDNOut, ...) when gen(::POMDP, ...) is implemented does not introduce much overhead. In fact, in some cases, the compiler will even optimize out calculations of extra genvars. For example:","category":"page"},{"location":"generative/#","page":"Generative (PO)MDP Interface","title":"Generative (PO)MDP Interface","text":"struct M <: MDP{Int, Int} end\n\nPOMDPs.gen(::M, s, a, rng) = (sp=s+a, r=s^2)\n\n@code_warntype gen(DDNOut(:sp), M(), 1, 1, Random.GLOBAL_RNG)","category":"page"},{"location":"generative/#","page":"Generative (PO)MDP Interface","title":"Generative (PO)MDP Interface","text":"will yield","category":"page"},{"location":"generative/#","page":"Generative (PO)MDP Interface","title":"Generative (PO)MDP Interface","text":"Body::Int64\n1 ─ %1 = (Base.add_int)(s, a)::Int64\n│        nothing\n└──      return %1","category":"page"},{"location":"generative/#","page":"Generative (PO)MDP Interface","title":"Generative (PO)MDP Interface","text":"indicating that the compiler will only perform the addition to find the next state and skip the s^2 calculation for the reward.","category":"page"},{"location":"generative/#","page":"Generative (PO)MDP Interface","title":"Generative (PO)MDP Interface","text":"Unfortunately, if random numbers are used in gen, the compiler will not be able to optimize out the change in the rng's state, so it may be beneficial to directly implement versions of gen(::DDNNode, ...). For example","category":"page"},{"location":"generative/#","page":"Generative (PO)MDP Interface","title":"Generative (PO)MDP Interface","text":"POMDPs.gen(::DDNNode{:sp}, ::M, s, a, rng) = s+a\nPOMDPs.reward(::M, s, a) = abs(s)\nPODMPs.gen(::DDNNode{:o}, ::M, s, a, sp, rng) = sp+randn(rng)","category":"page"},{"location":"generative/#","page":"Generative (PO)MDP Interface","title":"Generative (PO)MDP Interface","text":"might be more efficient than","category":"page"},{"location":"generative/#","page":"Generative (PO)MDP Interface","title":"Generative (PO)MDP Interface","text":"function POMDPs.gen(::M, s, a, rng)\n    sp = s + a\n    return (sp=sp, r=abs(s), o=sp+randn(rng))\nend","category":"page"},{"location":"generative/#","page":"Generative (PO)MDP Interface","title":"Generative (PO)MDP Interface","text":"in the context of particle filtering.","category":"page"},{"location":"generative/#","page":"Generative (PO)MDP Interface","title":"Generative (PO)MDP Interface","text":"As always, though, one should resist the urge towards premature optimization; careful profiling to see what is actually slow is much more effective than speculation.","category":"page"},{"location":"def_solver/#Defining-a-Solver-1","page":"Defining a Solver","title":"Defining a Solver","text":"","category":"section"},{"location":"def_solver/#","page":"Defining a Solver","title":"Defining a Solver","text":"In this section, we will walk through an implementation of the QMDP algorithm. QMDP is the fully observable approximation of a POMDP policy, and relies on the Q-values to determine actions.","category":"page"},{"location":"def_solver/#Background-1","page":"Defining a Solver","title":"Background","text":"","category":"section"},{"location":"def_solver/#","page":"Defining a Solver","title":"Defining a Solver","text":"Let's say we are working with a POMDP defined by the tuple (mathcalS mathcalA mathcalZ T R O gamma), where mathcalS, mathcalA, mathcalZ are the discrete state, action, and observation spaces respectively. The QMDP algorithm assumes it is given a discrete POMDP. In our model T  mathcalS times mathcalA times mathcalS rightarrow 0 1 is the transition function, R mathcalS times mathcalA rightarrow mathbbR is the reward function, and O mathcalZ times mathcalA times mathcalS rightarrow 01 is the observation function. In a POMDP, our goal is to compute a policy pi that maps beliefs to actions pi b rightarrow a. For QMDP, a belief can be represented by a discrete probability distribution over the state space (although there may be other ways to define a belief in general and POMDPs.jl allows this flexibility).","category":"page"},{"location":"def_solver/#","page":"Defining a Solver","title":"Defining a Solver","text":"It can be shown (e.g. in [1], section 6.3.2) that the optimal value function for a POMDP can be written in terms of alpha vectors. In the QMDP approximation, there is a single alpha vector that corresponds to each action (alpha_a), and the policy is computed according to","category":"page"},{"location":"def_solver/#","page":"Defining a Solver","title":"Defining a Solver","text":"pi(b) = undersetatextargmax  alpha_a^Tb","category":"page"},{"location":"def_solver/#","page":"Defining a Solver","title":"Defining a Solver","text":"Thus, the alpha vectors can be used to compactly represent a QMDP policy.","category":"page"},{"location":"def_solver/#QMDP-Algorithm-1","page":"Defining a Solver","title":"QMDP Algorithm","text":"","category":"section"},{"location":"def_solver/#","page":"Defining a Solver","title":"Defining a Solver","text":"QMDP uses the columns of the Q-matrix obtained by solving the MDP defined by (mathcalS mathcalA T R gamma) (that is, the fully observable MDP that forms the basis for the POMDP we are trying to solve). If you are familiar with the value iteration algorithm for MDPs, the procedure for finding these alpha vectors is identical. Let's first initialize the alpha vectors alpha_a^0 = 0 for all s, and then iterate","category":"page"},{"location":"def_solver/#","page":"Defining a Solver","title":"Defining a Solver","text":"alpha_a^k+1(s) = R(sa) + gamma sum_s T(ssa) max_a alpha_a^k(s)","category":"page"},{"location":"def_solver/#","page":"Defining a Solver","title":"Defining a Solver","text":"After enough iterations, the alpha vectors converge to the QMDP approximation.","category":"page"},{"location":"def_solver/#","page":"Defining a Solver","title":"Defining a Solver","text":"Remember that QMDP is just an approximation method, and does not guarantee that the alpha vectors you obtain actually represent your POMDP value function. Specifically, QMDP has trouble in problems with information gathering actions (because we completely ignored the observation function when computing our policy). However, QMDP works very well in problems where a particular choice of action has little impact on the reduction in state uncertainty.","category":"page"},{"location":"def_solver/#Requirements-for-a-Solver-1","page":"Defining a Solver","title":"Requirements for a Solver","text":"","category":"section"},{"location":"def_solver/#","page":"Defining a Solver","title":"Defining a Solver","text":"Before getting into the implementation details, let's first go through what a POMDP solver must be able to do and support. We need three custom types that inherit from abstract types in POMDPs.jl. These type are Solver, Policy, and Updater. It is usually useful to have a custom type that represents the belief used by your policy as well.","category":"page"},{"location":"def_solver/#","page":"Defining a Solver","title":"Defining a Solver","text":"The requirements are as follows:","category":"page"},{"location":"def_solver/#","page":"Defining a Solver","title":"Defining a Solver","text":"# types\nQMDPSolver\nQMDPPolicy\nDiscreteUpdater # already implemented for us in BeliefUpdaters\nDiscreteBelief # already implemented for us in BeliefUpdaters\n# methods\nupdater(p::QMDPPolicy) # returns a belief updater suitable for use with QMDPPolicy\ninitialize_belief(bu::DiscreteUpdater, initial_state_dist) # returns a Discrete belief\nsolve(solver::QMDPSolver, pomdp::POMDP) # solves the POMDP and returns a policy\nupdate(bu::DiscreteUpdater, belief_old::DiscreteBelief, action, obs) # returns an updated belied (already implemented)\naction(policy::QMDPPolicy, b::DiscreteBelief) # returns a QMDP action","category":"page"},{"location":"def_solver/#","page":"Defining a Solver","title":"Defining a Solver","text":"You can find the implementations of these types and methods below.","category":"page"},{"location":"def_solver/#Defining-the-Solver-and-Policy-Types-1","page":"Defining a Solver","title":"Defining the Solver and Policy Types","text":"","category":"section"},{"location":"def_solver/#","page":"Defining a Solver","title":"Defining a Solver","text":"Let's first define the Solver type. The QMDP solver type should contain all the information needed to compute a policy (other than the problem itself). This information can be thought of as the hyperparameters of the solver. In QMDP, we only need two hyper-parameters. We may want to set the maximum number of iterations that the algorithm runs for, and a tolerance value (also known as the Bellman residual). Both of these quantities define terminating criteria for the algorithm. The algorithm stops either when the maximum number of iterations has been reached or when the infinity norm of the difference in utility values between two iterations goes below the tolerance value. The type definition has the form:","category":"page"},{"location":"def_solver/#","page":"Defining a Solver","title":"Defining a Solver","text":"using POMDPs # first load the POMDPs module\ntype QMDPSolver <: Solver\n    max_iterations::Int64 # max number of iterations QMDP runs for\n    tolerance::Float64 # Bellman residual: terminates when max||Ut-Ut-1|| < tolerance\nend\n# default constructor\nQMDPSolver(;max_iterations::Int64=100, tolerance::Float64=1e-3) = QMDPSolver(max_iterations, tolerance)","category":"page"},{"location":"def_solver/#","page":"Defining a Solver","title":"Defining a Solver","text":"Note that the QMDPSolver inherits from the abstract Solver type that's part of POMDPs.jl.","category":"page"},{"location":"def_solver/#","page":"Defining a Solver","title":"Defining a Solver","text":"Now, let's define a policy type. In general, the policy should contain all the information needed to map a belief to an action. As mentioned earlier, we need alpha vectors to be part of our policy. We can represent the alpha vectors using a matrix of size mathcalS times mathcalA. Recall that in POMDPs.jl, the actions can be represented in a number of ways (Int64, concrete types, etc), so we need a way to map these actions to integers so we can index into our alpha matrix. The type looks like:","category":"page"},{"location":"def_solver/#","page":"Defining a Solver","title":"Defining a Solver","text":"using POMDPModelTools # for ordered_actions\n\ntype QMDPPolicy <: Policy\n    alphas::Matrix{Float64} # matrix of alpha vectors |S|x|A|\n    action_map::Vector{Any} # maps indices to actions\n    pomdp::POMDP            # models for convenience\nend\n# default constructor\nfunction QMDPPolicy(pomdp::POMDP)\n    ns = n_states(pomdp)\n    na = n_actions(pomdp)\n    alphas = zeros(ns, na)\n    am = Any[]\n    space = ordered_actions(pomdp)\n    for a in iterator(space)\n        push!(am, a)\n    end\n    return QMDPPolicy(alphas, am, pomdp)\nend","category":"page"},{"location":"def_solver/#","page":"Defining a Solver","title":"Defining a Solver","text":"Now that we have our solver and policy types, we can write the solve function to compute the policy.","category":"page"},{"location":"def_solver/#Writing-the-Solve-Function-1","page":"Defining a Solver","title":"Writing the Solve Function","text":"","category":"section"},{"location":"def_solver/#","page":"Defining a Solver","title":"Defining a Solver","text":"The solve function takes in a solver, a POMDP, and an optional policy argument. Let's compute those alpha vectors!","category":"page"},{"location":"def_solver/#","page":"Defining a Solver","title":"Defining a Solver","text":"function POMDPs.solve(solver::QMDPSolver, pomdp::POMDP)\n\n    policy = QMDPPolicy(pomdp)\n\n    # get solver parameters\n    max_iterations = solver.max_iterations\n    tolerance = solver.tolerance\n    discount_factor = discount(pomdp)\n\n    # intialize the alpha-vectors\n    alphas = policy.alphas\n\n    # initalize space\n    sspace = ordered_states(pomdp)  # returns a discrete state space object of the pomdp\n    aspace = ordered_actions(pomdp) # returns a discrete action space object\n\n    # main loop\n    for i = 1:max_iterations\n        residual = 0.0\n        # state loop\n        for (istate, s) in enumerate(sspace)\n            old_alpha = maximum(alphas[istate,:]) # for residual\n            max_alpha = -Inf\n            # action loop\n            # alpha(s) = R(s,a) + discount_factor * sum(T(s'|s,a)max(alpha(s'))\n            for (iaction, a) in enumerate(aspace)\n                # the transition function modifies the dist argument to a distribution availible from that state-action pair\n                dist = transition(pomdp, s, a) # fills distribution over neighbors\n                q_new = 0.0\n                for sp in iterator(dist)\n                    # pdf returns the probability mass of sp in dist\n                    p = pdf(dist, sp)\n                    p == 0.0 ? continue : nothing # skip if zero prob\n                    # returns the reward from s-a-sp triple\n                    r = reward(pomdp, s, a, sp)\n    \n                    # stateindex returns an integer\n                    sidx = stateindex(pomdp, sp)\n                    q_new += p * (r + discount_factor * maximum(alphas[sidx,:]))\n                end\n                new_alpha = q_new\n                alphas[istate, iaction] = new_alpha\n                new_alpha > max_alpha ? (max_alpha = new_alpha) : nothing\n            end # actiom\n            # update the value array\n            diff = abs(max_alpha - old_alpha)\n            diff > residual ? (residual = diff) : nothing\n        end # state\n        # check if below Bellman residual\n        residual < tolerance ? break : nothing\n    end # main\n    # return the policy\n    policy\nend","category":"page"},{"location":"def_solver/#","page":"Defining a Solver","title":"Defining a Solver","text":"At each iteration, the algorithm iterates over the state space and computes an alpha vector for each action. There is a check at the end to see if the Bellman residual has been satisfied. The solve function assumes the following POMDPs.jl functions are implemented by the user of QMDP:","category":"page"},{"location":"def_solver/#","page":"Defining a Solver","title":"Defining a Solver","text":"states(pomdp) # (in ordered_states) returns a state space object of the pomdp\nactions(pomdp) # (in ordered_actions) returns the action space object of the pomdp\ntransition(pomdp, s, a) # returns the transition distribution for the s, a pair\nreward(pomdp, s, a, sp) # returns real valued reward from s, a, sp triple\npdf(dist, sp) # returns the probability of sp being in dist\nstateindex(pomdp, sp) # returns the integer index of sp (for discrete state spaces)","category":"page"},{"location":"def_solver/#","page":"Defining a Solver","title":"Defining a Solver","text":"Now that we have a solve function, we define the action function to let users evaluate the policy:","category":"page"},{"location":"def_solver/#","page":"Defining a Solver","title":"Defining a Solver","text":"using LinearAlgebra\n\nfunction POMDPs.action(policy::QMDPPolicy, b::DiscreteBelief)\n    alphas = policy.alphas\n    ihi = 0\n    vhi = -Inf\n    (ns, na) = size(alphas)\n    @assert length(b.b) == ns \"Length of belief and alpha-vector size mismatch\"\n    # see which action gives the highest util value\n    for ai = 1:na\n        util = dot(alphas[:,ai], b.b)\n        if util > vhi\n            vhi = util\n            ihi = ai\n        end\n    end\n    # map the index to action\n    return policy.action_map[ihi]\nend","category":"page"},{"location":"def_solver/#Belief-Updates-1","page":"Defining a Solver","title":"Belief Updates","text":"","category":"section"},{"location":"def_solver/#","page":"Defining a Solver","title":"Defining a Solver","text":"Let's now talk about how we deal with beliefs. Since QMDP is a discrete POMDP solver, we can assume that the user will represent their belief as a probablity distribution over states. That means that we can also use a discrete belief to work with our policy! Lucky for us, the JuliaPOMDP organization contains tools that we can use out of the box for working with discrete beliefs. The POMDPToolbox package contains a DiscreteBelief type that does exactly what we need. The updater function allows us to declare that the DiscreteUpdater is the default updater to be used with a QMDP policy:","category":"page"},{"location":"def_solver/#","page":"Defining a Solver","title":"Defining a Solver","text":"using BeliefUpdaters # remeber to load the package that implements discrete beliefs for us\nPOMDPs.updater(p::QMDPPolicy) = DiscreteUpdater(p.pomdp) ","category":"page"},{"location":"def_solver/#","page":"Defining a Solver","title":"Defining a Solver","text":"These are all the functions that you'll need to have a working POMDPs.jl solver. Let's now use existing benchmark models to evaluate it.","category":"page"},{"location":"def_solver/#Evaluating-the-Solver-1","page":"Defining a Solver","title":"Evaluating the Solver","text":"","category":"section"},{"location":"def_solver/#","page":"Defining a Solver","title":"Defining a Solver","text":"We'll use the POMDPModels package from JuliaPOMDP to initialize a Tiger POMDP problem and solve it with QMDP.","category":"page"},{"location":"def_solver/#","page":"Defining a Solver","title":"Defining a Solver","text":"using POMDPModels\nusing POMDPSimulators\n\n# initialize model and solver\npomdp = TigerPOMDP()\nsolver = QMDPSolver()\n\n# compute the QMDP policy\npolicy = solve(solver, pomdp)\n\n# initalize updater and belief\nb_up = updater(policy)\ninit_dist = initialstate_distribution(pomdp)\n\n# create a simulator object for recording histories\nsim_hist = HistoryRecorder(max_steps=100)\n\n# run a simulation\nr = simulate(sim_hist, pomdp, policy, b_up, init_dist)","category":"page"},{"location":"def_solver/#","page":"Defining a Solver","title":"Defining a Solver","text":"That's all you need to define a solver and evaluate its performance!","category":"page"},{"location":"def_solver/#Defining-Requirements-1","page":"Defining a Solver","title":"Defining Requirements","text":"","category":"section"},{"location":"def_solver/#","page":"Defining a Solver","title":"Defining a Solver","text":"If you share your solver, in order to make it easy to use, specifying requirements as described here is highly recommended.","category":"page"},{"location":"def_solver/#","page":"Defining a Solver","title":"Defining a Solver","text":"[1] Decision Making Under Uncertainty: Theory and Application by Mykel J. Kochenderfer, MIT Press, 2015","category":"page"},{"location":"get_started/#Getting-Started-1","page":"Getting Started","title":"Getting Started","text":"","category":"section"},{"location":"get_started/#","page":"Getting Started","title":"Getting Started","text":"Before writing our own POMDP problems or solvers, let's try out some of the available solvers and problem models available in JuliaPOMDP.","category":"page"},{"location":"get_started/#","page":"Getting Started","title":"Getting Started","text":"Here is a short piece of code that solves the Tiger POMDP using QMDP, and evaluates the results. Note that you must have the QMDP, POMDPModels, and POMDPToolbox modules installed.","category":"page"},{"location":"get_started/#","page":"Getting Started","title":"Getting Started","text":"using POMDPs, QMDP, POMDPModels, POMDPSimulators\n\n# initialize problem and solver\npomdp = TigerPOMDP() # from POMDPModels\nsolver = QMDPSolver() # from QMDP\n\n# compute a policy\npolicy = solve(solver, pomdp)\n\n#evaluate the policy\nbelief_updater = updater(policy) # the default QMDP belief updater (discrete Bayesian filter)\ninit_dist = initialstate_distribution(pomdp) # from POMDPModels\nhr = HistoryRecorder(max_steps=100) # from POMDPSimulators\nhist = simulate(hr, pomdp, policy, belief_updater, init_dist) # run 100 step simulation\nprintln(\"reward: $(discounted_reward(hist))\")","category":"page"},{"location":"get_started/#","page":"Getting Started","title":"Getting Started","text":"The first part of the code loads the desired packages and initializes the problem and the solver. Next, we compute a POMDP policy. Lastly, we evaluate the results.","category":"page"},{"location":"get_started/#","page":"Getting Started","title":"Getting Started","text":"There are a few things to mention here. First, the TigerPOMDP type implements all the functions required by QMDPSolver to compute a policy. Second, each policy has a default updater (essentially a filter used to update the belief of the POMDP). To learn more about Updaters check out the Concepts section.","category":"page"},{"location":"#[POMDPs.jl](https://github.com/JuliaPOMDP/POMDPs.jl)-1","page":"POMDPs.jl","title":"POMDPs.jl","text":"","category":"section"},{"location":"#","page":"POMDPs.jl","title":"POMDPs.jl","text":"A Julia interface for defining, solving and simulating partially observable Markov decision processes and their fully observable counterparts.","category":"page"},{"location":"#Package-Features-1","page":"POMDPs.jl","title":"Package Features","text":"","category":"section"},{"location":"#","page":"POMDPs.jl","title":"POMDPs.jl","text":"General interface that can handle problems with discrete and continuous state/action/observation spaces\nA number of popular state-of-the-art solvers available to use out of the box\nTools that make it easy to define problems and simulate solutions\nSimple integration of custom solvers into the existing interface","category":"page"},{"location":"#Available-Packages-1","page":"POMDPs.jl","title":"Available Packages","text":"","category":"section"},{"location":"#","page":"POMDPs.jl","title":"POMDPs.jl","text":"The POMDPs.jl package contains the interface used for expressing and solving Markov decision processes (MDPs) and partially observable Markov decision processes (POMDPs) in the Julia programming language. The JuliaPOMDP community maintains these packages. The list of solver and support packages is maintained at the POMDPs.jl Readme.","category":"page"},{"location":"#Manual-Outline-1","page":"POMDPs.jl","title":"Manual Outline","text":"","category":"section"},{"location":"#","page":"POMDPs.jl","title":"POMDPs.jl","text":"When updating these documents, make sure this is synced with docs/make.jl!!","category":"page"},{"location":"#Basics-1","page":"POMDPs.jl","title":"Basics","text":"","category":"section"},{"location":"#","page":"POMDPs.jl","title":"POMDPs.jl","text":"Pages = [\"index.md\", \"install.md\", \"get_started.md\", \"concepts.md\"]","category":"page"},{"location":"#Defining-POMDP-Models-1","page":"POMDPs.jl","title":"Defining POMDP Models","text":"","category":"section"},{"location":"#","page":"POMDPs.jl","title":"POMDPs.jl","text":"Pages = [ \"def_pomdp.md\", \"explicit.md\", \"generative.md\", \"requirements.md\", \"interfaces.md\" ]","category":"page"},{"location":"#Writing-Solvers-and-Updaters-1","page":"POMDPs.jl","title":"Writing Solvers and Updaters","text":"","category":"section"},{"location":"#","page":"POMDPs.jl","title":"POMDPs.jl","text":"Pages = [ \"def_solver.md\", \"specifying_requirements.md\", \"def_updater.md\" ]","category":"page"},{"location":"#Analyzing-Results-1","page":"POMDPs.jl","title":"Analyzing Results","text":"","category":"section"},{"location":"#","page":"POMDPs.jl","title":"POMDPs.jl","text":"Pages = [ \"simulation.md\", \"run_simulation.md\", \"policy_interaction.md\" ]","category":"page"},{"location":"#Reference-1","page":"POMDPs.jl","title":"Reference","text":"","category":"section"},{"location":"#","page":"POMDPs.jl","title":"POMDPs.jl","text":"Pages = [\"faq.md\", \"api.md\"]","category":"page"}]
}
