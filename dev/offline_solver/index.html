<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Example: Defining an offline solver · POMDPs.jl</title><script data-outdated-warner src="../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.045/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.24/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../"><img src="../assets/logo.png" alt="POMDPs.jl logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../">POMDPs.jl</a></span></div><form class="docs-search" action="../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><span class="tocitem">Basics</span><ul><li><a class="tocitem" href="../">POMDPs.jl</a></li><li><a class="tocitem" href="../install/">Installation</a></li><li><a class="tocitem" href="../get_started/">Getting Started</a></li><li><a class="tocitem" href="../concepts/">Concepts and Architecture</a></li></ul></li><li><span class="tocitem">Defining (PO)MDP Models</span><ul><li><a class="tocitem" href="../def_pomdp/">Defining POMDPs and MDPs</a></li><li><a class="tocitem" href="../interfaces/">Spaces and Distributions</a></li></ul></li><li><span class="tocitem">Writing Solvers</span><ul><li><a class="tocitem" href="../def_solver/">Solvers</a></li><li class="is-active"><a class="tocitem" href>Example: Defining an offline solver</a></li><li><a class="tocitem" href="../online_solver/">Example: Defining an online solver</a></li></ul></li><li><span class="tocitem">Writing Belief Updaters</span><ul><li><a class="tocitem" href="../def_updater/">Defining a Belief Updater</a></li></ul></li><li><span class="tocitem">Analyzing Results</span><ul><li><a class="tocitem" href="../simulation/">Simulation Standard</a></li><li><a class="tocitem" href="../run_simulation/">Running Simulations</a></li><li><a class="tocitem" href="../policy_interaction/">Interacting with Policies</a></li></ul></li><li><span class="tocitem">POMDPTools</span><ul><li><a class="tocitem" href="../POMDPTools/">POMDPTools: the standard library for POMDPs.jl</a></li><li><a class="tocitem" href="../POMDPTools/distributions/">Implemented Distributions</a></li><li><a class="tocitem" href="../POMDPTools/model/">Model Tools</a></li><li><a class="tocitem" href="../POMDPTools/visualization/">Visualization</a></li><li><a class="tocitem" href="../POMDPTools/beliefs/">Implemented Belief Updaters</a></li><li><a class="tocitem" href="../POMDPTools/policies/">Implemented Policies</a></li><li><a class="tocitem" href="../POMDPTools/simulators/">Implemented Simulators</a></li><li><a class="tocitem" href="../POMDPTools/common_rl/">CommonRLInterface Integration</a></li><li><a class="tocitem" href="../POMDPTools/testing/">Testing</a></li></ul></li><li><a class="tocitem" href="../faq/">Frequently Asked Questions (FAQ)</a></li><li><a class="tocitem" href="../api/">API Documentation</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Writing Solvers</a></li><li class="is-active"><a href>Example: Defining an offline solver</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Example: Defining an offline solver</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/JuliaPOMDP/POMDPs.jl/blob/master/docs/src/offline_solver.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Example:-Defining-an-offline-solver"><a class="docs-heading-anchor" href="#Example:-Defining-an-offline-solver">Example: Defining an offline solver</a><a id="Example:-Defining-an-offline-solver-1"></a><a class="docs-heading-anchor-permalink" href="#Example:-Defining-an-offline-solver" title="Permalink"></a></h1><p>In this example, we will define a simple <a href="../def_solver/#Online-and-Offline-Solvers">offline solver</a> that works for both POMDPs and MDPs. In order to focus on the code structure, we will not create an algorithm that finds an optimal policy, but rather a <em>greedy policy</em>, that is, one that optimizes the expected immediate reward. For information on using this solver in a simulation, see <a href="../run_simulation/#Running-Simulations">Running Simulations</a>.</p><p>We begin by creating a solver type. Since there are no adjustable parameters for the solver, it is an empty type, but for a more complex solver, parameters would usually be included as type fields.</p><pre><code class="language-julia hljs">using POMDPs

struct GreedyOfflineSolver &lt;: Solver end</code></pre><p>Next, we define the functions that will make the solver work for both MDPs and POMDPs.</p><h3 id="MDP-Case"><a class="docs-heading-anchor" href="#MDP-Case">MDP Case</a><a id="MDP-Case-1"></a><a class="docs-heading-anchor-permalink" href="#MDP-Case" title="Permalink"></a></h3><p>Finding a greedy policy for an MDP consists of determining the action that has the best reward for each state. First, we create a simple policy object that holds a greedy action for each state.</p><pre><code class="language-julia hljs">struct DictPolicy{S,A} &lt;: Policy
    actions::Dict{S,A}
end

POMDPs.action(p::DictPolicy, s) = p.actions[s]</code></pre><div class="admonition is-info"><header class="admonition-header">Note</header><div class="admonition-body"><p>A <code>POMDPTools.VectorPolicy</code> could be used here. We include this example to show how to define a custom policy.</p></div></div><p>The solve function calculates the best greedy action for each state and saves it in a policy. To have the widest possible compatibility with POMDP models, we want to use <a href="../api/#POMDPs.reward"><code>reward</code></a><code>(m, s, a, sp)</code> instead of <a href="../api/#POMDPs.reward"><code>reward</code></a><code>(m, s, a)</code>, which means we need to calculate the expectation of the reward over transitions to every possible next state.</p><pre><code class="language-julia hljs">function POMDPs.solve(::GreedyOfflineSolver, m::MDP)

    best_actions = Dict{statetype(m), actiontype(m)}()

    for s in states(m)
        if !isterminal(m, s)
            best = -Inf
            for a in actions(m)
                td = transition(m, s, a)
                r = 0.0
                for sp in support(td)
                    r += pdf(td, sp) * reward(m, s, a, sp)
                end
                if r &gt;= best
                    best_actions[s] = a
                    best = r
                end
            end
        end
    end
    
    return DictPolicy(best_actions)
end</code></pre><div class="admonition is-info"><header class="admonition-header">Note</header><div class="admonition-body"><p>We limited this implementation to using basic POMDPs.jl implementation functions, but tools such as <code>POMDPTools.StateActionReward</code>, <code>POMDPTools.ordered_states</code>, and <code>POMDPTools.weighted_iterator</code> could have been used for a more concise and efficient implementation.</p></div></div><p>We can now verify whether the policy produces the greedy action on an example from POMDPModels:</p><pre><code class="language-julia hljs">using POMDPModels

gw = SimpleGridWorld(size=(2,1), rewards=Dict(GWPos(2,1)=&gt;1.0))
policy = solve(GreedyOfflineSolver(), gw)

action(policy, GWPos(1,1))

# output

:right</code></pre><h3 id="POMDP-Case"><a class="docs-heading-anchor" href="#POMDP-Case">POMDP Case</a><a id="POMDP-Case-1"></a><a class="docs-heading-anchor-permalink" href="#POMDP-Case" title="Permalink"></a></h3><p>For a POMDP, the greedy solution is the action that maximizes the expected immediate reward according to the belief. Since there are an infinite number of possible beliefs, the greedy solution for every belief cannot be calculated online. However, the greedy policy can take the form of an alpha vector policy where each action has an associated alpha vector with each entry corresponding to the immediate reward from taking the action in that state.</p><p>Again, because a POMDP, may have <a href="../api/#POMDPs.reward"><code>reward</code></a><code>(m, s, a, sp, o)</code> instead of <a href="../api/#POMDPs.reward"><code>reward</code></a><code>(m, s, a)</code>, we use the former and calculate the expectation over all next states and observations.</p><pre><code class="language-julia hljs">using POMDPTools: AlphaVectorPolicy

function POMDPs.solve(::GreedyOfflineSolver, m::POMDP)

    alphas = Vector{Float64}[]

    for a in actions(m)
        alpha = zeros(length(states(m)))
        for s in states(m)
            if !isterminal(m, s)
                r = 0.0
                td = transition(m, s, a)
                for sp in support(td)
                    tp = pdf(td, sp)
                    od = observation(m, s, a, sp)
                    for o in support(od)
                        r += tp * pdf(od, o) * reward(m, s, a, sp, o)
                    end
                end
                alpha[stateindex(m, s)] = r
            end
        end
        push!(alphas, alpha)
    end
    
    return AlphaVectorPolicy(m, alphas, collect(actions(m)))
end</code></pre><p>We can now verify that a policy created by the solver determines the correct greedy actions:</p><pre><code class="language-julia hljs">using POMDPModels
using POMDPTools: Deterministic, Uniform

tiger = TigerPOMDP()
policy = solve(GreedyOfflineSolver(), tiger)

@assert action(policy, Deterministic(TIGER_LEFT)) == TIGER_OPEN_RIGHT
@assert action(policy, Deterministic(TIGER_RIGHT)) == TIGER_OPEN_LEFT
@assert action(policy, Uniform(states(tiger))) == TIGER_LISTEN</code></pre></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../def_solver/">« Solvers</a><a class="docs-footer-nextpage" href="../online_solver/">Example: Defining an online solver »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.24 on <span class="colophon-date" title="Wednesday 7 June 2023 19:50">Wednesday 7 June 2023</span>. Using Julia version 1.9.0.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
